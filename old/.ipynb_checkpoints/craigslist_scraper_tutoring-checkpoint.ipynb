{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import requests\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv \n",
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I picked the 10 largest metropolitan areas by population to scrape data from, as well as Sacramento, since it's nearby\n",
    "regions_to_scrape = ['sf_bay_area',\n",
    "                    'new_york',\n",
    "                    'los_angeles',\n",
    "                    'sacramento',\n",
    "                    'chicago',\n",
    "                    'san_diego',\n",
    "                    'houston',\n",
    "                    'phoenix',\n",
    "                    'philadelphia',\n",
    "                    'dallas',\n",
    "                    'san_antonio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-testament",
   "metadata": {},
   "source": [
    "# Extract Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "public-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Session and Retry object to manage the quota Craigslist imposes on HTTP get requests within a certain time period \n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "committed-bhutan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time is 17:31:03\n",
      "Process will finish at 17:32:53\n",
      "Time remaining: 110 seconds.\n",
      "\n",
      "sfbayarea response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 100 seconds.\n",
      "\n",
      "newyork response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 90 seconds.\n",
      "\n",
      "losangeles response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 80 seconds.\n",
      "\n",
      "sacramento response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 70 seconds.\n",
      "\n",
      "chicago response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 60 seconds.\n",
      "\n",
      "sandiego response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 50 seconds.\n",
      "\n",
      "houston response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 40 seconds.\n",
      "\n",
      "phoenix response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 30 seconds.\n",
      "\n",
      "philadelphia response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 20 seconds.\n",
      "\n",
      "dallas response received.\n",
      "\n",
      "Waiting for next response...\n",
      "Time remaining: 10 seconds.\n",
      "\n",
      "sanantonio response received.  Process completed.\n"
     ]
    }
   ],
   "source": [
    "# Walk through each region in our list of regions_to_scrape to get the HTML page corresponding to a search for \"math tutor\" in the services section\n",
    "\n",
    "response_dict = {}\n",
    "sleep_timer = 10\n",
    "num_regions = len(regions_to_scrape)\n",
    "current_time = dt.datetime.now()\n",
    "finish_time = current_time + dt.timedelta(seconds = num_regions * sleep_timer)\n",
    "\n",
    "print(F\"Current time is {current_time.strftime('%H:%M:%S')}\")\n",
    "print(F\"Process will finish at {finish_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "for count, region in enumerate(regions_to_scrape):\n",
    "    # Impose a timer to help prevent too many HTTP requests that would result in a\n",
    "    # ban\n",
    "    time_remaining = (num_regions * sleep_timer) - (count * sleep_timer)\n",
    "    print(F'Time remaining: {time_remaining} seconds.')\n",
    "    time.sleep(sleep_timer)\n",
    "    \n",
    "    current_region = region.replace('_', '')\n",
    "    current_response = session.get('https://' + current_region + '.craigslist.org/d/services/search/bbb?query=math%20tutor&sort=rel')\n",
    "    response_dict[region] = current_response\n",
    "    if count != num_regions - 1:\n",
    "        print()\n",
    "        print(current_region + \" response received.\")\n",
    "        print()\n",
    "        print(\"Waiting for next response...\")\n",
    "    else:\n",
    "        print()\n",
    "        print(current_region + \" response received.  Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flush-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through each region to get a list of all individual postings for math tutoring \n",
    "# in the results page we searched up earlier.\n",
    "posts_dict = {}\n",
    "for region in response_dict:\n",
    "    #current_region = region\n",
    "    current_html_soup = BeautifulSoup(response_dict[region].text, 'html.parser')\n",
    "    current_posts = current_html_soup.find_all('li', class_='result-row')\n",
    "    posts_dict[region] = current_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time is 17:33:02\n",
      "Process will finish by 21:13:02\n",
      "\n",
      "Post number 10 in sf_bay_area is being extracted.\n",
      "Post number 20 in sf_bay_area is being extracted.\n",
      "Post number 30 in sf_bay_area is being extracted.\n",
      "Post number 40 in sf_bay_area is being extracted.\n",
      "Post number 50 in sf_bay_area is being extracted.\n",
      "Post number 60 in sf_bay_area is being extracted.\n",
      "Post number 70 in sf_bay_area is being extracted.\n",
      "Post number 80 in sf_bay_area is being extracted.\n",
      "Post number 90 in sf_bay_area is being extracted.\n",
      "Post number 100 in sf_bay_area is being extracted.\n",
      "Post number 110 in sf_bay_area is being extracted.\n",
      "\n",
      "Soup objects for sf_bay_area acquired.  Waiting for next region...\n",
      "Post number 10 in new_york is being extracted.\n",
      "Post number 20 in new_york is being extracted.\n",
      "Post number 30 in new_york is being extracted.\n",
      "Post number 40 in new_york is being extracted.\n",
      "Post number 50 in new_york is being extracted.\n",
      "Post number 60 in new_york is being extracted.\n",
      "Post number 70 in new_york is being extracted.\n",
      "Post number 80 in new_york is being extracted.\n",
      "Post number 90 in new_york is being extracted.\n",
      "Post number 100 in new_york is being extracted.\n",
      "Post number 110 in new_york is being extracted.\n",
      "\n",
      "Soup objects for new_york acquired.  Waiting for next region...\n",
      "Post number 10 in los_angeles is being extracted.\n",
      "Post number 20 in los_angeles is being extracted.\n",
      "Post number 30 in los_angeles is being extracted.\n",
      "Post number 40 in los_angeles is being extracted.\n",
      "Post number 50 in los_angeles is being extracted.\n",
      "Post number 60 in los_angeles is being extracted.\n",
      "Post number 70 in los_angeles is being extracted.\n",
      "Post number 80 in los_angeles is being extracted.\n",
      "Post number 90 in los_angeles is being extracted.\n",
      "Post number 100 in los_angeles is being extracted.\n",
      "Post number 110 in los_angeles is being extracted.\n",
      "\n",
      "Soup objects for los_angeles acquired.  Waiting for next region...\n",
      "Post number 10 in sacramento is being extracted.\n",
      "Post number 20 in sacramento is being extracted.\n",
      "Post number 30 in sacramento is being extracted.\n",
      "Post number 40 in sacramento is being extracted.\n",
      "Post number 50 in sacramento is being extracted.\n",
      "Post number 60 in sacramento is being extracted.\n",
      "Post number 70 in sacramento is being extracted.\n",
      "Post number 80 in sacramento is being extracted.\n",
      "Post number 90 in sacramento is being extracted.\n",
      "Post number 100 in sacramento is being extracted.\n",
      "Post number 110 in sacramento is being extracted.\n",
      "\n",
      "Soup objects for sacramento acquired.  Waiting for next region...\n",
      "Post number 10 in chicago is being extracted.\n",
      "Post number 20 in chicago is being extracted.\n",
      "Post number 30 in chicago is being extracted.\n",
      "Post number 40 in chicago is being extracted.\n",
      "Post number 50 in chicago is being extracted.\n",
      "\n",
      "Soup objects for chicago acquired.  Waiting for next region...\n",
      "Post number 10 in san_diego is being extracted.\n",
      "Post number 20 in san_diego is being extracted.\n",
      "Post number 30 in san_diego is being extracted.\n",
      "Post number 40 in san_diego is being extracted.\n",
      "Post number 50 in san_diego is being extracted.\n",
      "Post number 60 in san_diego is being extracted.\n",
      "Post number 70 in san_diego is being extracted.\n",
      "Post number 80 in san_diego is being extracted.\n",
      "Post number 90 in san_diego is being extracted.\n",
      "Post number 100 in san_diego is being extracted.\n",
      "Post number 110 in san_diego is being extracted.\n",
      "\n",
      "Soup objects for san_diego acquired.  Waiting for next region...\n",
      "Post number 10 in houston is being extracted.\n",
      "Post number 20 in houston is being extracted.\n",
      "Post number 30 in houston is being extracted.\n",
      "Post number 40 in houston is being extracted.\n",
      "Post number 50 in houston is being extracted.\n",
      "Post number 60 in houston is being extracted.\n",
      "Post number 70 in houston is being extracted.\n",
      "\n",
      "Soup objects for houston acquired.  Waiting for next region...\n",
      "Post number 10 in phoenix is being extracted.\n",
      "Post number 20 in phoenix is being extracted.\n",
      "Post number 30 in phoenix is being extracted.\n",
      "Post number 40 in phoenix is being extracted.\n",
      "\n",
      "Soup objects for phoenix acquired.  Waiting for next region...\n",
      "Post number 10 in philadelphia is being extracted.\n",
      "Post number 20 in philadelphia is being extracted.\n",
      "Post number 30 in philadelphia is being extracted.\n",
      "Post number 40 in philadelphia is being extracted.\n",
      "Post number 50 in philadelphia is being extracted.\n",
      "Post number 60 in philadelphia is being extracted.\n",
      "Post number 70 in philadelphia is being extracted.\n",
      "Post number 80 in philadelphia is being extracted.\n",
      "Post number 90 in philadelphia is being extracted.\n",
      "Post number 100 in philadelphia is being extracted.\n",
      "Post number 110 in philadelphia is being extracted.\n",
      "\n",
      "Soup objects for philadelphia acquired.  Waiting for next region...\n",
      "Post number 10 in dallas is being extracted.\n",
      "Post number 20 in dallas is being extracted.\n",
      "Post number 30 in dallas is being extracted.\n",
      "Post number 40 in dallas is being extracted.\n",
      "Post number 50 in dallas is being extracted.\n",
      "Post number 60 in dallas is being extracted.\n",
      "Post number 70 in dallas is being extracted.\n",
      "Post number 80 in dallas is being extracted.\n",
      "\n",
      "Soup objects for dallas acquired.  Waiting for next region...\n",
      "Post number 10 in san_antonio is being extracted.\n",
      "Post number 20 in san_antonio is being extracted.\n",
      "Post number 30 in san_antonio is being extracted.\n",
      "Post number 40 in san_antonio is being extracted.\n",
      "Post number 50 in san_antonio is being extracted.\n",
      "Post number 60 in san_antonio is being extracted.\n",
      "Post number 70 in san_antonio is being extracted.\n",
      "Post number 80 in san_antonio is being extracted.\n",
      "Post number 90 in san_antonio is being extracted.\n",
      "\n",
      "Soup objects for san_antonio acquired.  Process complete.\n"
     ]
    }
   ],
   "source": [
    "soup_objects_dict = {}\n",
    "\n",
    "current_time = dt.datetime.now()\n",
    "num_seconds = num_regions * 120 * 10\n",
    "max_finish_time = current_time + dt.timedelta(seconds=num_seconds)\n",
    "\n",
    "print(F\"Current time is {current_time.strftime('%H:%M:%S')}\")\n",
    "print(F\"Process will finish by {max_finish_time.strftime('%H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "for count, region in enumerate(posts_dict, start=1):\n",
    "    # Walk through each region and create a list of soup_objects to scrape from by \n",
    "    # storing them into memory.  This way we only have to send these get requests \n",
    "    # once and Craigslist doesn't ban us for sending the same https requests over \n",
    "    # and over\n",
    "    soup_objects_list = []\n",
    "    #link_list = []\n",
    "    for i, post in enumerate(posts_dict[region]):\n",
    "        # Impose a timer so that we send each get request between 5 and 10 seconds.\n",
    "        # This is again to help prevent from getting banned for too many HTTP \n",
    "        # requests.\n",
    "        random_int = random.randint(5,10)\n",
    "        time.sleep(random_int)\n",
    "        current_link = post.a.get('href')\n",
    "        #link_list.append(current_link)\n",
    "        response_object = session.get(current_link)\n",
    "        soup_object = BeautifulSoup(response_object.text, 'html.parser')\n",
    "        soup_objects_list.append(soup_object) \n",
    "        # Impose condition that every 10th post will trigger something printed\n",
    "        # to the screen.  This part of the code is a long process and I wanted\n",
    "        # something to help keep track of how much progress has been made\n",
    "        if (i !=0) and ((i-1) % 10 == 9):\n",
    "            print(F\"Post number {i} in {region} is being extracted.\")\n",
    "    \n",
    "    soup_objects_dict[region] = soup_objects_list\n",
    "    if count != len(posts_dict):\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Waiting for next region...\")\n",
    "        print()\n",
    "    else:\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-interaction",
   "metadata": {},
   "source": [
    "## Pre-processing Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "modular-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "error_list_text = []\n",
    "error_list_links = []\n",
    "\n",
    "# Walk through each region that contains a list of soup objects corresponding to the # search of services for math tutors.\n",
    "for search_region in soup_objects_dict:\n",
    "    # Initialize several lists to store relevant information for analysis\n",
    "    price_list = []\n",
    "    city_list = []\n",
    "    datetime_list = []\n",
    "    body_text_list = []\n",
    "    subregion_list = []\n",
    "    region_list = []\n",
    "    link_list = []\n",
    "    search_region_price_list = []\n",
    "    \n",
    "    # Walk through each soup object in the list corresponding to the search region \n",
    "    # and get the link of the soup object to scrape from.\n",
    "    for soup in soup_objects_dict[search_region]:\n",
    "        try:\n",
    "            link = soup.find(\"meta\", property=\"og:url\")['content']\n",
    "        except:\n",
    "            # In case a link can't be found, we add the soup object to a list\n",
    "            # to inspect later and set link to 'None', which we'll use to filter\n",
    "            # these results out later\n",
    "            link = 'None'\n",
    "            error_list_links.append(soup)\n",
    "            print(\"Couldn't get link\")\n",
    "\n",
    "        # Extract region of post from Craigslist\n",
    "        post_region = soup.find_all('li',class_='crumb area')[0].find('a').get_text()\n",
    "        post_region = post_region.replace(' ', '_')\n",
    "        post_region = post_region.lower()\n",
    "        \n",
    "        # Only let posts through that have a link to scrape from and those posts \n",
    "        # where the region of the post matches the region of the search.  Some CL \n",
    "        # search results are for neighboring areas, ones that come up in a different\n",
    "        # region than the region your search was from, which leads to duplicates in \n",
    "        # areas like Los Angeles and San Diego.  This will weed out duplicates.\n",
    "        if post_region == search_region and link != 'None':\n",
    "            region_list.append(post_region)\n",
    "            link_list.append(link)\n",
    "\n",
    "            # Get text of postingbody of the post and remove unwanted text.\n",
    "            try:\n",
    "                text = soup.find('section', id='postingbody').get_text()\n",
    "                #text = text.replace('\\n', '')\n",
    "                text = text.replace(';', ',') # We do this so that we can use ; as \n",
    "                                              # a delimiter when copying data from a \n",
    "                                              # CSV file into a SQL database later.\n",
    "                text = text.replace('QR Code Link to This Post', '') # We do this \n",
    "                                                                     # because this\n",
    "                                                                     # text from one\n",
    "                                                                     # post in\n",
    "                                                                     # particular was                                                                      # giving me \n",
    "                                                                     # trouble and\n",
    "                                                                     # the best way I \n",
    "                                                                     # could find to \n",
    "                                                                     # handle it was \n",
    "                                                                     # to remove the \n",
    "                                                                     # text.\n",
    "                text = text.replace(u'\\xa0', u' ')\n",
    "                body_text_list.append(text)\n",
    "                \n",
    "            except:\n",
    "                error_list_text.append(soup)\n",
    "                body_text_list.append('None')\n",
    "                print(\"Couldn't get text\")\n",
    "\n",
    "            # Use regular expressions to find all instances of prices in the text\n",
    "            #old_prices = re.findall('(?:[\\$]{1}[,\\d]+.?\\d*)', text)\n",
    "            old_prices = re.findall('(?:[\\$]{1}[,\\d]+\\d*)', text)\n",
    "            # Alternative, if trying to capture decimals \n",
    "            # ^(?:\\${1}\\d+(?:,\\d{3})*(?:\\.{1}\\d{2}){0,1})?$\n",
    "            \n",
    "            # Append prices before they're processed to a separate list, in case we\n",
    "            # need to isolate issues and fix them later.\n",
    "            search_region_price_list.append(old_prices)\n",
    "            \n",
    "            # Intialize empty list to store the new prices after processing old\n",
    "            # prices.\n",
    "            new_prices = []\n",
    "\n",
    "            # Walk through each price in the post.\n",
    "            for price in old_prices:\n",
    "                # Clean unwanted characters.\n",
    "                price = price.replace('$', '')\n",
    "                price = price.replace('/', '')\n",
    "                price = price.replace('!', '')\n",
    "                price = price.replace('h', '')\n",
    "                price = price.replace('.', '')\n",
    "                price = price.replace(')', '')\n",
    "                price = price.replace(',', '')\n",
    "                price = price.replace('>', '')\n",
    "                price = price.rstrip()   \n",
    "                # Some tutors give prices as a range ie '$30-40'.  In order to\n",
    "                # work with this data, I split based on the hyphen, then I can \n",
    "                # use each price individually.\n",
    "                split_prices = price.split('-')\n",
    "\n",
    "                # Walk through each price in the posting, after any necessary splits \n",
    "                # have been made.\n",
    "                for p in split_prices:\n",
    "                    # Only proceed if the post contained prices, ie if p is a non-\n",
    "                    # empty string.\n",
    "                    if len(p)!=0:\n",
    "                        \n",
    "                        try:\n",
    "                            # Convert string price to int.\n",
    "                            new_int = int(p)\n",
    "                            new_prices.append(new_int)\n",
    "                        \n",
    "                        except:\n",
    "                            # Show which prices aren't able to convert to an int and \n",
    "                            # the post they came from so we can isolate and fix the \n",
    "                            # issue.\n",
    "                            print(F'Error converting this price: {p}')\n",
    "                            print(old_prices)\n",
    "                            print()\n",
    "                            print('Here is the text of the post:')\n",
    "                            print()\n",
    "                            print(text)\n",
    "                            print('-'*50)\n",
    "                            print()\n",
    "                            # Set prices that can't be covered to NaN so the process \n",
    "                            # can finish.\n",
    "                            new_prices.append(np.nan) \n",
    "\n",
    "                            \n",
    "            # For posts that had no prices listed, we append new_prices with \"None\"\n",
    "            if len(new_prices)==0:\n",
    "                price_list.append('None')\n",
    "            # For posts that had a single price, we use it.\n",
    "            elif len(new_prices)==1:\n",
    "                price_list.append(new_prices[0])\n",
    "            # For posts that contained two prices, we average them.  This helps with \n",
    "            # posts that give a range of prices (ie $25-30).\n",
    "            elif len(new_prices)==2:\n",
    "                avg_price_2 = np.average(new_prices)\n",
    "                price_list.append(avg_price_2)\n",
    "            # If a post has more than 3 prices, we append them, but this means we \n",
    "            # have to inspect them manually and deal with them later.\n",
    "            else:\n",
    "                price_list.append(new_prices)\n",
    "\n",
    "\n",
    "            # Get city information for each posting.\n",
    "            try:\n",
    "                city = soup.find(class_='postingtitletext').small.get_text()\n",
    "                \n",
    "                # Because of the way CL operates, one has to choose a city from a\n",
    "                # radio button list that CL provides when one creates a post to offer \n",
    "                # a service, however later, there's a field where they can type in \n",
    "                # any city they want.  Many people will randomly choose a city from \n",
    "                # the radio button list, but then  post their city as \"online\".  This \n",
    "                # makes sure we capture them. \n",
    "                re_pattern = re.compile('online')\n",
    "                online_flag = re.search(re_pattern, city.lower())\n",
    "                if online_flag:\n",
    "                    city_list.append('Online')\n",
    "                else:\n",
    "                    # Strip out leading and trailing white spaces, replace\n",
    "                    # parentheses, and capitalize each word in the str.\n",
    "                    city = city.strip()\n",
    "                    city = city.replace('(', '').replace(')', '')        \n",
    "                    city = city.title()\n",
    "                    city_list.append(city)\n",
    "            except:\n",
    "                # If a post has no city information, use None\n",
    "                city_list.append('None')\n",
    "\n",
    "            # Extract subregion of Craigslist that the post was made in.\n",
    "            # This will allow for comparison of prices across different cities\n",
    "            # within the same metropolitan sub_region.\n",
    "            try:\n",
    "                subregion = soup.find_all('li', class_='crumb subarea')[0].find('a').get_text()\n",
    "                subregion = subregion.title()\n",
    "                subregion_list.append(subregion)\n",
    "            except:\n",
    "                subregion_list.append('None')\n",
    "\n",
    "\n",
    "            # Extract time the posting was made.\n",
    "            try:\n",
    "                dt_object = soup.find('time')['datetime']\n",
    "                datetime_list.append(dt_object)\n",
    "            except:\n",
    "                datetime_list.append('None')\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Create temporary df to store results for each region\n",
    "    temp_df = pd.DataFrame(data=zip(datetime_list,\n",
    "                                    link_list, \n",
    "                                    price_list, \n",
    "                                    city_list, \n",
    "                                    subregion_list, \n",
    "                                    region_list, \n",
    "                                    body_text_list,\n",
    "                                    search_region_price_list),\n",
    "                        columns=['date_posted', \n",
    "                                 'link', \n",
    "                                 'price', \n",
    "                                 'city', \n",
    "                                 'subregion', \n",
    "                                 'region', \n",
    "                                 'post_text',\n",
    "                                 'price_list']\n",
    "                          )\n",
    "    \n",
    "    # Append each temporary df to a list, which we can concatenate into one larger \n",
    "    # df, later.\n",
    "    df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nearby-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for errors in getting text from a post, or from getting the URL of a post.\n",
    "len(error_list_text), len(error_list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "endangered-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dfs for each region into one larger df and check its shape.\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spanish-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     495\n",
       "False    360\n",
       "Name: post_text, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get date of html request to label our output with.\n",
    "date_of_html_request = str(dt.date.today())\n",
    "\n",
    "# Include the date posts were scraped on to track tutoring prices over time.\n",
    "df['posts_scraped_on'] = date_of_html_request\n",
    "\n",
    "# Count duplicates.\n",
    "df['post_text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-compound",
   "metadata": {},
   "source": [
    "### Dropping Duplicates or posts that contained no prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gorgeous-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of duplicate results, then drop them and reset indices.\n",
    "duplicate_indices = df[df['post_text'].duplicated()==True].index\n",
    "df_no_dups = df.drop(index=duplicate_indices)\n",
    "df_no_dups = df_no_dups.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "guided-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out results that don't have a price and reset indices.\n",
    "df_with_prices = df_no_dups[df_no_dups['price']!='None']\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "residential-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 138 posts that had prices included and weren't duplicates.\n",
      "Only 38.33% of the posts that we scraped remain.\n"
     ]
    }
   ],
   "source": [
    "post_with_prices_count = len(df_with_prices)\n",
    "num_posts = len(df_no_dups)\n",
    "\n",
    "percent_with_prices = post_with_prices_count/num_posts * 100\n",
    "\n",
    "print(F\"There were {post_with_prices_count} posts that had prices included and weren't duplicates.\")\n",
    "print(F\"Only {percent_with_prices:.2f}% of the posts that we scraped remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-start",
   "metadata": {},
   "source": [
    "### Extracting complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-measurement",
   "metadata": {},
   "source": [
    "# Transforming Craigslist data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-andrew",
   "metadata": {},
   "source": [
    "### Are there any posts that might need manual cleaning?  This would include:\n",
    "* Posts that had 3 or more prices\n",
    "* Posts that were marked as being null during pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "statutory-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of which prices are still lists, as opposed to single values, we will \n",
    "# need to investigate these later.\n",
    "df_with_prices['prices_need_cleaning'] = df_with_prices['price'].apply(lambda x: isinstance(x, list) and len(x) >= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-teddy",
   "metadata": {},
   "source": [
    "## Investigating posts that had three or more prices listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conventional-mirror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [90, 60, 40]\n",
       "11               [100, 115, 130, 65, 30, 60]\n",
       "13                      [40, 40, 40, 40, 40]\n",
       "14                              [30, 35, 45]\n",
       "17                              [90, 60, 40]\n",
       "20                      [40, 40, 40, 40, 40]\n",
       "22                          [45, 55, 40, 50]\n",
       "28                  [35, 35, 40, 40, 55, 80]\n",
       "31                          [40, 40, 45, 45]\n",
       "35                              [40, 40, 40]\n",
       "42               [100, 115, 130, 65, 30, 60]\n",
       "50                      [40, 40, 40, 40, 40]\n",
       "52                          [30, 30, 60, 90]\n",
       "53     [40, 80, 40, 10, 40, 30, 40, 80, 100]\n",
       "56                          [25, 30, 50, 50]\n",
       "59                          [40, 40, 40, 40]\n",
       "68              [40, 30, 40, 30, 40, 40, 40]\n",
       "83                            [50, 100, 135]\n",
       "84              [40, 30, 40, 30, 40, 40, 40]\n",
       "86                              [65, 55, 55]\n",
       "90                          [20, 25, 30, 30]\n",
       "93                              [25, 35, 20]\n",
       "94                          [30, 30, 60, 90]\n",
       "99                            [50, 100, 135]\n",
       "100                             [40, 40, 40]\n",
       "108                             [50, 10, 50]\n",
       "111                             [30, 40, 50]\n",
       "116                             [30, 40, 50]\n",
       "120                             [65, 55, 55]\n",
       "125                             [50, 10, 50]\n",
       "129             [40, 30, 40, 30, 40, 40, 40]\n",
       "131                         [20, 30, 20, 30]\n",
       "133                             [50, 10, 50]\n",
       "135                             [25, 50, 25]\n",
       "Name: price, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the entries with 3 or more prices listed, let's investigate why\n",
    "df_with_prices[df_with_prices['prices_need_cleaning']==True]['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contrary-immunology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/eby/lss/d/oakland-3rd-year-medical-student-as/7426289294.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect links manually, one by one, to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df_with_prices.iloc[1]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-integration",
   "metadata": {},
   "source": [
    "### Cleaning posts with three or more prices manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "raised-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_col_idx = df_with_prices.columns.get_loc('price')\n",
    "need_clean_col_idx = df_with_prices.columns.get_loc('prices_need_cleaning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-optics",
   "metadata": {},
   "source": [
    "#### Dropping duplicates that weren't spotted earlier\n",
    "\n",
    "These posts come up multiple times and are duplicates, but they change the text of their posting just slightly, so Pandas is unable to detect the duplicate postings on it's own, and we have to find and drop them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "musical-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of duplicates, set the first instance to the price that makes the most \n",
    "# sense, then drop all remaining duplicate posts.\n",
    "kenari_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('kenaritutor.com')==True].index\n",
    "\n",
    "try:\n",
    "    # Because the ad says $90 in person, $60 for online, and Corona Virus pricing of\n",
    "    # $40 for online weekdays, I'm using the $40 per hour rate because it seems the\n",
    "    # most reasonable.  We also set prices_need_cleaning to False b/c the prices have\n",
    "    # been cleaned\n",
    "    df_with_prices.iloc[kenari_tutor_idx[0],\n",
    "                        [price_col_idx,\n",
    "                         need_clean_col_idx]\n",
    "                       ] = 40, False\n",
    "except:\n",
    "    print('Issue with kenari_tutor_idx and iloc.')\n",
    "    pass\n",
    "\n",
    "# Drop duplicates and reset indices\n",
    "df_with_prices.drop(labels=kenari_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "identical-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of duplicates.\n",
    "park_academy_idx = df_with_prices[df_with_prices['post_text'].str.contains('(949) 490-0872', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    # The add says $55/hr for K-12, then $65/hr for AP/Honors, as well as Pre-calc, \n",
    "    # etc., I'm going to average the two prices.  Set needs cleaning column to False \n",
    "    # b/c the prices have been cleaned.\n",
    "    df_with_prices.iloc[park_academy_idx[0],\n",
    "                        [price_col_idx,\n",
    "                         need_clean_col_idx]\n",
    "                       ] = 60, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with park_academy_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "# Drop duplicates and reset indices\n",
    "df_with_prices.drop(labels=park_academy_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "competitive-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates, correct price, drop duplicates\n",
    "star_star_college_math_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('https://www.youtube.com/channel/UCqhFZRmUqOAAPMQpo58TV7g'\n",
    "                   ) == True].index\n",
    "\n",
    "try:\n",
    "    # The ad repeats the price of $40 over and over, so I'm replacing the price with \n",
    "    # a single instance.  We also set prices_need_cleaning to False b/c the prices \n",
    "    # have been cleaned.\n",
    "    df_with_prices.iloc[star_star_college_math_tutor_idx[0], [price_col_idx, need_clean_col_idx]] = 40, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with star_star_college_math_tutor_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "# Drop duplicates and reset indices\n",
    "df_with_prices.drop(labels=star_star_college_math_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "smoking-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates, correct price, drop duplicates\n",
    "poway_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('CSUSM: 342')==True].index\n",
    "\n",
    "try:\n",
    "    # This ad says $30 for one hour.\n",
    "    df_with_prices.iloc[poway_tutor_idx[0], [price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with poway_tutor_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "df_with_prices.drop(labels=poway_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-machinery",
   "metadata": {},
   "source": [
    "#### Distilling long lists of prices down to one price\n",
    "\n",
    "Next, we distill posts that had more complicated text that involved three or more prices, such as :\n",
    "\n",
    "* $40$/hr, $50$/1.5hr, $60$/2hr\n",
    "  * Complicated pricing schedule\n",
    "* $40$/hr but $10$ additional per person, if a group session is desired\n",
    "  * Group rates\n",
    "* $30$/hr Science, $40$/hr math, come and try a first session for the reduced price of $20$.\n",
    "  * Special offers\n",
    "\n",
    "into a single price.  Other posts repeated their prices multiple times, so we distill those down to a single price as well, then mark any of the entries we changed as being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "careful-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad mentions several prices for different subjects, but explicitly says $30 for math.\n",
    "la_honda_idx = df_with_prices[df_with_prices['post_text'].str.contains('909-640-3570')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[la_honda_idx,[price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with la_honda_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# This ad mentions $45 for lower division college courses, which are a large segment of the subjects I help with, so I'm using that price to compare myself against.\n",
    "ucb_phd_student_and_ta_idx = df_with_prices[df_with_prices['post_text'].str.contains('Former UC-Berkeley economics Ph.D. student and TA')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucb_phd_student_and_ta_idx,[price_col_idx, need_clean_col_idx]] = 45, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucb_phd_student_and_ta_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# Says $40 for in person, or $45 for at home, so I took the average.\n",
    "san_mateo_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('I mainly tutor, in person, at the Downtown Redwood City, downtown San Mateo')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[san_mateo_tutor_idx,[price_col_idx, need_clean_col_idx]] = 42.5, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with san_mateo_tutor and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sophisticated-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This guy has weird price structuring, but I used his hourly rate for each time interval, $100 for 80 minutes, $115 for 100 minutes, $130 for 120 minutes, then averaged those hourly rates to estimate for what a single hour would cost.\n",
    "oakland_exp_tutor_online_idx = df_with_prices[df_with_prices['post_text'].str.contains('I received a full scholarship to University of Cincinnati and held a 3.8 GPA through my masterâ€™s program in aerospace')==True].index\n",
    "\n",
    "oakland_tutor_avg_rate = ((100/80) + (115/100) + (130/120)) * 60 / 3\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[oakland_exp_tutor_online_idx,[price_col_idx, need_clean_col_idx]] = oakland_tutor_avg_rate, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with oakland_exp_tutor_online_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "# This guy's ad explcityly says $57 per hour.\n",
    "blake_tutoring_indices = df_with_prices[df_with_prices['post_text'].str.contains('BlakeTutoring.com', case=False)==True].index\n",
    "\n",
    "df_with_prices.iloc[blake_tutoring_indices, price_col_idx] = 57\n",
    "\n",
    "\n",
    "# Charges $50 per hour for sessions under 3 hours\n",
    "spss_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('Worked for 2 companies named', case=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[spss_tutor_idx, [price_col_idx, need_clean_col_idx]] = 50, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with spss_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "explicit-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $25/hr for high school, $30/hr for college, just went with $30/hr\n",
    "sharp_mind_idx = df_with_prices[df_with_prices['post_text'].str.contains('(650) 398-9490', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[sharp_mind_idx, [price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with sharp_mind_idx and iloc.\")\n",
    "    pass\n",
    "    \n",
    "    \n",
    "# Says $50/hr    \n",
    "trevor_skelly_idx = df_with_prices[df_with_prices['post_text'].str.contains('trevorskelly')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[trevor_skelly_idx, [price_col_idx, need_clean_col_idx]] = 50, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with trevor_skelly_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "internal-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $25/hr if meeting near CSU Sac, $35/hr if they drive to you, $20/hr for online.\n",
    "# I chose $30/hr to split the difference between the in person prices.\n",
    "best_math_idx = df_with_prices[df_with_prices['post_text'].str.contains('bestmathtutoring.com')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[best_math_idx, [price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with best_math_idx and iloc.\")\n",
    "    pass    \n",
    "\n",
    "# Says #60 per hour.\n",
    "glasses_lady_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"offering virtual one-on-one Math tutoring via Zoom\")==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[glasses_lady_idx, [price_col_idx, need_clean_col_idx]] = 60, False\n",
    "except:\n",
    "    print(\"Issue with glasses_lady_idx and iloc.\")\n",
    "    pass    \n",
    "\n",
    "\n",
    "ucla_grad_henry_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"916 390-7923\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucla_grad_henry_idx, [price_col_idx, need_clean_col_idx]] = 35, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucla_grad_henry_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nonprofit-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_d_idx = df_with_prices[df_with_prices['post_text'].str.contains('Peter D.')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[peter_d_idx, [price_col_idx, need_clean_col_idx]] = 40, False\n",
    "except:\n",
    "    print(\"Issue with peter_d_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "average-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $20/hr online, $30/hr in person, split the difference at $25\n",
    "austin_sabrina_idx = df_with_prices[df_with_prices['post_text'].str.contains('My girlfriend Sabrina')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[austin_sabrina_idx, [price_col_idx, need_clean_col_idx]] = 25, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with austin_sabrina_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "catholic-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_farrell_idx = df_with_prices[df_with_prices['post_text'].str.contains('Alexander Farrell')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[alex_farrell_idx, [price_col_idx, need_clean_col_idx]] = 25, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with alex_farrell_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "formal-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post says $30/hr for Precalc/Trig and $50/hr for Calculus, so I took the average\n",
    "lonzo_tutoring_idx = df_with_prices[df_with_prices['post_text'].str.contains('951-795-5027', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[lonzo_tutoring_idx, [price_col_idx, need_clean_col_idx]] = 40, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with lonzo_tutoring_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "improving-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post includes many prices, but states $55/hr for Precalc and $80/hr for Calculus, so I took the average of those prices\n",
    "aerospace_engineer_idx = df_with_prices[df_with_prices['post_text'].str.contains('undergraduate students at UC San Diego', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[aerospace_engineer_idx, [price_col_idx, need_clean_col_idx]] = (55 + 80)/2, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with aerospace_engineer_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-natural",
   "metadata": {},
   "source": [
    "## Checking results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-terry",
   "metadata": {},
   "source": [
    "#### Are there any posts that were marked as needing to be cleaned that we missed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "designed-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no posts that had multiple prices still needing cleaning.\n"
     ]
    }
   ],
   "source": [
    "num_still_as_list = len(df_with_prices[df_with_prices['prices_need_cleaning']==True]['price'])\n",
    "\n",
    "if num_still_as_list==0:\n",
    "    print(\"There are no posts that had multiple prices still needing cleaning.\")\n",
    "else:\n",
    "    print(F\"There are {num_still_as_list} posts that still have multiple prices needing cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pointed-shift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: price, dtype: object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the posts with three or more that still need cleaning.\n",
    "df_with_prices[df_with_prices['prices_need_cleaning']==True]['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-office",
   "metadata": {},
   "source": [
    "#### Are there any posts with a price that was marked as being null during pre-processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "discrete-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no posts that have null prices.\n"
     ]
    }
   ],
   "source": [
    "num_null_prices = len(df_with_prices[df_with_prices['price'].isnull()==True])\n",
    "\n",
    "if num_null_prices==0:\n",
    "    print(\"There are no posts that have null prices.\")\n",
    "else:\n",
    "    print(F\"There are {num_null_prices} posts that have null prices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "arabic-observer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>subregion</th>\n",
       "      <th>region</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "      <th>posts_scraped_on</th>\n",
       "      <th>prices_need_cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_posted, link, price, city, subregion, region, post_text, price_list, posts_scraped_on, prices_need_cleaning]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the posts with prices that are NaN because Python wasn't able to parse them properly during pre-processing.\n",
    "df_with_prices[df_with_prices['price'].isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-substance",
   "metadata": {},
   "source": [
    "## Investigating posts with extreme prices.  Are there any price outliers that we need to clean?\n",
    "\n",
    "Prices >= 100 or <= 20 are what I would consider to be extreme prices.  Let's flag and use the flag to locate and then investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hydraulic-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of which prices are what I would consider to be unusual\n",
    "df_with_prices['price_to_investigate'] = df_with_prices['price'].apply(lambda x: (x>=100) | (x<=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "further-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\n\"Hey there!\\n\\nMy name is Angel and ...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>200</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[$200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20000</td>\n",
       "      <td>\\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...</td>\n",
       "      <td>[$20,000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>19</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...</td>\n",
       "      <td>[$19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15</td>\n",
       "      <td>\\n\\n\\n\\n\\njargon free math tutor $15 all level...</td>\n",
       "      <td>[$15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>150000</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is John Nguyen and I'...</td>\n",
       "      <td>[$150,000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>125.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nI do Private Tutoring 7 days a week ...</td>\n",
       "      <td>[$25, $225]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>120</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...</td>\n",
       "      <td>[$120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>150.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name is Daniel. I graduate...</td>\n",
       "      <td>[$200, $100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>180</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[$180]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi everyone, do you need math, physi...</td>\n",
       "      <td>[$15, $25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>20000</td>\n",
       "      <td>\\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...</td>\n",
       "      <td>[$20,000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n\\n\\nTenured math professor at a major un...</td>\n",
       "      <td>[$100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>18</td>\n",
       "      <td>\\n\\n\\n\\n\\nSAT prep for as low as $18 per hour!...</td>\n",
       "      <td>[$18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>104.5</td>\n",
       "      <td>\\n\\n\\n\\n\\nI train students to attack the GMAT ...</td>\n",
       "      <td>[$84, $125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>20000</td>\n",
       "      <td>\\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...</td>\n",
       "      <td>[$20,000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n\\n\\n\\nHigh School/College Math and Chemist...</td>\n",
       "      <td>[$10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>20000</td>\n",
       "      <td>\\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...</td>\n",
       "      <td>[$20,000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nYes, you read right! I--Mr. C--am of...</td>\n",
       "      <td>[$20, $20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price                                          post_text    price_list\n",
       "9        20  \\n\\n\\n\\n\\n\"Hey there!\\n\\nMy name is Angel and ...         [$20]\n",
       "15       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "27      200  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...        [$200]\n",
       "32    20000  \\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...     [$20,000]\n",
       "33       19  \\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...         [$19]\n",
       "44       15  \\n\\n\\n\\n\\njargon free math tutor $15 all level...         [$15]\n",
       "50       20  \\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...         [$20]\n",
       "51       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "53       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "57       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "59       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "62       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "64       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "66   150000  \\n\\n\\n\\n\\nHello! My name is John Nguyen and I'...    [$150,000]\n",
       "68    125.0  \\n\\n\\n\\n\\nI do Private Tutoring 7 days a week ...   [$25, $225]\n",
       "69       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "74      120  \\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...        [$120]\n",
       "75    150.0  \\n\\n\\n\\n\\nG'day! My name is Daniel. I graduate...  [$200, $100]\n",
       "76      180  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...        [$180]\n",
       "78       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "82       20  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...         [$20]\n",
       "89     20.0  \\n\\n\\n\\n\\nHi everyone, do you need math, physi...    [$15, $25]\n",
       "90    20000  \\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...     [$20,000]\n",
       "94      100  \\n\\n\\n\\n\\nTenured math professor at a major un...        [$100]\n",
       "95       18  \\n\\n\\n\\n\\nSAT prep for as low as $18 per hour!...         [$18]\n",
       "96    104.5  \\n\\n\\n\\n\\nI train students to attack the GMAT ...   [$84, $125]\n",
       "99    20000  \\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...     [$20,000]\n",
       "108      10  \\n\\n\\n\\n\\nHigh School/College Math and Chemist...         [$10]\n",
       "114   20000  \\n\\n\\n\\n\\nGMAT/GREI'm a full-time GMAT/GRE ins...     [$20,000]\n",
       "125    20.0  \\n\\n\\n\\n\\nYes, you read right! I--Mr. C--am of...    [$20, $20]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[df_with_prices['price_to_investigate']==True][['price', 'post_text', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sitting-buddy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nHello, \\n\\nI am an engineering working professional in the aerospace industry looking to offer math tutoring services to middle school, high school, and college students. I used to tutor undergraduate students at UC San Diego for over four years and want to continue to work with students to improve their learning outcomes and academic achievements. Please see below for more information on sessions and rates. \\n\\nPre-Algebra: $35/hr\\nAlgebra I: $35/hr\\nAlgebra II: $40/hr\\nGeometry: $40/hr\\nPre-Calculus: $55/hr\\nCalculus I and II: $80/hr\\n\\nSessions will be conducted on Zoom. \\n\\nMy availability:\\nMonday - Thursday 4pm - 9pm (PT)\\nI am flexible with my schedule so please let me know what works best for the student. \\n\\nThanks!\\n\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sfc/lss/d/san-francisco-math-tutoring-available/7426882297.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=26\n",
    "  display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-belize",
   "metadata": {},
   "source": [
    "### Dropping posts with extreme prices that are duplicates or aren't relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bizarre-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This person mentions no prices, the only $ amount specified is how much they earned # in scholarships, so we drop all instances\n",
    "at_geemale_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"at geemale\")==True].index\n",
    "\n",
    "df_with_prices.drop(labels=at_geemale_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "objective-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all instances, keep first instance, drop all the remaining duplicates\n",
    "ansari_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"peerlinc.com\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=ansari_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "advanced-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all instances, keep first instance, drop all the remaining duplicates\n",
    "ridgewood_nyc_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"(646) 326-2191\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=ridgewood_nyc_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "correct-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutor's ad only mentions the rate per student for an hour long group session, so we drop all instances\n",
    "why_exceptional_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"countless time-saving, test-taking strategies\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=why_exceptional_tutor_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "current-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for SAT prep only, not really what I'm competing against, so we drop all instances\n",
    "study_house_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"STUDY HOUSE LLC\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=study_house_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "green-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for poker tutoring/coaching, not really what I'm competing against, so we drop all instances.  He also mentions he tutors math in this post, but he has a separate post up that we've captured which has his math tutoring pricing information.\n",
    "australia_daniel_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"I'm available as a dealer if you need one\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=australia_daniel_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cordless-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for MCAT tutoring, not really what I'm competing against, so we drop all instances.\n",
    "connor_MCAT_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"My name is Connor and I've been teaching the MCAT since 2016\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=connor_MCAT_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "nutritional-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad doesn't mention a price, so we drop all instances.\n",
    "john_baptist_nguyen_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"john-baptist-nguyen\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=john_baptist_nguyen_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-storage",
   "metadata": {},
   "source": [
    "### Correct pricing information for posts with extreme prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "medium-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_to_investigate_col_idx = df_with_prices.columns.get_loc('price_to_investigate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "oriented-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $25/hr but then mentions a prepay plan for $225.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $25\n",
    "james_edward_nassir_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"James Edward Nassir, EE, Educator, and Discoverer of the 5th Force\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[james_edward_nassir_idx, [price_col_idx, price_to_investigate_col_idx]] = 25, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with james_edward_nassir_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "radical-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $50/hr but then mentions a prepay plan for $160 for 4 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $50\n",
    "google_maps_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"willing to travel if Google Maps\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[google_maps_idx, [price_col_idx, price_to_investigate_col_idx]] = 50, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with google_maps_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "allied-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $84/hr but then mentions a $125 for 1.5 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $84\n",
    "rescue_animals_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"TestTrainerinc\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[rescue_animals_idx, [price_col_idx, price_to_investigate_col_idx]] = 84, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with rescue_animals_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "nutritional-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $45/hr for high school or college, but then mentions a $35 for middle school.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $45, since I primarily tutor high school or college students.\n",
    "rancho_penasquitos_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"Rancho Penasquitos (Park Village Neighborhood)\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[rancho_penasquitos_idx, [price_col_idx, price_to_investigate_col_idx]] = 45, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with rancho_penasquitos_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-amount",
   "metadata": {},
   "source": [
    "## Checking results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-hazard",
   "metadata": {},
   "source": [
    "#### Are there any posts with extreme prices that we marked which still need investigation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "blessed-rapid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\n\"Hey there!\\n\\nMy name is Angel and ...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>19</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...</td>\n",
       "      <td>[$19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15</td>\n",
       "      <td>\\n\\n\\n\\n\\njargon free math tutor $15 all level...</td>\n",
       "      <td>[$15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...</td>\n",
       "      <td>[$20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>120</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...</td>\n",
       "      <td>[$120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi everyone, do you need math, physi...</td>\n",
       "      <td>[$15, $25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n\\n\\nTenured math professor at a major un...</td>\n",
       "      <td>[$100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nYes, you read right! I--Mr. C--am of...</td>\n",
       "      <td>[$20, $20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price                                          post_text  price_list\n",
       "9      20  \\n\\n\\n\\n\\n\"Hey there!\\n\\nMy name is Angel and ...       [$20]\n",
       "30     19  \\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...       [$19]\n",
       "41     15  \\n\\n\\n\\n\\njargon free math tutor $15 all level...       [$15]\n",
       "47     20  \\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...       [$20]\n",
       "63    120  \\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...      [$120]\n",
       "74   20.0  \\n\\n\\n\\n\\nHi everyone, do you need math, physi...  [$15, $25]\n",
       "78    100  \\n\\n\\n\\n\\nTenured math professor at a major un...      [$100]\n",
       "106  20.0  \\n\\n\\n\\n\\nYes, you read right! I--Mr. C--am of...  [$20, $20]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the posts that remain, but they all have prices which agree with the posting, so no cleaning is needed.\n",
    "df_with_prices[df_with_prices['price_to_investigate']==True][['price', 'post_text', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "velvet-monte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sby/lss/d/palo-alto-free-hour-experienced-tutor/7423496674.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If any prices still need cleaning, inspect their links to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df_with_prices.iloc[9]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "automatic-bangkok",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 11)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size of results after transforming is complete\n",
    "df_with_prices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-capacity",
   "metadata": {},
   "source": [
    "#### Everything looks good.  Transforming complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-jurisdiction",
   "metadata": {},
   "source": [
    "# Saving results\n",
    "\n",
    "### Store results locally as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "spread-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.  CL links will expire after some number of days, the prices_need_cleaning and price_to_investigate columns have been manually inspected, and lastly we've distilled the multiple prices in the price_list down to a single value\n",
    "df_for_sql = df_with_prices.drop(labels=['prices_need_cleaning','link', 'price_list', 'price_to_investigate'], axis=1)\n",
    "\n",
    "# In order for psycopg2 to parse our CSV file correctly later, we need to escape all new line characters by adding an additional \\ in front of \\n.\n",
    "df_for_sql['post_text'] = df_for_sql['post_text'].str.replace('\\n', '\\\\n')\n",
    "\n",
    "# Store cleaned data as CSV file in preparation for importing to SQL database\n",
    "df_for_sql.to_csv(\"./{}_all_regions_with_prices.csv\".format(date_of_html_request), index=False, sep=';')\n",
    "\n",
    "# Store original data, before we applied any cleaning to it, in case it's needed for something later on.\n",
    "df.to_csv(\"./{}_all_regions_posts.csv\".format(date_of_html_request), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-boards",
   "metadata": {},
   "source": [
    "### Importing into PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "conscious-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to PSQL database\n",
    "conn = psycopg2.connect(\"host=localhost dbname=rancher user=rancher\")\n",
    "\n",
    "# Instantiate a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Use cursor object to create a database for storing the information we scraped and cleaned, if one doesn't already exist.\n",
    "cur.execute(\"\"\"    \n",
    "    CREATE TABLE IF NOT EXISTS cl_tutoring2(\n",
    "    id SERIAL primary key,\n",
    "    date_scraped date,\n",
    "    price decimal,\n",
    "    city text,\n",
    "    subregion text,\n",
    "    region text,\n",
    "    post_text text,\n",
    "    date_posted timestamp\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "framed-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Copy data from our CSV file into database.  \n",
    "### Note, we can use the ; separator freely because we replaced all instances of semicolons in a post to commas during the preprocessing stage, ensuring that psycopg2 won't misinterpret a semicolon in the body of a post as a separator, splitting a row in the CSV file into too many columns as a result.\n",
    "### Also, we must specify null=\"\" because Python represents null values as an empty string when writing to a CSV file and psycopg2 needs to know how null values are represented in the CSV file in order to properly insert null values into the database\n",
    "with open(str(date_of_html_request) + '_all_regions_with_prices.csv', 'r') as file:\n",
    "    next(file) # Skip the header row\n",
    "    cur.copy_from(file, 'cl_tutoring2', sep=';', null=\"\", columns=('date_posted', 'price', 'city', 'subregion', 'region', 'post_text', 'date_scraped'))\n",
    "    \n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-wheat",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-guess",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
