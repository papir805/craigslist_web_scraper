{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import requests\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv \n",
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I picked the 10 largest metropolitan areas by population to scrape data from, as well as Sacramento, since it's nearby\n",
    "# regions_to_scrape = ['sf_bay_area',\n",
    "#                     'new_york',\n",
    "#                     'los_angeles',\n",
    "#                     'sacramento',\n",
    "#                     'chicago',\n",
    "#                     'san_diego',\n",
    "#                     'houston',\n",
    "#                     'phoenix',\n",
    "#                     'philadelphia',\n",
    "#                     'dallas',\n",
    "#                     'san_antonio']\n",
    "\n",
    "regions_to_scrape = ['sf_bay_area']\n",
    "\n",
    "num_regions = len(regions_to_scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-testament",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Extract Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "public-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Session and Retry object to manage the quota Craigslist imposes on HTTP get requests within a certain time period \n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "committed-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Walk through each region in our list of regions_to_scrape to get the HTML page corresponding to a search for \"math tutor\" in the services section\n",
    "\n",
    "# response_dict = {}\n",
    "# sleep_timer = 10\n",
    "# num_regions = len(regions_to_scrape)\n",
    "# current_time = dt.datetime.now()\n",
    "# finish_time = current_time + dt.timedelta(seconds = num_regions * sleep_timer)\n",
    "\n",
    "# print(F\"Current time is {current_time.strftime('%H:%M:%S')}\")\n",
    "# print(F\"Process will finish at {finish_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# for count, region in enumerate(regions_to_scrape):\n",
    "#     # Impose a timer to help prevent too many HTTP requests that would result in a\n",
    "#     # ban\n",
    "#     time_remaining = (num_regions * sleep_timer) - (count * sleep_timer)\n",
    "#     print(F'Time remaining: {time_remaining} seconds.')\n",
    "#     time.sleep(sleep_timer)\n",
    "    \n",
    "#     current_region = region.replace('_', '')\n",
    "#     current_response = session.get('https://' + current_region + '.craigslist.org/d/services/search/bbb?query=math%20tutor&sort=rel')\n",
    "#     response_dict[region] = current_response\n",
    "#     if count != num_regions - 1:\n",
    "#         print()\n",
    "#         print(current_region + \" response received.\")\n",
    "#         print()\n",
    "#         print(\"Waiting for next response...\")\n",
    "#     else:\n",
    "#         print()\n",
    "#         print(current_region + \" response received.  Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16391dd-e42c-4422-9a1b-61506a3da3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_region = 'sfbayarea'\n",
    "# current_response = session.get('https://' + current_region + '.craigslist.org/d/services/search/bbb?query=math%20tutor&sort=rel')\n",
    "# region_response_list = []\n",
    "# region_response_list.append(current_response)\n",
    "# #next_reponse = sf_bay_soup.find(class_='button next').get('href')\n",
    "# is_next_button = True\n",
    "# while is_next_button:\n",
    "#     try:\n",
    "#         next_response = current_response\n",
    "#         next_soup = BeautifulSoup(next_response.text, 'html.parser')\n",
    "#         html_suffix = next_soup.find(class_='button next').get('href')\n",
    "#         if html_suffix != '':\n",
    "#             new_button = 'https://sfbay.craigslist.org' + html_suffix\n",
    "#             #print(new_button)\n",
    "#             current_response = session.get(new_button)\n",
    "#             region_response_list.append(current_response)\n",
    "#         else:\n",
    "#             is_next_button = False\n",
    "#     #next_buttons.append(new_button)\n",
    "    \n",
    "#     #new_soup = BeautifulSoup(next_response.text, 'html.parser')\n",
    "#     except:\n",
    "#         pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a748d9e7-d53d-40e5-a24a-679531b7a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sf_bay_area 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "sf_bay_area 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "sf_bay_area 3 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for sf_bay_area received.  Process completed.\n"
     ]
    }
   ],
   "source": [
    "# Walk through each region in our list of regions_to_scrape to get the HTML page corresponding to a search for \"math tutor\" in the services section\n",
    "\n",
    "response_dict = {}\n",
    "sleep_timer = 10\n",
    "\n",
    "for count, region in enumerate(regions_to_scrape):\n",
    "    \n",
    "    i=1\n",
    "    current_region = region.replace('_', '')\n",
    "    current_response = session.get('https://' + current_region + '.craigslist.org/d/services/search/bbb?query=math%20tutor&sort=rel')\n",
    "    print(F\"{region} {i} response received.\")\n",
    "    print(F\"Waiting {sleep_timer} seconds...\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(sleep_timer)\n",
    "    \n",
    "    region_response_list = []\n",
    "    region_response_list.append(current_response)\n",
    "\n",
    "    \n",
    "    is_next_button = True\n",
    "    while is_next_button:\n",
    "        i+=1\n",
    "        try:\n",
    "            next_response = current_response\n",
    "            next_soup = BeautifulSoup(next_response.text, 'html.parser')\n",
    "            html_suffix = next_soup.find(class_='button next').get('href')\n",
    "            if html_suffix != '':\n",
    "                new_button = 'https://sfbay.craigslist.org' + html_suffix\n",
    "                #print(new_button)\n",
    "                current_response = session.get(new_button)\n",
    "                region_response_list.append(current_response)\n",
    "                \n",
    "                time.sleep(sleep_timer)\n",
    "                \n",
    "                print(F\"{region} {i} response received.\")\n",
    "                print(F\"Waiting {sleep_timer} seconds...\")\n",
    "                print()\n",
    "            else:\n",
    "                is_next_button = False\n",
    "                print(F\"Last response for {region} received.  Process completed.\")\n",
    "        #next_buttons.append(new_button)\n",
    "\n",
    "        #new_soup = BeautifulSoup(next_response.text, 'html.parser')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    response_dict[region] = region_response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "flush-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Walk through each region to get a list of all individual postings for math tutoring \n",
    "# # in the results page we searched up earlier.\n",
    "# posts_dict = {}\n",
    "# for region in response_dict:\n",
    "#     #current_region = region\n",
    "#     current_html_soup = BeautifulSoup(response_dict[region].text, 'html.parser')\n",
    "#     current_posts = current_html_soup.find_all('li', class_='result-row')\n",
    "#     posts_dict[region] = current_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3d3e52-075f-4d8e-8494-41ef4f040317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through each region to get a list of all individual postings for math tutoring \n",
    "# in the results page we searched up earlier.\n",
    "posts_dict = {}\n",
    "for region, responses in response_dict.items():\n",
    "    #current_region = region\n",
    "    region_posts = []\n",
    "    for response in responses:\n",
    "        current_html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        current_posts = current_html_soup.find_all('li', class_='result-row')\n",
    "        region_posts.extend(current_posts)\n",
    "    posts_dict[region] = region_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "empirical-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time is 01:44:59\n",
      "Process will finish by 02:04:59\n",
      "\n",
      "Post number 10 in sf_bay_area is being extracted.\n",
      "Post number 20 in sf_bay_area is being extracted.\n",
      "Post number 30 in sf_bay_area is being extracted.\n",
      "Post number 40 in sf_bay_area is being extracted.\n",
      "Post number 50 in sf_bay_area is being extracted.\n",
      "Post number 60 in sf_bay_area is being extracted.\n",
      "Post number 70 in sf_bay_area is being extracted.\n",
      "Post number 80 in sf_bay_area is being extracted.\n",
      "Post number 90 in sf_bay_area is being extracted.\n",
      "Post number 100 in sf_bay_area is being extracted.\n",
      "Post number 110 in sf_bay_area is being extracted.\n",
      "Post number 120 in sf_bay_area is being extracted.\n",
      "Post number 130 in sf_bay_area is being extracted.\n",
      "Post number 140 in sf_bay_area is being extracted.\n",
      "Post number 150 in sf_bay_area is being extracted.\n",
      "Post number 160 in sf_bay_area is being extracted.\n",
      "Post number 170 in sf_bay_area is being extracted.\n",
      "Post number 180 in sf_bay_area is being extracted.\n",
      "Post number 190 in sf_bay_area is being extracted.\n",
      "Post number 200 in sf_bay_area is being extracted.\n",
      "Post number 210 in sf_bay_area is being extracted.\n",
      "Post number 220 in sf_bay_area is being extracted.\n",
      "Post number 230 in sf_bay_area is being extracted.\n",
      "Post number 240 in sf_bay_area is being extracted.\n",
      "Post number 250 in sf_bay_area is being extracted.\n",
      "Post number 260 in sf_bay_area is being extracted.\n",
      "Post number 270 in sf_bay_area is being extracted.\n",
      "Post number 280 in sf_bay_area is being extracted.\n",
      "\n",
      "Soup objects for sf_bay_area acquired.  Process complete.\n"
     ]
    }
   ],
   "source": [
    "soup_objects_dict = {}\n",
    "\n",
    "current_time = dt.datetime.now()\n",
    "num_seconds = num_regions * 120 * 10\n",
    "max_finish_time = current_time + dt.timedelta(seconds=num_seconds)\n",
    "\n",
    "print(F\"Current time is {current_time.strftime('%H:%M:%S')}\")\n",
    "print(F\"Process will finish by {max_finish_time.strftime('%H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "for count, region in enumerate(posts_dict, start=1):\n",
    "    # Walk through each region and create a list of soup_objects to scrape from by \n",
    "    # storing them into memory.  This way we only have to send these get requests \n",
    "    # once and Craigslist doesn't ban us for sending the same https requests over \n",
    "    # and over\n",
    "    soup_objects_list = []\n",
    "    #link_list = []\n",
    "    for i, post in enumerate(posts_dict[region]):\n",
    "        # Impose a timer so that we send each get request between 5 and 10 seconds.\n",
    "        # This is again to help prevent from getting banned for too many HTTP \n",
    "        # requests.\n",
    "        random_int = random.randint(5,10)\n",
    "        time.sleep(random_int)\n",
    "        current_link = post.a.get('href')\n",
    "        #link_list.append(current_link)\n",
    "        response_object = session.get(current_link)\n",
    "        soup_object = BeautifulSoup(response_object.text, 'html.parser')\n",
    "        soup_objects_list.append(soup_object) \n",
    "        # Impose condition that every 10th post will trigger something printed\n",
    "        # to the screen.  This part of the code is a long process and I wanted\n",
    "        # something to help keep track of how much progress has been made\n",
    "        if (i !=0) and ((i-1) % 10 == 9):\n",
    "            print(F\"Post number {i} in {region} is being extracted.\")\n",
    "    \n",
    "    soup_objects_dict[region] = soup_objects_list\n",
    "    if count != len(posts_dict):\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Waiting for next region...\")\n",
    "        print()\n",
    "    else:\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-interaction",
   "metadata": {},
   "source": [
    "## Pre-processing Craigslist Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35126b65-65a6-42b2-84ae-2a544d2d86f9",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4a7a5760-2365-4de6-b16b-9e6e7e494423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "error_list_text = []\n",
    "error_list_links = []\n",
    "\n",
    "# Walk through each region that contains a list of soup objects corresponding to the # search of services for math tutors.\n",
    "for search_region in soup_objects_dict:\n",
    "    # Initialize several lists to store relevant information for analysis\n",
    "    price_list = []\n",
    "    city_list = []\n",
    "    datetime_list = []\n",
    "    body_text_list = []\n",
    "    subregion_list = []\n",
    "    region_list = []\n",
    "    link_list = []\n",
    "    search_region_price_list = []\n",
    "    \n",
    "    # Walk through each soup object in the list corresponding to the search region \n",
    "    # and get the link of the soup object to scrape from.\n",
    "    for soup in soup_objects_dict[search_region]:\n",
    "        try:\n",
    "            link = soup.find(\"meta\", property=\"og:url\")['content']\n",
    "        except:\n",
    "            # In case a link can't be found, we add the soup object to a list\n",
    "            # to inspect later and set link to 'None', which we'll use to filter\n",
    "            # these results out later\n",
    "            link = 'None'\n",
    "            error_list_links.append(soup)\n",
    "            print(\"Couldn't get link\")\n",
    "\n",
    "        # Extract region of post from Craigslist\n",
    "        post_region = soup.find_all('li',class_='crumb area')[0].find('a').get_text()\n",
    "        post_region = post_region.replace(' ', '_')\n",
    "        post_region = post_region.lower()\n",
    "        \n",
    "        if link != 'None':\n",
    "            \n",
    "            # Get text of postingbody of the post and remove unwanted text.\n",
    "            try:\n",
    "                text = soup.find('section', id='postingbody').get_text()\n",
    "                #text = text.replace('\\n', '')\n",
    "                text = text.replace(';', ',') # We do this so that we can use ; as \n",
    "                                              # a delimiter when copying data from a \n",
    "                                              # CSV file into a SQL database later.\n",
    "                text = text.replace('QR Code Link to This Post', '') # We do this \n",
    "                                                                     # because this\n",
    "                                                                     # text from one\n",
    "                                                                     # post in\n",
    "                                                                     # particular was                                                                      # giving me \n",
    "                                                                     # trouble and\n",
    "                                                                     # the best way I \n",
    "                                                                     # could find to \n",
    "                                                                     # handle it was \n",
    "                                                                     # to remove the \n",
    "                                                                     # text.\n",
    "                text = text.replace(u'\\xa0', u' ')\n",
    "                \n",
    "                text_not_duplicated = text not in body_text_list\n",
    "            \n",
    "            except:\n",
    "                error_list_text.append(soup)\n",
    "                text = 'None'\n",
    "                text_not_duplicated = True\n",
    "                #body_text_list.append(text)\n",
    "                print(\"Couldn't get text\")\n",
    "            \n",
    "            # Only let posts through that have a link to scrape from and those posts \n",
    "            # where the region of the post matches the region of the search.  Some CL \n",
    "            # search results are for neighboring areas, ones that come up in a different\n",
    "            # region than the region your search was from, which leads to duplicates in \n",
    "            # areas like Los Angeles and San Diego.  This will weed out duplicates.\n",
    "            if post_region == search_region and text_not_duplicated:\n",
    "                region_list.append(post_region)\n",
    "                link_list.append(link)\n",
    "                body_text_list.append(text)\n",
    "               \n",
    "                # Use regular expressions to find all instances of prices in the text\n",
    "                #old_prices = re.findall('(?:[\\$]{1}[,\\d]+.?\\d*)', text)\n",
    "                old_prices = re.findall('(?:[\\$]{1}[,\\d]+\\d*)', text)\n",
    "                # Alternative, if trying to capture decimals \n",
    "                # ^(?:\\${1}\\d+(?:,\\d{3})*(?:\\.{1}\\d{2}){0,1})?$\n",
    "\n",
    "                \n",
    "\n",
    "                # Intialize empty list to store the new prices after processing old\n",
    "                # prices.\n",
    "                new_prices = []\n",
    "                #print(F\"Initialized new_prices: {new_prices}\")\n",
    "                # Walk through each price in the post.\n",
    "                for price in old_prices:\n",
    "                    # Clean unwanted characters.\n",
    "                    price = price.replace('$', '')\n",
    "                    price = price.replace('/', '')\n",
    "                    price = price.replace('!', '')\n",
    "                    price = price.replace('h', '')\n",
    "                    price = price.replace('.', '')\n",
    "                    price = price.replace(')', '')\n",
    "                    price = price.replace(',', '')\n",
    "                    price = price.replace('>', '')\n",
    "                    price = price.rstrip()   \n",
    "                    # Some tutors give prices as a range ie '$30-40'.  In order to\n",
    "                    # work with this data, I split based on the hyphen, then I can \n",
    "                    # use each price individually.\n",
    "                    split_prices = price.split('-')\n",
    "                #print(F\"Here are the old_prices: {old_prices}\")\n",
    "                #print(F\"Here are the split_prices: {split_prices}\")\n",
    "\n",
    "                    # Walk through each price in the posting, after any necessary splits \n",
    "                    # have been made.\n",
    "                    for p in split_prices:\n",
    "                        # Only proceed if the post contained prices, ie if p is a non-\n",
    "                        # empty string.\n",
    "                        if len(p)!=0:\n",
    "\n",
    "                            try:\n",
    "                                # Convert string price to int.\n",
    "                                new_int = int(p)\n",
    "                                if new_int <= 200:\n",
    "                                    new_prices.append(new_int)\n",
    "\n",
    "                            except:\n",
    "                                # Show which prices aren't able to convert to an int and \n",
    "                                # the post they came from so we can isolate and fix the \n",
    "                                # issue.\n",
    "                                print(F'Error converting this price: {p}')\n",
    "                                print(split_prices)\n",
    "                                print()\n",
    "                                print('Here is the text of the post:')\n",
    "                                print()\n",
    "                                print(text)\n",
    "                                print('-'*50)\n",
    "                                print()\n",
    "                                # Set prices that can't be covered to NaN so the process \n",
    "                                # can finish.\n",
    "                                new_prices.append(np.nan) \n",
    "                #print(F\"Here are the processed new_prices: {new_prices}\")\n",
    "                        #print(len(new_prices))\n",
    "\n",
    "                    \n",
    "                # Append prices before they're processed to a separate list, in case we\n",
    "                # need to isolate issues and fix them later.\n",
    "                \n",
    "                search_region_price_list.append(new_prices)\n",
    "\n",
    "                # For posts that had no prices listed, we append new_prices with \"None\"\n",
    "                if len(new_prices)==0:\n",
    "                    #price_list.append('None')\n",
    "                    price_list.append(np.nan)\n",
    "                # For posts that had a single price, we use it.\n",
    "                elif len(new_prices)==1:\n",
    "                    price_list.append(new_prices[0])\n",
    "                # For posts that contained two prices, we average them.  This helps with \n",
    "                # posts that give a range of prices (ie $25-30).\n",
    "                elif len(new_prices)==2:\n",
    "                    avg_price_2 = np.average(new_prices)\n",
    "                    price_list.append(avg_price_2)\n",
    "                # If a post has more than 3 prices, we append them, but this means we \n",
    "                # have to inspect them manually and deal with them later.\n",
    "                else:\n",
    "                    #price_list.append(new_prices)\n",
    "                    price_list.append(np.nan)\n",
    "                #print(price_list)\n",
    "                \n",
    "                \n",
    "                # Get city information for each posting.\n",
    "                try:\n",
    "                    city = soup.find(class_='postingtitletext').small.get_text()\n",
    "\n",
    "                    # Because of the way CL operates, one has to choose a city from a\n",
    "                    # radio button list that CL provides when one creates a post to offer \n",
    "                    # a service, however later, there's a field where they can type in \n",
    "                    # any city they want.  Many people will randomly choose a city from \n",
    "                    # the radio button list, but then  post their city as \"online\".  This \n",
    "                    # makes sure we capture them. \n",
    "                    re_pattern = re.compile('online')\n",
    "                    online_flag = re.search(re_pattern, city.lower())\n",
    "                    if online_flag:\n",
    "                        city_list.append('Online')\n",
    "                    else:\n",
    "                        # Strip out leading and trailing white spaces, replace\n",
    "                        # parentheses, and capitalize each word in the str.\n",
    "                        city = city.strip()\n",
    "                        city = city.replace('(', '').replace(')', '')        \n",
    "                        city = city.title()\n",
    "                        city_list.append(city)\n",
    "                except:\n",
    "                    # If a post has no city information, use None\n",
    "                    city_list.append('None')\n",
    "\n",
    "                # Extract subregion of Craigslist that the post was made in.\n",
    "                # This will allow for comparison of prices across different cities\n",
    "                # within the same metropolitan sub_region.\n",
    "                try:\n",
    "                    subregion = soup.find_all('li', class_='crumb subarea')[0].find('a').get_text()\n",
    "                    subregion = subregion.title()\n",
    "                    subregion_list.append(subregion)\n",
    "                except:\n",
    "                    subregion_list.append('None')\n",
    "\n",
    "\n",
    "                # Extract time the posting was made.\n",
    "                try:\n",
    "                    dt_object = soup.find('time')['datetime']\n",
    "                    datetime_list.append(dt_object)\n",
    "                except:\n",
    "                    datetime_list.append('None')\n",
    "            else:\n",
    "                pass\n",
    "    #print(price_list)\n",
    "    # Create temporary df to store results for each region\n",
    "    temp_df = pd.DataFrame(data=zip(datetime_list,\n",
    "                                    link_list, \n",
    "                                    price_list, \n",
    "                                    city_list, \n",
    "                                    subregion_list, \n",
    "                                    region_list, \n",
    "                                    body_text_list,\n",
    "                                    search_region_price_list),\n",
    "                        columns=['date_posted', \n",
    "                                 'link', \n",
    "                                 'price', \n",
    "                                 'city', \n",
    "                                 'subregion', \n",
    "                                 'region', \n",
    "                                 'post_text',\n",
    "                                 'price_list']\n",
    "                          )\n",
    "    \n",
    "    # Append each temporary df to a list, which we can concatenate into one larger \n",
    "    # df, later.\n",
    "    df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59649d42-cc4c-4710-94c5-cf013f8594aa",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "modular-jordan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_list = []\n",
    "# error_list_text = []\n",
    "# error_list_links = []\n",
    "\n",
    "# # Walk through each region that contains a list of soup objects corresponding to the # search of services for math tutors.\n",
    "# for search_region in soup_objects_dict:\n",
    "#     # Initialize several lists to store relevant information for analysis\n",
    "#     price_list = []\n",
    "#     city_list = []\n",
    "#     datetime_list = []\n",
    "#     body_text_list = []\n",
    "#     subregion_list = []\n",
    "#     region_list = []\n",
    "#     link_list = []\n",
    "#     search_region_price_list = []\n",
    "    \n",
    "#     # Walk through each soup object in the list corresponding to the search region \n",
    "#     # and get the link of the soup object to scrape from.\n",
    "#     for soup in soup_objects_dict[search_region]:\n",
    "#         try:\n",
    "#             link = soup.find(\"meta\", property=\"og:url\")['content']\n",
    "#         except:\n",
    "#             # In case a link can't be found, we add the soup object to a list\n",
    "#             # to inspect later and set link to 'None', which we'll use to filter\n",
    "#             # these results out later\n",
    "#             link = 'None'\n",
    "#             error_list_links.append(soup)\n",
    "#             print(\"Couldn't get link\")\n",
    "\n",
    "#         # Extract region of post from Craigslist\n",
    "#         post_region = soup.find_all('li',class_='crumb area')[0].find('a').get_text()\n",
    "#         post_region = post_region.replace(' ', '_')\n",
    "#         post_region = post_region.lower()\n",
    "        \n",
    "#         # Only let posts through that have a link to scrape from and those posts \n",
    "#         # where the region of the post matches the region of the search.  Some CL \n",
    "#         # search results are for neighboring areas, ones that come up in a different\n",
    "#         # region than the region your search was from, which leads to duplicates in \n",
    "#         # areas like Los Angeles and San Diego.  This will weed out duplicates.\n",
    "#         if post_region == search_region and link != 'None':\n",
    "#             region_list.append(post_region)\n",
    "#             link_list.append(link)\n",
    "\n",
    "#             # Get text of postingbody of the post and remove unwanted text.\n",
    "#             try:\n",
    "#                 text = soup.find('section', id='postingbody').get_text()\n",
    "#                 #text = text.replace('\\n', '')\n",
    "#                 text = text.replace(';', ',') # We do this so that we can use ; as \n",
    "#                                               # a delimiter when copying data from a \n",
    "#                                               # CSV file into a SQL database later.\n",
    "#                 text = text.replace('QR Code Link to This Post', '') # We do this \n",
    "#                                                                      # because this\n",
    "#                                                                      # text from one\n",
    "#                                                                      # post in\n",
    "#                                                                      # particular was                                                                      # giving me \n",
    "#                                                                      # trouble and\n",
    "#                                                                      # the best way I \n",
    "#                                                                      # could find to \n",
    "#                                                                      # handle it was \n",
    "#                                                                      # to remove the \n",
    "#                                                                      # text.\n",
    "#                 text = text.replace(u'\\xa0', u' ')\n",
    "#                 body_text_list.append(text)\n",
    "                \n",
    "#             except:\n",
    "#                 error_list_text.append(soup)\n",
    "#                 body_text_list.append('None')\n",
    "#                 print(\"Couldn't get text\")\n",
    "\n",
    "#             # Use regular expressions to find all instances of prices in the text\n",
    "#             #old_prices = re.findall('(?:[\\$]{1}[,\\d]+.?\\d*)', text)\n",
    "#             old_prices = re.findall('(?:[\\$]{1}[,\\d]+\\d*)', text)\n",
    "#             # Alternative, if trying to capture decimals \n",
    "#             # ^(?:\\${1}\\d+(?:,\\d{3})*(?:\\.{1}\\d{2}){0,1})?$\n",
    "            \n",
    "#             # Append prices before they're processed to a separate list, in case we\n",
    "#             # need to isolate issues and fix them later.\n",
    "#             search_region_price_list.append(old_prices)\n",
    "            \n",
    "#             # Intialize empty list to store the new prices after processing old\n",
    "#             # prices.\n",
    "#             new_prices = []\n",
    "\n",
    "#             # Walk through each price in the post.\n",
    "#             for price in old_prices:\n",
    "#                 # Clean unwanted characters.\n",
    "#                 price = price.replace('$', '')\n",
    "#                 price = price.replace('/', '')\n",
    "#                 price = price.replace('!', '')\n",
    "#                 price = price.replace('h', '')\n",
    "#                 price = price.replace('.', '')\n",
    "#                 price = price.replace(')', '')\n",
    "#                 price = price.replace(',', '')\n",
    "#                 price = price.replace('>', '')\n",
    "#                 price = price.rstrip()   \n",
    "#                 # Some tutors give prices as a range ie '$30-40'.  In order to\n",
    "#                 # work with this data, I split based on the hyphen, then I can \n",
    "#                 # use each price individually.\n",
    "#                 split_prices = price.split('-')\n",
    "\n",
    "#                 # Walk through each price in the posting, after any necessary splits \n",
    "#                 # have been made.\n",
    "#                 for p in split_prices:\n",
    "#                     # Only proceed if the post contained prices, ie if p is a non-\n",
    "#                     # empty string.\n",
    "#                     if len(p)!=0:\n",
    "                        \n",
    "#                         try:\n",
    "#                             # Convert string price to int.\n",
    "#                             new_int = int(p)\n",
    "#                             new_prices.append(new_int)\n",
    "                        \n",
    "#                         except:\n",
    "#                             # Show which prices aren't able to convert to an int and \n",
    "#                             # the post they came from so we can isolate and fix the \n",
    "#                             # issue.\n",
    "#                             print(F'Error converting this price: {p}')\n",
    "#                             print(old_prices)\n",
    "#                             print()\n",
    "#                             print('Here is the text of the post:')\n",
    "#                             print()\n",
    "#                             print(text)\n",
    "#                             print('-'*50)\n",
    "#                             print()\n",
    "#                             # Set prices that can't be covered to NaN so the process \n",
    "#                             # can finish.\n",
    "#                             new_prices.append(np.nan) \n",
    "\n",
    "                            \n",
    "#             # For posts that had no prices listed, we append new_prices with \"None\"\n",
    "#             if len(new_prices)==0:\n",
    "#                 #price_list.append('None')\n",
    "#                 price_list.append(np.nan)\n",
    "#             # For posts that had a single price, we use it.\n",
    "#             elif len(new_prices)==1:\n",
    "#                 price_list.append(new_prices[0])\n",
    "#             # For posts that contained two prices, we average them.  This helps with \n",
    "#             # posts that give a range of prices (ie $25-30).\n",
    "#             elif len(new_prices)==2:\n",
    "#                 avg_price_2 = np.average(new_prices)\n",
    "#                 price_list.append(avg_price_2)\n",
    "#             # If a post has more than 3 prices, we append them, but this means we \n",
    "#             # have to inspect them manually and deal with them later.\n",
    "#             else:\n",
    "#                 #price_list.append(new_prices)\n",
    "#                 price_list.append(np.nan)\n",
    "\n",
    "#             # Get city information for each posting.\n",
    "#             try:\n",
    "#                 city = soup.find(class_='postingtitletext').small.get_text()\n",
    "                \n",
    "#                 # Because of the way CL operates, one has to choose a city from a\n",
    "#                 # radio button list that CL provides when one creates a post to offer \n",
    "#                 # a service, however later, there's a field where they can type in \n",
    "#                 # any city they want.  Many people will randomly choose a city from \n",
    "#                 # the radio button list, but then  post their city as \"online\".  This \n",
    "#                 # makes sure we capture them. \n",
    "#                 re_pattern = re.compile('online')\n",
    "#                 online_flag = re.search(re_pattern, city.lower())\n",
    "#                 if online_flag:\n",
    "#                     city_list.append('Online')\n",
    "#                 else:\n",
    "#                     # Strip out leading and trailing white spaces, replace\n",
    "#                     # parentheses, and capitalize each word in the str.\n",
    "#                     city = city.strip()\n",
    "#                     city = city.replace('(', '').replace(')', '')        \n",
    "#                     city = city.title()\n",
    "#                     city_list.append(city)\n",
    "#             except:\n",
    "#                 # If a post has no city information, use None\n",
    "#                 city_list.append('None')\n",
    "\n",
    "#             # Extract subregion of Craigslist that the post was made in.\n",
    "#             # This will allow for comparison of prices across different cities\n",
    "#             # within the same metropolitan sub_region.\n",
    "#             try:\n",
    "#                 subregion = soup.find_all('li', class_='crumb subarea')[0].find('a').get_text()\n",
    "#                 subregion = subregion.title()\n",
    "#                 subregion_list.append(subregion)\n",
    "#             except:\n",
    "#                 subregion_list.append('None')\n",
    "\n",
    "\n",
    "#             # Extract time the posting was made.\n",
    "#             try:\n",
    "#                 dt_object = soup.find('time')['datetime']\n",
    "#                 datetime_list.append(dt_object)\n",
    "#             except:\n",
    "#                 datetime_list.append('None')\n",
    "#         else:\n",
    "#             pass\n",
    "    \n",
    "#     # Create temporary df to store results for each region\n",
    "#     temp_df = pd.DataFrame(data=zip(datetime_list,\n",
    "#                                     link_list, \n",
    "#                                     price_list, \n",
    "#                                     city_list, \n",
    "#                                     subregion_list, \n",
    "#                                     region_list, \n",
    "#                                     body_text_list,\n",
    "#                                     search_region_price_list),\n",
    "#                         columns=['date_posted', \n",
    "#                                  'link', \n",
    "#                                  'price', \n",
    "#                                  'city', \n",
    "#                                  'subregion', \n",
    "#                                  'region', \n",
    "#                                  'post_text',\n",
    "#                                  'price_list']\n",
    "#                           )\n",
    "    \n",
    "#     # Append each temporary df to a list, which we can concatenate into one larger \n",
    "#     # df, later.\n",
    "#     df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "nearby-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for errors in getting text from a post, or from getting the URL of a post.\n",
    "len(error_list_text), len(error_list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "endangered-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 8)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dfs for each region into one larger df and check its shape.\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "spanish-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    202\n",
       "Name: post_text, dtype: int64"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get date of html request to label our output with.\n",
    "date_of_html_request = str(dt.date.today())\n",
    "\n",
    "# Include the date posts were scraped on to track tutoring prices over time.\n",
    "df['posts_scraped_on'] = date_of_html_request\n",
    "\n",
    "# Count duplicates.\n",
    "df['post_text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "661dadc6-b5d7-4779-ab8c-c1086a23c774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 9)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['price'].isnull()==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "4716794b-73e1-4381-93ea-bc0d1e2f422f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>subregion</th>\n",
       "      <th>region</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "      <th>posts_scraped_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-06T12:46:35-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/sunnyva...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...</td>\n",
       "      <td>[90, 60, 40]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-06T10:55:52-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n☎️  (646) 450-2967 -Molly\\nSubject...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-06T10:04:30-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/pen/lss/d/stanfor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nProfessional full-time tutor availab...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-06T09:46:37-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/math-ph...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Richmond / Seacliff</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\n*****I am currently offering both Zo...</td>\n",
       "      <td>[120]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-06T09:39:16-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nProfessional full-time tutor availab...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date_posted  \\\n",
       "0  2022-01-06T12:46:35-0800   \n",
       "1  2022-01-06T10:55:52-0800   \n",
       "2  2022-01-06T10:04:30-0800   \n",
       "3  2022-01-06T09:46:37-0800   \n",
       "4  2022-01-06T09:39:16-0800   \n",
       "\n",
       "                                                link  price  \\\n",
       "0  https://sfbay.craigslist.org/sby/lss/d/sunnyva...    NaN   \n",
       "1  https://sfbay.craigslist.org/sfc/lss/d/san-fra...    NaN   \n",
       "2  https://sfbay.craigslist.org/pen/lss/d/stanfor...    NaN   \n",
       "3  https://sfbay.craigslist.org/sfc/lss/d/math-ph...  120.0   \n",
       "4  https://sfbay.craigslist.org/sfc/lss/d/san-fra...    NaN   \n",
       "\n",
       "                  city      subregion       region  \\\n",
       "0            Cupertino      South Bay  sf_bay_area   \n",
       "1                 None  San Francisco  sf_bay_area   \n",
       "2                 None      Peninsula  sf_bay_area   \n",
       "3  Richmond / Seacliff  San Francisco  sf_bay_area   \n",
       "4                 None  San Francisco  sf_bay_area   \n",
       "\n",
       "                                           post_text    price_list  \\\n",
       "0  \\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...  [90, 60, 40]   \n",
       "1  \\n\\n\\n\\n\\n\\n☎️  (646) 450-2967 -Molly\\nSubject...            []   \n",
       "2  \\n\\n\\n\\n\\nProfessional full-time tutor availab...            []   \n",
       "3  \\n\\n\\n\\n\\n*****I am currently offering both Zo...         [120]   \n",
       "4  \\n\\n\\n\\n\\nProfessional full-time tutor availab...            []   \n",
       "\n",
       "  posts_scraped_on  \n",
       "0       2022-01-07  \n",
       "1       2022-01-07  \n",
       "2       2022-01-07  \n",
       "3       2022-01-07  \n",
       "4       2022-01-07  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-compound",
   "metadata": {},
   "source": [
    "### Dropping Duplicates or posts that contained no prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "gorgeous-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of duplicate results, then drop them and reset indices.\n",
    "duplicate_indices = df[df['post_text'].duplicated()==True].index\n",
    "df_no_dups = df.drop(index=duplicate_indices)\n",
    "df_no_dups = df_no_dups.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a767e25d-833d-4374-b2f8-dfd1b6447536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>subregion</th>\n",
       "      <th>region</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "      <th>posts_scraped_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-06T12:46:35-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/sunnyva...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...</td>\n",
       "      <td>[90, 60, 40]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-06T10:55:52-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n☎️  (646) 450-2967 -Molly\\nSubject...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-06T10:04:30-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/pen/lss/d/stanfor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nProfessional full-time tutor availab...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-06T09:46:37-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/math-ph...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Richmond / Seacliff</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\n*****I am currently offering both Zo...</td>\n",
       "      <td>[120]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-06T09:39:16-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nProfessional full-time tutor availab...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date_posted  \\\n",
       "0  2022-01-06T12:46:35-0800   \n",
       "1  2022-01-06T10:55:52-0800   \n",
       "2  2022-01-06T10:04:30-0800   \n",
       "3  2022-01-06T09:46:37-0800   \n",
       "4  2022-01-06T09:39:16-0800   \n",
       "\n",
       "                                                link  price  \\\n",
       "0  https://sfbay.craigslist.org/sby/lss/d/sunnyva...    NaN   \n",
       "1  https://sfbay.craigslist.org/sfc/lss/d/san-fra...    NaN   \n",
       "2  https://sfbay.craigslist.org/pen/lss/d/stanfor...    NaN   \n",
       "3  https://sfbay.craigslist.org/sfc/lss/d/math-ph...  120.0   \n",
       "4  https://sfbay.craigslist.org/sfc/lss/d/san-fra...    NaN   \n",
       "\n",
       "                  city      subregion       region  \\\n",
       "0            Cupertino      South Bay  sf_bay_area   \n",
       "1                 None  San Francisco  sf_bay_area   \n",
       "2                 None      Peninsula  sf_bay_area   \n",
       "3  Richmond / Seacliff  San Francisco  sf_bay_area   \n",
       "4                 None  San Francisco  sf_bay_area   \n",
       "\n",
       "                                           post_text    price_list  \\\n",
       "0  \\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...  [90, 60, 40]   \n",
       "1  \\n\\n\\n\\n\\n\\n☎️  (646) 450-2967 -Molly\\nSubject...            []   \n",
       "2  \\n\\n\\n\\n\\nProfessional full-time tutor availab...            []   \n",
       "3  \\n\\n\\n\\n\\n*****I am currently offering both Zo...         [120]   \n",
       "4  \\n\\n\\n\\n\\nProfessional full-time tutor availab...            []   \n",
       "\n",
       "  posts_scraped_on  \n",
       "0       2022-01-07  \n",
       "1       2022-01-07  \n",
       "2       2022-01-07  \n",
       "3       2022-01-07  \n",
       "4       2022-01-07  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "434e8abf-7b66-4232-9059-93272966788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dups['len_of_price_list'] = df_no_dups['price_list'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "guided-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out results that don't have a price and reset indices.\n",
    "df_with_prices = df_no_dups[df_no_dups['len_of_price_list'] > 0]\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "273eca8b-0205-4458-9536-0cebd167096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 10)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "5118dd4a-b106-4d3c-830b-42db498f7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              [90, 60, 40]\n",
       "6                              [30, 35, 45]\n",
       "7                          [40, 40, 45, 45]\n",
       "14                         [30, 30, 60, 90]\n",
       "17                            [60, 50, 100]\n",
       "26              [100, 115, 130, 65, 30, 60]\n",
       "29                     [40, 40, 40, 40, 40]\n",
       "31                             [50, 10, 50]\n",
       "32                             [50, 10, 50]\n",
       "33                           [50, 100, 135]\n",
       "34                             [30, 35, 45]\n",
       "36                             [30, 50, 60]\n",
       "40    [40, 80, 40, 10, 40, 30, 40, 80, 100]\n",
       "48                             [90, 60, 40]\n",
       "49                             [25, 45, 25]\n",
       "50                 [35, 35, 40, 40, 55, 80]\n",
       "51                 [35, 35, 40, 40, 55, 80]\n",
       "55                     [40, 40, 40, 40, 40]\n",
       "58                         [45, 55, 40, 50]\n",
       "66                             [65, 55, 55]\n",
       "69                         [20, 25, 30, 30]\n",
       "74                             [65, 55, 55]\n",
       "Name: price_list, dtype: object"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the entries with 3 or more prices listed, let's investigate why\n",
    "df_with_prices[df_with_prices['len_of_price_list'] >= 3]['price_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb0dec-46fb-4693-b660-10214d8f6b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f718cae-2b3a-4114-959d-bd75897770d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "30016071-166a-4547-8427-dda18ecd0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "b3a49309-3138-467e-aaa0-75c18e28125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_comparison_all = df_with_prices['post_text']\n",
    "vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "tfidf_all = vect.fit_transform(text_for_comparison_all)\n",
    "pairwise_similarity_all = tfidf_all * tfidf_all.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c32c21bb-4dfd-407f-ace7-638919e715c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_array_all = pairwise_similarity_all.toarray()\n",
    "np.fill_diagonal(pairwise_array_all, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a9eac825-1b84-4d75-ba69-0c097dcda621",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_where_arr = np.argwhere(pairwise_array_all > 0.9)\n",
    "#arg_where_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b0108-5e04-4def-8dc9-4b6bfc3074da",
   "metadata": {},
   "source": [
    "### AMAZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "73fab42c-c87a-4bb8-aba1-ebdee2d49820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row_idx = []\n",
    "dup_row_idx = []\n",
    "for row in arg_where_arr:\n",
    "    current_idx = row[0]\n",
    "    #print(F\"Current row: {row}, Current idx: {current_idx}\")\n",
    "    duplicate_list = []\n",
    "    if current_idx in df_row_idx:\n",
    "        continue\n",
    "    else:\n",
    "        df_row_idx.append(current_idx)\n",
    "    for other_row in arg_where_arr:\n",
    "        other_idx = other_row[1]\n",
    "        #print(F\"Here's the other_row: {other_row}, Other idx: {other_idx}\")\n",
    "        if current_idx == other_row[0]:\n",
    "            duplicate_list.append(other_idx)\n",
    "    #print(F\"This is the current dup_list: {duplicate_list}\")\n",
    "    #print()\n",
    "    dup_row_idx.append(duplicate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be08c90-cd70-4233-bd42-d06363c6aa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "549352ad-60c4-491b-9d5e-0b992d6cab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_prices['match'] = np.array(df_with_prices.index.values, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "46266bca-e7b0-474a-a5d0-3390195de079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_prices['match'] = df_with_prices['match'].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "e5dfacdf-89b4-4318-a5fc-5031d512625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rancher/opt/anaconda3/envs/ox/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n",
      "/Users/rancher/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/internals/blocks.py:937: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n"
     ]
    }
   ],
   "source": [
    "df_with_prices.iloc[df_row_idx, 10] = dup_row_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "bd7d8f9a-7ac0-4e70-955b-3bd619fc47a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [48]\n",
       "1              [1]\n",
       "2              [2]\n",
       "3             [62]\n",
       "4              [4]\n",
       "          ...     \n",
       "71            [37]\n",
       "72            [72]\n",
       "73    [46, 67, 70]\n",
       "74            [66]\n",
       "75            [64]\n",
       "Name: match, Length: 76, dtype: object"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "f05a6d76-1e8a-44d9-b458-1dbf3b3e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i, row in df_with_prices.iterrows():\n",
    "    indices.append(i)\n",
    "    df_with_prices = df_with_prices.drop(\n",
    "        index=[item for item in row[\"match\"] if item not in indices], errors=\"ignore\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "858d2c1b-b43b-4fb2-8e8e-a4771b77046b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 11)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "20b3acba-5d0b-4841-9a88-0c1703bd879c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              [90, 60, 40]\n",
       "6                              [30, 35, 45]\n",
       "7                          [40, 40, 45, 45]\n",
       "14                         [30, 30, 60, 90]\n",
       "17                            [60, 50, 100]\n",
       "26              [100, 115, 130, 65, 30, 60]\n",
       "29                     [40, 40, 40, 40, 40]\n",
       "31                             [50, 10, 50]\n",
       "33                           [50, 100, 135]\n",
       "36                             [30, 50, 60]\n",
       "40    [40, 80, 40, 10, 40, 30, 40, 80, 100]\n",
       "49                             [25, 45, 25]\n",
       "50                 [35, 35, 40, 40, 55, 80]\n",
       "58                         [45, 55, 40, 50]\n",
       "66                             [65, 55, 55]\n",
       "69                         [20, 25, 30, 30]\n",
       "Name: price_list, dtype: object"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the entries with 3 or more prices listed, let's investigate why\n",
    "df_with_prices[df_with_prices['len_of_price_list'] >= 3]['price_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b463769-43d6-4db8-b69c-5ea6ebb27ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e86f5-b4b3-4a17-bfb8-e3b45782e1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a45a7b-8854-4326-a68f-dd27fa125dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0f4e5-e2fa-4360-8567-eef50a8e99e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1a1a3-936c-4616-82af-9537b62fdab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79aafc1a-7f65-4efd-ba62-74dd91fceb15",
   "metadata": {},
   "source": [
    "### ABOVE IS AMAZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7e31c654-154e-4e5a-a1ae-8fa5e8615dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 48],\n",
       "       [ 3, 62],\n",
       "       [ 6, 34],\n",
       "       [10, 18],\n",
       "       [10, 18],\n",
       "       [20, 61],\n",
       "       [29, 55],\n",
       "       [31, 32],\n",
       "       [31, 32],\n",
       "       [ 6, 34],\n",
       "       [37, 71],\n",
       "       [39, 46],\n",
       "       [39, 70],\n",
       "       [41, 54],\n",
       "       [41, 65],\n",
       "       [39, 46],\n",
       "       [46, 70],\n",
       "       [46, 73],\n",
       "       [ 0, 48],\n",
       "       [50, 51],\n",
       "       [50, 51],\n",
       "       [41, 54],\n",
       "       [54, 65],\n",
       "       [29, 55],\n",
       "       [20, 61],\n",
       "       [ 3, 62],\n",
       "       [64, 75],\n",
       "       [41, 65],\n",
       "       [54, 65],\n",
       "       [66, 74],\n",
       "       [67, 70],\n",
       "       [67, 73],\n",
       "       [39, 70],\n",
       "       [46, 70],\n",
       "       [67, 70],\n",
       "       [70, 73],\n",
       "       [37, 71],\n",
       "       [46, 73],\n",
       "       [67, 73],\n",
       "       [70, 73],\n",
       "       [66, 74],\n",
       "       [64, 75]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_arg_where_arr = np.sort(arg_where_arr,axis=-1)\n",
    "sorted_arg_where_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "888b84ef-d10f-4286-9412-0db97a49856f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0, 48],\n",
       "        [ 3, 62],\n",
       "        [ 6, 34],\n",
       "        [10, 18],\n",
       "        [20, 61],\n",
       "        [29, 55],\n",
       "        [31, 32],\n",
       "        [37, 71],\n",
       "        [39, 46],\n",
       "        [39, 70],\n",
       "        [41, 54],\n",
       "        [41, 65],\n",
       "        [46, 70],\n",
       "        [46, 73],\n",
       "        [50, 51],\n",
       "        [54, 65],\n",
       "        [64, 75],\n",
       "        [66, 74],\n",
       "        [67, 70],\n",
       "        [67, 73],\n",
       "        [70, 73]]),\n",
       " array([ 0,  1,  2,  3,  5,  6,  7, 10, 11, 12, 13, 14, 16, 17, 19, 22, 26,\n",
       "        29, 30, 31, 35]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted_arg_where_arr = np.sort(arg_where_arr,axis=-1)\n",
    "unique_1, idx_1 = np.unique(sorted_arg_where_arr, return_index = True, axis=0)\n",
    "unique_1, idx_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f7485e67-92b2-4ce7-9a32-0976d5abc075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  6, 10, 18, 20, 29, 31, 32, 34, 37, 39, 41, 46, 48, 50, 51,\n",
       "       54, 55, 61, 62, 64, 65, 66, 67, 70, 71, 73, 74, 75])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_where_arr = np.argwhere(pairwise_array_all > 0.9)\n",
    "unique_2 = np.unique(arg_where_arr)\n",
    "unique_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f56e53-741a-4056-9fc6-3bf07aa2420d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89764bcd-dc82-430d-95b8-9b02b3e11375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5f085c21-df6d-4328-bcfd-68b79095a855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>subregion</th>\n",
       "      <th>region</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "      <th>posts_scraped_on</th>\n",
       "      <th>len_of_price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-06T12:46:35-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/sunnyva...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...</td>\n",
       "      <td>[90, 60, 40]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-05T12:44:14-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/pen/lss/d/stanfor...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nI am an experienced male tutor with ...</td>\n",
       "      <td>[50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-01-05T08:22:37-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/la-hond...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nFeel free to text me at 909-640-3570...</td>\n",
       "      <td>[30, 35, 45]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-01-04T09:20:40-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nABOUT ME\\n\\nI'm a math and science t...</td>\n",
       "      <td>[60, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-12-28T09:51:28-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nABOUT ME\\n\\nI'm a math and science t...</td>\n",
       "      <td>[50, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-12-27T12:56:25-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/san-jos...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello All,\\n\\nMy name is Qusai. I ha...</td>\n",
       "      <td>[45]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-12-16T08:27:03-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/torranc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\n★★ College Math Tutor★ Calculus I ,I...</td>\n",
       "      <td>[40, 40, 40, 40, 40]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022-01-05T18:45:34-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richmond / Seacliff</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nTutoring rate. $50/hr, Group Rate, $...</td>\n",
       "      <td>[50, 10, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022-01-05T18:43:39-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/millbra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richmond / Seacliff</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nTutoring rate. $50/hr, Group Rate, $...</td>\n",
       "      <td>[50, 10, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021-12-14T13:24:02-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/la-hond...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nFeel free to text me at 909-640-3570...</td>\n",
       "      <td>[30, 35, 45]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022-01-04T05:54:43-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Ingleside / Sfsu / Ccsf</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...</td>\n",
       "      <td>[80, 120]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022-01-03T19:21:42-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/los-gat...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>San Jose North</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...</td>\n",
       "      <td>[40, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022-01-03T11:45:53-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Haight Ashbury</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[150]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2022-01-02T13:14:55-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/san-jos...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>San Jose North</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...</td>\n",
       "      <td>[50, 60]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-12-12T16:32:16-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/sunnyva...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...</td>\n",
       "      <td>[90, 60, 40]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2021-12-30T17:31:58-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/mountai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello,\\n\\nI am an engineering workin...</td>\n",
       "      <td>[35, 35, 40, 40, 55, 80]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2021-12-30T17:24:03-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nob Hill</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello, \\n\\nI am an engineering worki...</td>\n",
       "      <td>[35, 35, 40, 40, 55, 80]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2021-12-29T18:24:30-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Haight Ashbury</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[200]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2021-12-11T07:53:23-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/torranc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\n★★ College Math Tutor★ Calculus I ,I...</td>\n",
       "      <td>[40, 40, 40, 40, 40]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2021-12-08T13:38:26-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/san-jos...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello All,\\n\\nMy name is Qusai. I ha...</td>\n",
       "      <td>[45]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2021-12-08T10:21:10-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/pen/lss/d/stanfor...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nI am an experienced male tutor with ...</td>\n",
       "      <td>[50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2022-01-03T11:17:44-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/11-onli...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>West Portal / Forest Hill</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nDo you have children who are current...</td>\n",
       "      <td>[50, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2021-12-20T10:21:37-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Haight Ashbury</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[180]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2021-12-17T10:38:11-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/pen/lss/d/burling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hillsborough</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nPark Tutoring \\n\\n\\nOne on One Tutor...</td>\n",
       "      <td>[65, 55, 55]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2021-12-16T10:18:05-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/san-jos...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>San Jose North</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...</td>\n",
       "      <td>[50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2021-12-10T09:09:59-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/san-jos...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>San Jose North</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...</td>\n",
       "      <td>[40, 50]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2021-12-09T22:19:17-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/san-fra...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Ingleside / Sfsu / Ccsf</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...</td>\n",
       "      <td>[80, 120]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2021-12-08T10:50:59-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/lss/d/san-jos...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>San Jose North</td>\n",
       "      <td>South Bay</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...</td>\n",
       "      <td>[50, 60]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2021-12-08T08:43:59-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/pen/lss/d/burling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hillsborough</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nPark Tutoring \\n\\n\\nOne on One Tutor...</td>\n",
       "      <td>[65, 55, 55]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2021-12-13T16:08:00-0800</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/lss/d/11-onli...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>West Portal / Forest Hill</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>sf_bay_area</td>\n",
       "      <td>\\n\\n\\n\\n\\nDo you have children who are current...</td>\n",
       "      <td>[45, 45]</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date_posted  \\\n",
       "0   2022-01-06T12:46:35-0800   \n",
       "3   2022-01-05T12:44:14-0800   \n",
       "6   2022-01-05T08:22:37-0800   \n",
       "10  2022-01-04T09:20:40-0800   \n",
       "18  2021-12-28T09:51:28-0800   \n",
       "20  2021-12-27T12:56:25-0800   \n",
       "29  2021-12-16T08:27:03-0800   \n",
       "31  2022-01-05T18:45:34-0800   \n",
       "32  2022-01-05T18:43:39-0800   \n",
       "34  2021-12-14T13:24:02-0800   \n",
       "37  2022-01-04T05:54:43-0800   \n",
       "39  2022-01-03T19:21:42-0800   \n",
       "41  2022-01-03T11:45:53-0800   \n",
       "46  2022-01-02T13:14:55-0800   \n",
       "48  2021-12-12T16:32:16-0800   \n",
       "50  2021-12-30T17:31:58-0800   \n",
       "51  2021-12-30T17:24:03-0800   \n",
       "54  2021-12-29T18:24:30-0800   \n",
       "55  2021-12-11T07:53:23-0800   \n",
       "61  2021-12-08T13:38:26-0800   \n",
       "62  2021-12-08T10:21:10-0800   \n",
       "64  2022-01-03T11:17:44-0800   \n",
       "65  2021-12-20T10:21:37-0800   \n",
       "66  2021-12-17T10:38:11-0800   \n",
       "67  2021-12-16T10:18:05-0800   \n",
       "70  2021-12-10T09:09:59-0800   \n",
       "71  2021-12-09T22:19:17-0800   \n",
       "73  2021-12-08T10:50:59-0800   \n",
       "74  2021-12-08T08:43:59-0800   \n",
       "75  2021-12-13T16:08:00-0800   \n",
       "\n",
       "                                                 link  price  \\\n",
       "0   https://sfbay.craigslist.org/sby/lss/d/sunnyva...    NaN   \n",
       "3   https://sfbay.craigslist.org/pen/lss/d/stanfor...   50.0   \n",
       "6   https://sfbay.craigslist.org/sby/lss/d/la-hond...    NaN   \n",
       "10  https://sfbay.craigslist.org/sfc/lss/d/san-fra...   55.0   \n",
       "18  https://sfbay.craigslist.org/sfc/lss/d/san-fra...   50.0   \n",
       "20  https://sfbay.craigslist.org/sby/lss/d/san-jos...   45.0   \n",
       "29  https://sfbay.craigslist.org/sfc/lss/d/torranc...    NaN   \n",
       "31  https://sfbay.craigslist.org/sfc/lss/d/san-fra...    NaN   \n",
       "32  https://sfbay.craigslist.org/sfc/lss/d/millbra...    NaN   \n",
       "34  https://sfbay.craigslist.org/sby/lss/d/la-hond...    NaN   \n",
       "37  https://sfbay.craigslist.org/sfc/lss/d/san-fra...  100.0   \n",
       "39  https://sfbay.craigslist.org/sby/lss/d/los-gat...   45.0   \n",
       "41  https://sfbay.craigslist.org/sfc/lss/d/san-fra...  150.0   \n",
       "46  https://sfbay.craigslist.org/sby/lss/d/san-jos...   55.0   \n",
       "48  https://sfbay.craigslist.org/sby/lss/d/sunnyva...    NaN   \n",
       "50  https://sfbay.craigslist.org/sby/lss/d/mountai...    NaN   \n",
       "51  https://sfbay.craigslist.org/sfc/lss/d/san-fra...    NaN   \n",
       "54  https://sfbay.craigslist.org/sfc/lss/d/san-fra...  200.0   \n",
       "55  https://sfbay.craigslist.org/sfc/lss/d/torranc...    NaN   \n",
       "61  https://sfbay.craigslist.org/sby/lss/d/san-jos...   45.0   \n",
       "62  https://sfbay.craigslist.org/pen/lss/d/stanfor...   50.0   \n",
       "64  https://sfbay.craigslist.org/sfc/lss/d/11-onli...   50.0   \n",
       "65  https://sfbay.craigslist.org/sfc/lss/d/san-fra...  180.0   \n",
       "66  https://sfbay.craigslist.org/pen/lss/d/burling...    NaN   \n",
       "67  https://sfbay.craigslist.org/sby/lss/d/san-jos...   50.0   \n",
       "70  https://sfbay.craigslist.org/sby/lss/d/san-jos...   45.0   \n",
       "71  https://sfbay.craigslist.org/sfc/lss/d/san-fra...  100.0   \n",
       "73  https://sfbay.craigslist.org/sby/lss/d/san-jos...   55.0   \n",
       "74  https://sfbay.craigslist.org/pen/lss/d/burling...    NaN   \n",
       "75  https://sfbay.craigslist.org/sfc/lss/d/11-onli...   45.0   \n",
       "\n",
       "                         city      subregion       region  \\\n",
       "0                   Cupertino      South Bay  sf_bay_area   \n",
       "3                   Palo Alto      Peninsula  sf_bay_area   \n",
       "6                        None      South Bay  sf_bay_area   \n",
       "10              San Francisco  San Francisco  sf_bay_area   \n",
       "18              San Francisco  San Francisco  sf_bay_area   \n",
       "20                   San Jose      South Bay  sf_bay_area   \n",
       "29                     Online  San Francisco  sf_bay_area   \n",
       "31        Richmond / Seacliff  San Francisco  sf_bay_area   \n",
       "32        Richmond / Seacliff  San Francisco  sf_bay_area   \n",
       "34                       None      South Bay  sf_bay_area   \n",
       "37    Ingleside / Sfsu / Ccsf  San Francisco  sf_bay_area   \n",
       "39             San Jose North      South Bay  sf_bay_area   \n",
       "41             Haight Ashbury  San Francisco  sf_bay_area   \n",
       "46             San Jose North      South Bay  sf_bay_area   \n",
       "48                  Cupertino      South Bay  sf_bay_area   \n",
       "50              Mountain View      South Bay  sf_bay_area   \n",
       "51                   Nob Hill  San Francisco  sf_bay_area   \n",
       "54             Haight Ashbury  San Francisco  sf_bay_area   \n",
       "55                     Online  San Francisco  sf_bay_area   \n",
       "61                   San Jose      South Bay  sf_bay_area   \n",
       "62                  Palo Alto      Peninsula  sf_bay_area   \n",
       "64  West Portal / Forest Hill  San Francisco  sf_bay_area   \n",
       "65             Haight Ashbury  San Francisco  sf_bay_area   \n",
       "66               Hillsborough      Peninsula  sf_bay_area   \n",
       "67             San Jose North      South Bay  sf_bay_area   \n",
       "70             San Jose North      South Bay  sf_bay_area   \n",
       "71    Ingleside / Sfsu / Ccsf  San Francisco  sf_bay_area   \n",
       "73             San Jose North      South Bay  sf_bay_area   \n",
       "74               Hillsborough      Peninsula  sf_bay_area   \n",
       "75  West Portal / Forest Hill  San Francisco  sf_bay_area   \n",
       "\n",
       "                                            post_text  \\\n",
       "0   \\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...   \n",
       "3   \\n\\n\\n\\n\\nI am an experienced male tutor with ...   \n",
       "6   \\n\\n\\n\\n\\nFeel free to text me at 909-640-3570...   \n",
       "10  \\n\\n\\n\\n\\nABOUT ME\\n\\nI'm a math and science t...   \n",
       "18  \\n\\n\\n\\n\\nABOUT ME\\n\\nI'm a math and science t...   \n",
       "20  \\n\\n\\n\\n\\nHello All,\\n\\nMy name is Qusai. I ha...   \n",
       "29  \\n\\n\\n\\n\\n★★ College Math Tutor★ Calculus I ,I...   \n",
       "31  \\n\\n\\n\\n\\nTutoring rate. $50/hr, Group Rate, $...   \n",
       "32  \\n\\n\\n\\n\\nTutoring rate. $50/hr, Group Rate, $...   \n",
       "34  \\n\\n\\n\\n\\nFeel free to text me at 909-640-3570...   \n",
       "37  \\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...   \n",
       "39  \\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...   \n",
       "41  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...   \n",
       "46  \\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...   \n",
       "48  \\n\\n\\n\\n\\nDon't wait to get a tutor! This appl...   \n",
       "50  \\n\\n\\n\\n\\nHello,\\n\\nI am an engineering workin...   \n",
       "51  \\n\\n\\n\\n\\nHello, \\n\\nI am an engineering worki...   \n",
       "54  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...   \n",
       "55  \\n\\n\\n\\n\\n★★ College Math Tutor★ Calculus I ,I...   \n",
       "61  \\n\\n\\n\\n\\nHello All,\\n\\nMy name is Qusai. I ha...   \n",
       "62  \\n\\n\\n\\n\\nI am an experienced male tutor with ...   \n",
       "64  \\n\\n\\n\\n\\nDo you have children who are current...   \n",
       "65  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...   \n",
       "66  \\n\\n\\n\\n\\nPark Tutoring \\n\\n\\nOne on One Tutor...   \n",
       "67  \\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...   \n",
       "70  \\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...   \n",
       "71  \\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...   \n",
       "73  \\n\\n\\n\\n\\nHi, I'm Mark, and I'm a student at U...   \n",
       "74  \\n\\n\\n\\n\\nPark Tutoring \\n\\n\\nOne on One Tutor...   \n",
       "75  \\n\\n\\n\\n\\nDo you have children who are current...   \n",
       "\n",
       "                  price_list posts_scraped_on  len_of_price_list  \n",
       "0               [90, 60, 40]       2022-01-07                  3  \n",
       "3                       [50]       2022-01-07                  1  \n",
       "6               [30, 35, 45]       2022-01-07                  3  \n",
       "10                  [60, 50]       2022-01-07                  2  \n",
       "18                  [50, 50]       2022-01-07                  2  \n",
       "20                      [45]       2022-01-07                  1  \n",
       "29      [40, 40, 40, 40, 40]       2022-01-07                  5  \n",
       "31              [50, 10, 50]       2022-01-07                  3  \n",
       "32              [50, 10, 50]       2022-01-07                  3  \n",
       "34              [30, 35, 45]       2022-01-07                  3  \n",
       "37                 [80, 120]       2022-01-07                  2  \n",
       "39                  [40, 50]       2022-01-07                  2  \n",
       "41                     [150]       2022-01-07                  1  \n",
       "46                  [50, 60]       2022-01-07                  2  \n",
       "48              [90, 60, 40]       2022-01-07                  3  \n",
       "50  [35, 35, 40, 40, 55, 80]       2022-01-07                  6  \n",
       "51  [35, 35, 40, 40, 55, 80]       2022-01-07                  6  \n",
       "54                     [200]       2022-01-07                  1  \n",
       "55      [40, 40, 40, 40, 40]       2022-01-07                  5  \n",
       "61                      [45]       2022-01-07                  1  \n",
       "62                      [50]       2022-01-07                  1  \n",
       "64                  [50, 50]       2022-01-07                  2  \n",
       "65                     [180]       2022-01-07                  1  \n",
       "66              [65, 55, 55]       2022-01-07                  3  \n",
       "67                      [50]       2022-01-07                  1  \n",
       "70                  [40, 50]       2022-01-07                  2  \n",
       "71                 [80, 120]       2022-01-07                  2  \n",
       "73                  [50, 60]       2022-01-07                  2  \n",
       "74              [65, 55, 55]       2022-01-07                  3  \n",
       "75                  [45, 45]       2022-01-07                  2  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_post_df = df_with_prices.iloc[unique_2]\n",
    "duplicated_post_df\n",
    "#duplicated_post_df.iloc[unique_1[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d830af-1a77-4b8a-8aa1-ac3aa960d94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33fa2c-25d4-4367-89c7-e65390911f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3b0dc731-d753-4322-8634-b0e03e5e18d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'match'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'match'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/jgdr2r255gz098q__v2pjv0r0000gn/T/ipykernel_15907/3613076380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     duplicated_post_df = duplicated_post_df.drop(\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"match\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'match'"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "for i, row in duplicated_post_df.iterrows():\n",
    "    indices.append(i)\n",
    "    duplicated_post_df = duplicated_post_df.drop(\n",
    "        index=[item for item in row[\"match\"] if item not in indices], errors=\"ignore\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4847bbf4-347a-4124-800b-dbeb3cc10a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e099e-7b8e-4316-9593-0f5f1336a67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfa75b-6d1e-42cd-9d88-dd916a88d978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9fb65df9-1422-4377-8dbe-62c64827a289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sby/lss/d/san-jose-math-english-biology-chemistry/7418176992.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect links manually, one by one, to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df_with_prices.iloc[73]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6f70821-477f-4603-a832-775ab281dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nABOUT ME\\n\\nI'm a math and science tutor who works with San Francisco/Daly City middle and high school students. I've been teaching for 4 years and specialize in helping high school students feel more confident in Algebra and Pre Calculus. My goal isn't to answer my students' questions as fast as possible or to do their homework, but to help them see patterns so they can solve problems when their big tests come around. \\n\\nMy experience started when I took honors and accelerated classes in middle and high school. Later in life, I graduated with a Civil Engineering degree from Santa Clara University and currently work at a Transportation Engineering company in San Francisco. I recently passed my Fundamentals of Engineering Exam where I studied multiple subjects in math and science so I am fresh on various topics.\\n\\nWhen I'm not working my day job, you can find me riding my bike, reading, or tutoring in Pre-Calculus or Japanese (as I am half Japanese/American). Tutoring brings me great joy as I love seeing my students feel more confident because they get better grades in classes they used to struggle in. \\n\\nNot only am I really good at tutoring, but I also connect easily with my students. I can provide references from previous families if needed. \\n\\nI am fully vaccinated. \\n\\nSUBJECTS\\nMath:\\n- Algebra (Pre, 1 and 2)\\n- Geometry\\n- Pre Calculus\\n- Calculus\\n- Probability and Statistics\\n\\nScience:\\n- Biology\\n- Chemistry\\n- Physics\\n- Environmental Science\\n- Circuits/Electrical\\n\\nI'm also a tutor in Japanese. Ask me for more details.\\n\\nAVAILABILITY & RATES\\nI'm generally available 7 days a week. \\nWeekdays 4-9 pm and Weekends 10 am - 8 pm.\\nI can either come to your house to tutor or we can meet at a local library\\nMy hourly rate is:\\n$60 - 80 (In Person)\\n$50-60 (Online)\\n\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.iloc[10, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce8a22a0-93e1-4ec5-9b93-3b6235b0af94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nABOUT ME\\n\\nI'm a math and science tutor who works with San Francisco/Daly City middle and high school students. I've been teaching for 4 years and specialize in helping high school students feel more confident in Algebra and Pre Calculus. My goal isn't to answer my students' questions as fast as possible or to do their homework, but to help them see patterns so they can solve problems when their big tests come around. \\n\\nMy experience started when I took honors and accelerated classes in middle and high school. Later in life, I graduated with a Civil Engineering degree from Santa Clara University and currently work at a Transportation Engineering company in San Francisco. I recently passed my Fundamentals of Engineering Exam where I studied multiple subjects in math and science so I am fresh on various topics.\\n\\nWhen I'm not working my day job, you can find me riding my bike, reading, or tutoring in Pre-Calculus or Japanese (as I am half Japanese/American). Tutoring brings me great joy as I love seeing my students feel more confident because they get better grades in classes they used to struggle in. \\n\\nNot only am I really good at tutoring, but I also connect easily with my students. I can provide references from previous families if needed. \\n\\nI am fully vaccinated. \\n\\nSUBJECTS\\nMath:\\n- Algebra (Pre, 1 and 2)\\n- Geometry\\n- Pre Calculus\\n- Calculus\\n- Probability and Statistics\\n\\nScience:\\n- Biology\\n- Chemistry\\n- Physics\\n- Environmental Science\\n- Circuits/Electrical\\n\\nI'm also a tutor in Japanese. Ask me for more details.\\n\\nAVAILABILITY & RATES\\nI'm generally available 7 days a week. \\nWeekdays 4-9 pm and Weekends 10 am - 8 pm.\\nI can either come to your house to tutor or we can meet at a local library\\nMy hourly rate is:\\n$50 - 80 (In Person)\\n$50 (Online)\\n\\n\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.iloc[18, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24237d9e-9a04-4d29-bb70-82cc4bf9857e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c236f6-0b46-4daa-ad98-b4399968b070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489027a7-7d7c-41fb-b0e8-9c8e8a3b108a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666cab00-dc63-4cfc-a328-29589ff488ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f6ad7-fc12-421e-b42a-d1e9b130dc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695e330-5271-4cb4-aa0d-f8acf838e4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4cb394-4944-4443-a5c3-6a7b890040d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e1dac-5344-4c66-8851-799d6c88f357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cfe21-6556-4c77-9ce4-a2fa3b36caae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ea2a5-6ca8-4197-918e-e05ef2f254e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d5dc1-6b45-4c6b-9876-850da05e55d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "residential-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 81 posts that had prices included and weren't duplicates.\n",
      "Only 40.10% of the posts that we scraped remain.\n"
     ]
    }
   ],
   "source": [
    "post_with_prices_count = len(df_with_prices)\n",
    "num_posts = len(df)\n",
    "\n",
    "percent_with_prices = post_with_prices_count/num_posts * 100\n",
    "\n",
    "print(F\"There were {post_with_prices_count} posts that had prices included and weren't duplicates.\")\n",
    "print(F\"Only {percent_with_prices:.2f}% of the posts that we scraped remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-start",
   "metadata": {},
   "source": [
    "### Extracting complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-measurement",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Transforming Craigslist data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-andrew",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Are there any posts that might need manual cleaning?  This would include:\n",
    "* Posts that had 3 or more prices\n",
    "* Posts that were marked as being null during pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf56c20b-219d-4dd0-a19d-4957dc46d26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[df_with_prices['len_of_price_list'] >= 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "statutory-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep track of which prices are still lists, as opposed to single values, we will \n",
    "# # need to investigate these later.\n",
    "# df_with_prices['prices_need_cleaning'] = df_with_prices['price'].apply(lambda x: isinstance(x, list) and len(x) >= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-teddy",
   "metadata": {},
   "source": [
    "## Investigating posts that had three or more prices listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "conventional-mirror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    [$90, $60, $40]\n",
       "6                                    [$30, $35, $45]\n",
       "7                               [$40, $40, $45, $45]\n",
       "14                              [$30, $30, $60, $90]\n",
       "17                                  [$60, $50, $100]\n",
       "26                 [$100, $115, $130, $65, $30, $60]\n",
       "29                         [$40, $40, $40, $40, $40]\n",
       "31                                   [$50, $10, $50]\n",
       "32                                   [$50, $10, $50]\n",
       "33                                 [$50, $100, $135]\n",
       "34                                   [$30, $35, $45]\n",
       "36                       [$30, $50, $500, $900, $60]\n",
       "40    [$40, $80, $40, $10, $40, $30, $40, $80, $100]\n",
       "48                                   [$90, $60, $40]\n",
       "50                                   [$25, $45, $25]\n",
       "51                    [$35, $35, $40, $40, $55, $80]\n",
       "52                    [$35, $35, $40, $40, $55, $80]\n",
       "56                         [$40, $40, $40, $40, $40]\n",
       "59                              [$45, $55, $40, $50]\n",
       "69                                   [$65, $55, $55]\n",
       "71                         [$200, $60,000, $240,000]\n",
       "72                              [$20, $25, $30, $30]\n",
       "79                                   [$65, $55, $55]\n",
       "Name: price_list, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the entries with 3 or more prices listed, let's investigate why\n",
    "df_with_prices[df_with_prices['len_of_price_list'] >= 3]['price_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a6106d30-41ee-47f0-9695-22b95ad7e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_output = df_with_prices[df_with_prices['len_of_price_list'] >= 3][['price_list', 'post_text']]\n",
    "df_to_output.to_csv('./test/duplicate_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "contrary-immunology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sfc/lss/d/san-francisco-523-mcat-score-offering/7425726283.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect links manually, one by one, to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df_with_prices.iloc[71]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c12634c4-eeaa-4689-a9d6-c2c8d606fa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nFeel free to text me at 909-640-3570\\n\\nHi, I was accepted into the Ivy Leagues and tutor math, chemistry, and SAT/ACT. I’ve been a tutor for over 8 years working with students of all ages. I help with homework, quizzes and tests. For ACT/SAT I scored in the top 1%, use official practice tests and teach high-level tricks. I tutor through videocall, so you can work with me from the comfort of your home 🏠.\\n\\n+ Math ($30)\\n+ Chem ($35)\\n+ SAT/ACT ($45)\\n+ Online Tutoring (Zoom/Skype/Facetime)\\n\\nGive me a call or text me at 909-640-3570. My schedule is flexible and we can get started ASAP.\\n    '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.iloc[6, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "865ed07d-0f93-426d-a305-cecbffeb06a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nFeel free to text me at 909-640-3570\\n\\nHi, I was accepted into the Ivy Leagues and tutor math, chemistry, and SAT/ACT. I’ve been a tutor for over 8 years working with students of all ages. I help with homework, quizzes and tests. For ACT/SAT I scored in the top 1%, use official practice tests and teach high-level tricks. I tutor through videocall, so you can work with me from the comfort of your home 🏠.\\n\\n+ Math ($30)\\n+ Chem ($35)\\n+ SAT/ACT ($45)\\n+ Online Tutoring (Zoom/Skype/Facetime)\\n\\nGive me a call or text me at 909-640-3570. My schedule is flexible and we can get started ASAP.    '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.iloc[34, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a8a88ff-2bcc-46e2-967d-fcb55d7e4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_comparison = df_with_prices[df_with_prices['len_of_price_list'] >= 3]['post_text']\n",
    "vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "tfidf = vect.fit_transform(text_for_comparison)\n",
    "pairwise_similarity = tfidf * tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "737662b9-1158-4406-9918-1d22f2e98ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23x23 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 529 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3eeaa770-cd5d-488c-89e3-b82f5b9a0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_array = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8052f1b-d18a-489f-805e-1578a60d3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(pairwise_array, np.nan)\n",
    "\n",
    "input_idx = text_for_comparison.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e11a36ef-f512-4842-b33d-ae502c7a00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, _ = pairwise_array.shape\n",
    "input_idx = np.arange(n)\n",
    "\n",
    "result_idx = np.nanargmax(pairwise_array[input_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec8eb5ed-1a09-4ca3-a91f-62931de3ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "1 10\n",
      "2 19\n",
      "3 4\n",
      "4 11\n",
      "5 2\n",
      "6 17\n",
      "7 8\n",
      "8 7\n",
      "9 6\n",
      "10 1\n",
      "11 2\n",
      "12 7\n",
      "13 0\n",
      "14 12\n",
      "15 16\n",
      "16 15\n",
      "17 6\n",
      "18 4\n",
      "19 22\n",
      "20 12\n",
      "21 19\n",
      "22 19\n"
     ]
    }
   ],
   "source": [
    "result_idx_list = []\n",
    "for i, e in enumerate(pairwise_array):\n",
    "    result_idx = np.nanargmax(pairwise_array[i])\n",
    "    print(i, result_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a320d9e2-e921-4a27-9c45-3372314df82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 13],\n",
       "       [ 1, 10],\n",
       "       [ 6, 17],\n",
       "       [ 7,  8],\n",
       "       [ 8,  7],\n",
       "       [10,  1],\n",
       "       [13,  0],\n",
       "       [15, 16],\n",
       "       [16, 15],\n",
       "       [17,  6],\n",
       "       [19, 22],\n",
       "       [22, 19]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(pairwise_array>0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1f01efe-5aaf-4ad1-91be-583938c75cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan, 0.12705684, 0.18056146, 0.06589012, 0.06451944,\n",
       "       0.15275626, 0.09575837, 0.21358041, 0.21358041, 0.06438806,\n",
       "       0.12705684, 0.15661501, 0.16212561, 0.99387535, 0.11653422,\n",
       "       0.18768661, 0.18768661, 0.09639098, 0.05293852, 0.10191922,\n",
       "       0.11268865, 0.21070209, 0.09370994])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64af91d5-e184-4fc6-9be5-348e9ba2b719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06451944, 0.05183842, 0.09116651, 0.13615657,        nan,\n",
       "       0.13872901, 0.11675677, 0.12562288, 0.12562288, 0.10836076,\n",
       "       0.05183842, 0.17286249, 0.10471089, 0.06218127, 0.0410628 ,\n",
       "       0.08683034, 0.08683034, 0.11662361, 0.14147651, 0.07514238,\n",
       "       0.09947472, 0.09156005, 0.07726004])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_array[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ca49e6e-c726-4962-a7ad-977909a70137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21358041, 0.13993582, 0.22548004, 0.04235144, 0.12562288,\n",
       "       0.22488836, 0.24339079,        nan, 1.        , 0.17861044,\n",
       "       0.13993582, 0.20977978, 0.24059611, 0.21187473, 0.0868625 ,\n",
       "       0.17167227, 0.17167227, 0.24401668, 0.04844057, 0.15632624,\n",
       "       0.08606989, 0.27533642, 0.14451547])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_array[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4bea0b3-b2b9-4c64-aee3-445d93b2f13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18768661, 0.09919328, 0.1450717 , 0.05388307, 0.08683034,\n",
       "       0.1889475 , 0.15061107, 0.17167227, 0.17167227, 0.07954573,\n",
       "       0.09919328, 0.11374956, 0.12808477, 0.18794197, 0.06994048,\n",
       "       1.        , 1.        , 0.1534486 , 0.10308808, 0.0795927 ,\n",
       "       0.08897193, 0.16652481, 0.06830996])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_array[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e747e3-3a33-4d82-9255-adbe158beca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c997a-09d3-4a8e-8fab-508175240e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963927d1-46c2-42f9-9b00-fed6db8861b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "962c88ec-0a65-4815-b93c-e73de232a765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([29], dtype='int64')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb205c-2fbe-4128-95fb-128cb0377f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "animal-integration",
   "metadata": {},
   "source": [
    "### Cleaning posts with three or more prices manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_col_idx = df_with_prices.columns.get_loc('price')\n",
    "need_clean_col_idx = df_with_prices.columns.get_loc('prices_need_cleaning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-optics",
   "metadata": {},
   "source": [
    "#### Dropping duplicates that weren't spotted earlier\n",
    "\n",
    "These posts come up multiple times and are duplicates, but they change the text of their posting just slightly, so Pandas is unable to detect the duplicate postings on it's own, and we have to find and drop them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of duplicates, set the first instance to the price that makes the most \n",
    "# sense, then drop all remaining duplicate posts.\n",
    "kenari_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('kenaritutor.com')==True].index\n",
    "\n",
    "try:\n",
    "    # Because the ad says $90 in person, $60 for online, and Corona Virus pricing of\n",
    "    # $40 for online weekdays, I'm using the $40 per hour rate because it seems the\n",
    "    # most reasonable.  We also set prices_need_cleaning to False b/c the prices have\n",
    "    # been cleaned\n",
    "    df_with_prices.iloc[kenari_tutor_idx[0],\n",
    "                        [price_col_idx,\n",
    "                         need_clean_col_idx]\n",
    "                       ] = 40, False\n",
    "except:\n",
    "    print('Issue with kenari_tutor_idx and iloc.')\n",
    "    pass\n",
    "\n",
    "# Drop duplicates and reset indices\n",
    "df_with_prices.drop(labels=kenari_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of duplicates.\n",
    "park_academy_idx = df_with_prices[df_with_prices['post_text'].str.contains('(949) 490-0872', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    # The add says $55/hr for K-12, then $65/hr for AP/Honors, as well as Pre-calc, \n",
    "    # etc., I'm going to average the two prices.  Set needs cleaning column to False \n",
    "    # b/c the prices have been cleaned.\n",
    "    df_with_prices.iloc[park_academy_idx[0],\n",
    "                        [price_col_idx,\n",
    "                         need_clean_col_idx]\n",
    "                       ] = 60, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with park_academy_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "# Drop duplicates and reset indices\n",
    "df_with_prices.drop(labels=park_academy_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates, correct price, drop duplicates\n",
    "star_star_college_math_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('https://www.youtube.com/channel/UCqhFZRmUqOAAPMQpo58TV7g'\n",
    "                   ) == True].index\n",
    "\n",
    "try:\n",
    "    # The ad repeats the price of $40 over and over, so I'm replacing the price with \n",
    "    # a single instance.  We also set prices_need_cleaning to False b/c the prices \n",
    "    # have been cleaned.\n",
    "    df_with_prices.iloc[star_star_college_math_tutor_idx[0], [price_col_idx, need_clean_col_idx]] = 40, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with star_star_college_math_tutor_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "# Drop duplicates and reset indices\n",
    "df_with_prices.drop(labels=star_star_college_math_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates, correct price, drop duplicates\n",
    "poway_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('CSUSM: 342')==True].index\n",
    "\n",
    "try:\n",
    "    # This ad says $30 for one hour.\n",
    "    df_with_prices.iloc[poway_tutor_idx[0], [price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with poway_tutor_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "df_with_prices.drop(labels=poway_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-machinery",
   "metadata": {},
   "source": [
    "#### Distilling long lists of prices down to one price\n",
    "\n",
    "Next, we distill posts that had more complicated text that involved three or more prices, such as :\n",
    "\n",
    "* $40$/hr, $50$/1.5hr, $60$/2hr\n",
    "  * Complicated pricing schedule\n",
    "* $40$/hr but $10$ additional per person, if a group session is desired\n",
    "  * Group rates\n",
    "* $30$/hr Science, $40$/hr math, come and try a first session for the reduced price of $20$.\n",
    "  * Special offers\n",
    "\n",
    "into a single price.  Other posts repeated their prices multiple times, so we distill those down to a single price as well, then mark any of the entries we changed as being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad mentions several prices for different subjects, but explicitly says $30 for math.\n",
    "la_honda_idx = df_with_prices[df_with_prices['post_text'].str.contains('909-640-3570')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[la_honda_idx,[price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with la_honda_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# This ad mentions $45 for lower division college courses, which are a large segment of the subjects I help with, so I'm using that price to compare myself against.\n",
    "ucb_phd_student_and_ta_idx = df_with_prices[df_with_prices['post_text'].str.contains('Former UC-Berkeley economics Ph.D. student and TA')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucb_phd_student_and_ta_idx,[price_col_idx, need_clean_col_idx]] = 45, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucb_phd_student_and_ta_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# Says $40 for in person, or $45 for at home, so I took the average.\n",
    "san_mateo_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('I mainly tutor, in person, at the Downtown Redwood City, downtown San Mateo')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[san_mateo_tutor_idx,[price_col_idx, need_clean_col_idx]] = 42.5, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with san_mateo_tutor and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This guy has weird price structuring, but I used his hourly rate for each time interval, $100 for 80 minutes, $115 for 100 minutes, $130 for 120 minutes, then averaged those hourly rates to estimate for what a single hour would cost.\n",
    "oakland_exp_tutor_online_idx = df_with_prices[df_with_prices['post_text'].str.contains('I received a full scholarship to University of Cincinnati and held a 3.8 GPA through my master’s program in aerospace')==True].index\n",
    "\n",
    "oakland_tutor_avg_rate = ((100/80) + (115/100) + (130/120)) * 60 / 3\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[oakland_exp_tutor_online_idx,[price_col_idx, need_clean_col_idx]] = oakland_tutor_avg_rate, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with oakland_exp_tutor_online_idx and iloc.\")\n",
    "    pass\n",
    "\n",
    "# This guy's ad explcityly says $57 per hour.\n",
    "blake_tutoring_indices = df_with_prices[df_with_prices['post_text'].str.contains('BlakeTutoring.com', case=False)==True].index\n",
    "\n",
    "df_with_prices.iloc[blake_tutoring_indices, price_col_idx] = 57\n",
    "\n",
    "\n",
    "# Charges $50 per hour for sessions under 3 hours\n",
    "spss_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('Worked for 2 companies named', case=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[spss_tutor_idx, [price_col_idx, need_clean_col_idx]] = 50, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with spss_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $25/hr for high school, $30/hr for college, just went with $30/hr\n",
    "sharp_mind_idx = df_with_prices[df_with_prices['post_text'].str.contains('(650) 398-9490', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[sharp_mind_idx, [price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with sharp_mind_idx and iloc.\")\n",
    "    pass\n",
    "    \n",
    "    \n",
    "# Says $50/hr    \n",
    "trevor_skelly_idx = df_with_prices[df_with_prices['post_text'].str.contains('trevorskelly')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[trevor_skelly_idx, [price_col_idx, need_clean_col_idx]] = 50, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with trevor_skelly_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $25/hr if meeting near CSU Sac, $35/hr if they drive to you, $20/hr for online.\n",
    "# I chose $30/hr to split the difference between the in person prices.\n",
    "best_math_idx = df_with_prices[df_with_prices['post_text'].str.contains('bestmathtutoring.com')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[best_math_idx, [price_col_idx, need_clean_col_idx]] = 30, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with best_math_idx and iloc.\")\n",
    "    pass    \n",
    "\n",
    "# Says #60 per hour.\n",
    "glasses_lady_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"offering virtual one-on-one Math tutoring via Zoom\")==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[glasses_lady_idx, [price_col_idx, need_clean_col_idx]] = 60, False\n",
    "except:\n",
    "    print(\"Issue with glasses_lady_idx and iloc.\")\n",
    "    pass    \n",
    "\n",
    "\n",
    "ucla_grad_henry_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"916 390-7923\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucla_grad_henry_idx, [price_col_idx, need_clean_col_idx]] = 35, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucla_grad_henry_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_d_idx = df_with_prices[df_with_prices['post_text'].str.contains('Peter D.')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[peter_d_idx, [price_col_idx, need_clean_col_idx]] = 40, False\n",
    "except:\n",
    "    print(\"Issue with peter_d_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $20/hr online, $30/hr in person, split the difference at $25\n",
    "austin_sabrina_idx = df_with_prices[df_with_prices['post_text'].str.contains('My girlfriend Sabrina')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[austin_sabrina_idx, [price_col_idx, need_clean_col_idx]] = 25, False\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with austin_sabrina_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_farrell_idx = df_with_prices[df_with_prices['post_text'].str.contains('Alexander Farrell')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[alex_farrell_idx, [price_col_idx, need_clean_col_idx]] = 25, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with alex_farrell_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post says $30/hr for Precalc/Trig and $50/hr for Calculus, so I took the average\n",
    "lonzo_tutoring_idx = df_with_prices[df_with_prices['post_text'].str.contains('951-795-5027', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[lonzo_tutoring_idx, [price_col_idx, need_clean_col_idx]] = 40, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with lonzo_tutoring_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post includes many prices, but states $55/hr for Precalc and $80/hr for Calculus, so I took the average of those prices\n",
    "aerospace_engineer_idx = df_with_prices[df_with_prices['post_text'].str.contains('undergraduate students at UC San Diego', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[aerospace_engineer_idx, [price_col_idx, need_clean_col_idx]] = (55 + 80)/2, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with aerospace_engineer_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-natural",
   "metadata": {},
   "source": [
    "## Checking results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-terry",
   "metadata": {},
   "source": [
    "#### Are there any posts that were marked as needing to be cleaned that we missed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_still_as_list = len(df_with_prices[df_with_prices['prices_need_cleaning']==True]['price'])\n",
    "\n",
    "if num_still_as_list==0:\n",
    "    print(\"There are no posts that had multiple prices still needing cleaning.\")\n",
    "else:\n",
    "    print(F\"There are {num_still_as_list} posts that still have multiple prices needing cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the posts with three or more that still need cleaning.\n",
    "df_with_prices[df_with_prices['prices_need_cleaning']==True]['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-office",
   "metadata": {},
   "source": [
    "#### Are there any posts with a price that was marked as being null during pre-processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_null_prices = len(df_with_prices[df_with_prices['price'].isnull()==True])\n",
    "\n",
    "if num_null_prices==0:\n",
    "    print(\"There are no posts that have null prices.\")\n",
    "else:\n",
    "    print(F\"There are {num_null_prices} posts that have null prices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the posts with prices that are NaN because Python wasn't able to parse them properly during pre-processing.\n",
    "df_with_prices[df_with_prices['price'].isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-substance",
   "metadata": {},
   "source": [
    "## Investigating posts with extreme prices.  Are there any price outliers that we need to clean?\n",
    "\n",
    "Prices >= 100 or <= 20 are what I would consider to be extreme prices.  Let's flag and use the flag to locate and then investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of which prices are what I would consider to be unusual\n",
    "df_with_prices['price_to_investigate'] = df_with_prices['price'].apply(lambda x: (x>=100) | (x<=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_prices[df_with_prices['price_to_investigate']==True][['price', 'post_text', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=75\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-belize",
   "metadata": {},
   "source": [
    "### Dropping posts with extreme prices that are duplicates or aren't relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This person mentions no prices, the only $ amount specified is how much they earned # in scholarships, so we drop all instances\n",
    "at_geemale_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"at geemale\")==True].index\n",
    "\n",
    "df_with_prices.drop(labels=at_geemale_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all instances, keep first instance, drop all the remaining duplicates\n",
    "ansari_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"peerlinc.com\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=ansari_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all instances, keep first instance, drop all the remaining duplicates\n",
    "ridgewood_nyc_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"(646) 326-2191\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=ridgewood_nyc_tutor_idx[1:], inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutor's ad only mentions the rate per student for an hour long group session, so we drop all instances\n",
    "why_exceptional_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"countless time-saving, test-taking strategies\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=why_exceptional_tutor_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for SAT prep only, not really what I'm competing against, so we drop all instances\n",
    "study_house_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"STUDY HOUSE LLC\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=study_house_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for poker tutoring/coaching, not really what I'm competing against, so we drop all instances.  He also mentions he tutors math in this post, but he has a separate post up that we've captured which has his math tutoring pricing information.\n",
    "australia_daniel_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"I'm available as a dealer if you need one\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=australia_daniel_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for MCAT tutoring, not really what I'm competing against, so we drop all instances.\n",
    "connor_MCAT_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"My name is Connor and I've been teaching the MCAT since 2016\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=connor_MCAT_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad doesn't mention a price, so we drop all instances.\n",
    "john_baptist_nguyen_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"john-baptist-nguyen\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=john_baptist_nguyen_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-storage",
   "metadata": {},
   "source": [
    "### Correct pricing information for posts with extreme prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_to_investigate_col_idx = df_with_prices.columns.get_loc('price_to_investigate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $25/hr but then mentions a prepay plan for $225.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $25\n",
    "james_edward_nassir_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"James Edward Nassir, EE, Educator, and Discoverer of the 5th Force\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[james_edward_nassir_idx, [price_col_idx, price_to_investigate_col_idx]] = 25, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with james_edward_nassir_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $50/hr but then mentions a prepay plan for $160 for 4 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $50\n",
    "google_maps_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"willing to travel if Google Maps\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[google_maps_idx, [price_col_idx, price_to_investigate_col_idx]] = 50, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with google_maps_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $84/hr but then mentions a $125 for 1.5 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $84\n",
    "rescue_animals_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"TestTrainerinc\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[rescue_animals_idx, [price_col_idx, price_to_investigate_col_idx]] = 84, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with rescue_animals_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $45/hr for high school or college, but then mentions a $35 for middle school.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $45, since I primarily tutor high school or college students.\n",
    "rancho_penasquitos_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"Rancho Penasquitos (Park Village Neighborhood)\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[rancho_penasquitos_idx, [price_col_idx, price_to_investigate_col_idx]] = 45, False\n",
    "\n",
    "except:\n",
    "    print(\"Issue with rancho_penasquitos_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-amount",
   "metadata": {},
   "source": [
    "## Checking results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-hazard",
   "metadata": {},
   "source": [
    "#### Are there any posts with extreme prices that we marked which still need investigation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the posts that remain, but they all have prices which agree with the posting, so no cleaning is needed.\n",
    "df_with_prices[df_with_prices['price_to_investigate']==True][['price', 'post_text', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any prices still need cleaning, inspect their links to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df_with_prices.iloc[9]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of results after transforming is complete\n",
    "df_with_prices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-capacity",
   "metadata": {},
   "source": [
    "#### Everything looks good.  Transforming complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-jurisdiction",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saving results\n",
    "\n",
    "### Store results locally as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.  CL links will expire after some number of days, the prices_need_cleaning and price_to_investigate columns have been manually inspected, and lastly we've distilled the multiple prices in the price_list down to a single value\n",
    "df_for_sql = df_with_prices.drop(labels=['prices_need_cleaning','link', 'price_list', 'price_to_investigate'], axis=1)\n",
    "\n",
    "# In order for psycopg2 to parse our CSV file correctly later, we need to escape all new line characters by adding an additional \\ in front of \\n.\n",
    "df_for_sql['post_text'] = df_for_sql['post_text'].str.replace('\\n', '\\\\n')\n",
    "\n",
    "# Store cleaned data as CSV file in preparation for importing to SQL database\n",
    "df_for_sql.to_csv(\"./{}_all_regions_with_prices.csv\".format(date_of_html_request), index=False, sep=';')\n",
    "\n",
    "# Store original data, before we applied any cleaning to it, in case it's needed for something later on.\n",
    "df.to_csv(\"./{}_all_regions_posts.csv\".format(date_of_html_request), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-boards",
   "metadata": {},
   "source": [
    "### Importing into PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to PSQL database\n",
    "conn = psycopg2.connect(\"host=localhost dbname=rancher user=rancher\")\n",
    "\n",
    "# Instantiate a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Use cursor object to create a database for storing the information we scraped and cleaned, if one doesn't already exist.\n",
    "cur.execute(\"\"\"    \n",
    "    CREATE TABLE IF NOT EXISTS cl_tutoring2(\n",
    "    id SERIAL primary key,\n",
    "    date_scraped date,\n",
    "    price decimal,\n",
    "    city text,\n",
    "    subregion text,\n",
    "    region text,\n",
    "    post_text text,\n",
    "    date_posted timestamp\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Copy data from our CSV file into database.  \n",
    "### Note, we can use the ; separator freely because we replaced all instances of semicolons in a post to commas during the preprocessing stage, ensuring that psycopg2 won't misinterpret a semicolon in the body of a post as a separator, splitting a row in the CSV file into too many columns as a result.\n",
    "### Also, we must specify null=\"\" because Python represents null values as an empty string when writing to a CSV file and psycopg2 needs to know how null values are represented in the CSV file in order to properly insert null values into the database\n",
    "with open(str(date_of_html_request) + '_all_regions_with_prices.csv', 'r') as file:\n",
    "    next(file) # Skip the header row\n",
    "    cur.copy_from(file, 'cl_tutoring2', sep=';', null=\"\", columns=('date_posted', 'price', 'city', 'subregion', 'region', 'post_text', 'date_scraped'))\n",
    "    \n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-wheat",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-guess",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
