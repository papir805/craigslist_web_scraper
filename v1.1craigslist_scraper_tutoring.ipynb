{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import requests\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv \n",
    "import psycopg2\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I picked the 10 largest metropolitan areas by population to scrape data from, as well as Sacramento, since it's nearby to me and is another major city\n",
    "regions_to_scrape = ['sf_bay_area',\n",
    "                    'new_york',\n",
    "                    'los_angeles',\n",
    "                    'sacramento',\n",
    "                    'chicago',\n",
    "                    'san_diego',\n",
    "                    'houston',\n",
    "                    'phoenix',\n",
    "                    'philadelphia',\n",
    "                    'dallas',\n",
    "                    'san_antonio']\n",
    "\n",
    "num_regions = len(regions_to_scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-testament",
   "metadata": {},
   "source": [
    "# *Extract* Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "public-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Session and Retry object to manage the quota Craigslist imposes on HTTP get requests within a certain time period \n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a748d9e7-d53d-40e5-a24a-679531b7a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sf_bay_area 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "sf_bay_area 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "sf_bay_area 3 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for sf_bay_area received.  Process completed.\n",
      "new_york 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "new_york 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "new_york 3 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for new_york received.  Process completed.\n",
      "los_angeles 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "los_angeles 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for los_angeles received.  Process completed.\n",
      "sacramento 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for sacramento received.  Process completed.\n",
      "chicago 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for chicago received.  Process completed.\n",
      "san_diego 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for san_diego received.  Process completed.\n",
      "houston 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for houston received.  Process completed.\n",
      "phoenix 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for phoenix received.  Process completed.\n",
      "philadelphia 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for philadelphia received.  Process completed.\n",
      "dallas 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for dallas received.  Process completed.\n",
      "san_antonio 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for san_antonio received.  Process completed.\n"
     ]
    }
   ],
   "source": [
    "# Walk through each region in our list of regions_to_scrape to get the HTML page corresponding to a search for \"math tutor\" in the services section\n",
    "\n",
    "response_dict = {}\n",
    "sleep_timer = 10\n",
    "\n",
    "for count, region in enumerate(regions_to_scrape):\n",
    "    # This gets the first page of search results\n",
    "    i=1\n",
    "    current_region = region.replace('_', '')\n",
    "    current_response = session.get('https://' + current_region + '.craigslist.org/d/services/search/bbb?query=math%20tutor&sort=rel')\n",
    "    print(F\"{region} {i} response received.\")\n",
    "    print(F\"Waiting {sleep_timer} seconds...\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(sleep_timer)\n",
    "    \n",
    "    region_response_list = []\n",
    "    region_response_list.append(current_response)\n",
    "\n",
    "    # This gets all subsequent pages, using the next button\n",
    "    is_next_button = True\n",
    "    while is_next_button:\n",
    "        i+=1\n",
    "        try:\n",
    "            next_response = current_response\n",
    "            next_soup = BeautifulSoup(next_response.text, 'html.parser')\n",
    "            html_suffix = next_soup.find(class_='button next').get('href')\n",
    "            if html_suffix != '':\n",
    "                new_button = 'https://' + current_region + '.craigslist.org' + html_suffix\n",
    "                current_response = session.get(new_button)\n",
    "                region_response_list.append(current_response)\n",
    "                \n",
    "                \n",
    "                time.sleep(sleep_timer)\n",
    "                print(F\"{region} {i} response received.\")\n",
    "                print(F\"Waiting {sleep_timer} seconds...\")\n",
    "                print()\n",
    "            else:\n",
    "                is_next_button = False\n",
    "                print(F\"Last response for {region} received.  Process completed.\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Store all search pages for math tutor\n",
    "    response_dict[region] = region_response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3d3e52-075f-4d8e-8494-41ef4f040317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through each region to get a list of all individual postings for math tutoring \n",
    "# in the results page we searched up earlier.\n",
    "posts_dict = {}\n",
    "for region, responses in response_dict.items():\n",
    "    #current_region = region\n",
    "    region_posts = []\n",
    "    for response in responses:\n",
    "        current_html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        current_posts = current_html_soup.find_all('li', class_='result-row')\n",
    "        region_posts.extend(current_posts)\n",
    "    posts_dict[region] = region_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time is 23:15:05\n",
      "Process will finish by 02:55:05\n",
      "\n",
      "Post number 10 in sf_bay_area is being extracted.\n",
      "Post number 20 in sf_bay_area is being extracted.\n",
      "Post number 30 in sf_bay_area is being extracted.\n",
      "Post number 40 in sf_bay_area is being extracted.\n",
      "Post number 50 in sf_bay_area is being extracted.\n",
      "Post number 60 in sf_bay_area is being extracted.\n",
      "Post number 70 in sf_bay_area is being extracted.\n",
      "Post number 80 in sf_bay_area is being extracted.\n",
      "Post number 90 in sf_bay_area is being extracted.\n",
      "Post number 100 in sf_bay_area is being extracted.\n",
      "Post number 110 in sf_bay_area is being extracted.\n",
      "Post number 120 in sf_bay_area is being extracted.\n",
      "Post number 130 in sf_bay_area is being extracted.\n",
      "Post number 140 in sf_bay_area is being extracted.\n",
      "Post number 150 in sf_bay_area is being extracted.\n",
      "Post number 160 in sf_bay_area is being extracted.\n",
      "Post number 170 in sf_bay_area is being extracted.\n",
      "Post number 180 in sf_bay_area is being extracted.\n",
      "Post number 190 in sf_bay_area is being extracted.\n",
      "Post number 200 in sf_bay_area is being extracted.\n",
      "Post number 210 in sf_bay_area is being extracted.\n",
      "Post number 220 in sf_bay_area is being extracted.\n",
      "Post number 230 in sf_bay_area is being extracted.\n",
      "Post number 240 in sf_bay_area is being extracted.\n",
      "Post number 250 in sf_bay_area is being extracted.\n",
      "Post number 260 in sf_bay_area is being extracted.\n",
      "Post number 270 in sf_bay_area is being extracted.\n",
      "Post number 280 in sf_bay_area is being extracted.\n",
      "Post number 290 in sf_bay_area is being extracted.\n",
      "\n",
      "Soup objects for sf_bay_area acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in new_york is being extracted.\n",
      "Post number 20 in new_york is being extracted.\n",
      "Post number 30 in new_york is being extracted.\n",
      "Post number 40 in new_york is being extracted.\n",
      "Post number 50 in new_york is being extracted.\n",
      "Post number 60 in new_york is being extracted.\n",
      "Post number 70 in new_york is being extracted.\n",
      "Post number 80 in new_york is being extracted.\n",
      "Post number 90 in new_york is being extracted.\n",
      "Post number 100 in new_york is being extracted.\n",
      "Post number 110 in new_york is being extracted.\n",
      "Post number 120 in new_york is being extracted.\n",
      "Post number 130 in new_york is being extracted.\n",
      "Post number 140 in new_york is being extracted.\n",
      "Post number 150 in new_york is being extracted.\n",
      "Post number 160 in new_york is being extracted.\n",
      "Post number 170 in new_york is being extracted.\n",
      "Post number 180 in new_york is being extracted.\n",
      "Post number 190 in new_york is being extracted.\n",
      "Post number 200 in new_york is being extracted.\n",
      "Post number 210 in new_york is being extracted.\n",
      "Post number 220 in new_york is being extracted.\n",
      "Post number 230 in new_york is being extracted.\n",
      "Post number 240 in new_york is being extracted.\n",
      "Post number 250 in new_york is being extracted.\n",
      "Post number 260 in new_york is being extracted.\n",
      "Post number 270 in new_york is being extracted.\n",
      "Post number 280 in new_york is being extracted.\n",
      "Post number 290 in new_york is being extracted.\n",
      "\n",
      "Soup objects for new_york acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in los_angeles is being extracted.\n",
      "Post number 20 in los_angeles is being extracted.\n",
      "Post number 30 in los_angeles is being extracted.\n",
      "Post number 40 in los_angeles is being extracted.\n",
      "Post number 50 in los_angeles is being extracted.\n",
      "Post number 60 in los_angeles is being extracted.\n",
      "Post number 70 in los_angeles is being extracted.\n",
      "Post number 80 in los_angeles is being extracted.\n",
      "Post number 90 in los_angeles is being extracted.\n",
      "Post number 100 in los_angeles is being extracted.\n",
      "Post number 110 in los_angeles is being extracted.\n",
      "Post number 120 in los_angeles is being extracted.\n",
      "Post number 130 in los_angeles is being extracted.\n",
      "Post number 140 in los_angeles is being extracted.\n",
      "Post number 150 in los_angeles is being extracted.\n",
      "Post number 160 in los_angeles is being extracted.\n",
      "Post number 170 in los_angeles is being extracted.\n",
      "Post number 180 in los_angeles is being extracted.\n",
      "Post number 190 in los_angeles is being extracted.\n",
      "Post number 200 in los_angeles is being extracted.\n",
      "\n",
      "Soup objects for los_angeles acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in sacramento is being extracted.\n",
      "Post number 20 in sacramento is being extracted.\n",
      "Post number 30 in sacramento is being extracted.\n",
      "Post number 40 in sacramento is being extracted.\n",
      "Post number 50 in sacramento is being extracted.\n",
      "Post number 60 in sacramento is being extracted.\n",
      "Post number 70 in sacramento is being extracted.\n",
      "Post number 80 in sacramento is being extracted.\n",
      "Post number 90 in sacramento is being extracted.\n",
      "Post number 100 in sacramento is being extracted.\n",
      "Post number 110 in sacramento is being extracted.\n",
      "\n",
      "Soup objects for sacramento acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in chicago is being extracted.\n",
      "Post number 20 in chicago is being extracted.\n",
      "Post number 30 in chicago is being extracted.\n",
      "Post number 40 in chicago is being extracted.\n",
      "Post number 50 in chicago is being extracted.\n",
      "Post number 60 in chicago is being extracted.\n",
      "\n",
      "Soup objects for chicago acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in san_diego is being extracted.\n",
      "Post number 20 in san_diego is being extracted.\n",
      "Post number 30 in san_diego is being extracted.\n",
      "Post number 40 in san_diego is being extracted.\n",
      "Post number 50 in san_diego is being extracted.\n",
      "Post number 60 in san_diego is being extracted.\n",
      "Post number 70 in san_diego is being extracted.\n",
      "Post number 80 in san_diego is being extracted.\n",
      "Post number 90 in san_diego is being extracted.\n",
      "Post number 100 in san_diego is being extracted.\n",
      "Post number 110 in san_diego is being extracted.\n",
      "\n",
      "Soup objects for san_diego acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in houston is being extracted.\n",
      "Post number 20 in houston is being extracted.\n",
      "Post number 30 in houston is being extracted.\n",
      "Post number 40 in houston is being extracted.\n",
      "Post number 50 in houston is being extracted.\n",
      "Post number 60 in houston is being extracted.\n",
      "Post number 70 in houston is being extracted.\n",
      "\n",
      "Soup objects for houston acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in phoenix is being extracted.\n",
      "Post number 20 in phoenix is being extracted.\n",
      "Post number 30 in phoenix is being extracted.\n",
      "Post number 40 in phoenix is being extracted.\n",
      "Post number 50 in phoenix is being extracted.\n",
      "\n",
      "Soup objects for phoenix acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in philadelphia is being extracted.\n",
      "Post number 20 in philadelphia is being extracted.\n",
      "Post number 30 in philadelphia is being extracted.\n",
      "Post number 40 in philadelphia is being extracted.\n",
      "Post number 50 in philadelphia is being extracted.\n",
      "Post number 60 in philadelphia is being extracted.\n",
      "Post number 70 in philadelphia is being extracted.\n",
      "Post number 80 in philadelphia is being extracted.\n",
      "Post number 90 in philadelphia is being extracted.\n",
      "Post number 100 in philadelphia is being extracted.\n",
      "Post number 110 in philadelphia is being extracted.\n",
      "\n",
      "Soup objects for philadelphia acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in dallas is being extracted.\n",
      "Post number 20 in dallas is being extracted.\n",
      "Post number 30 in dallas is being extracted.\n",
      "Post number 40 in dallas is being extracted.\n",
      "Post number 50 in dallas is being extracted.\n",
      "Post number 60 in dallas is being extracted.\n",
      "Post number 70 in dallas is being extracted.\n",
      "Post number 80 in dallas is being extracted.\n",
      "\n",
      "Soup objects for dallas acquired.  Waiting for next region...\n",
      "\n",
      "Post number 10 in san_antonio is being extracted.\n",
      "Post number 20 in san_antonio is being extracted.\n",
      "Post number 30 in san_antonio is being extracted.\n",
      "Post number 40 in san_antonio is being extracted.\n",
      "Post number 50 in san_antonio is being extracted.\n",
      "Post number 60 in san_antonio is being extracted.\n",
      "Post number 70 in san_antonio is being extracted.\n",
      "Post number 80 in san_antonio is being extracted.\n",
      "Post number 90 in san_antonio is being extracted.\n",
      "\n",
      "Soup objects for san_antonio acquired.  Process complete.\n"
     ]
    }
   ],
   "source": [
    "soup_objects_dict = {}\n",
    "\n",
    "current_time = dt.datetime.now()\n",
    "num_seconds = num_regions * 120 * 10\n",
    "max_finish_time = current_time + dt.timedelta(seconds=num_seconds)\n",
    "\n",
    "print(F\"Current time is {current_time.strftime('%H:%M:%S')}\")\n",
    "print(F\"Process will finish by {max_finish_time.strftime('%H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "for count, region in enumerate(posts_dict, start=1):\n",
    "    # Walk through each region and create a list of soup_objects to scrape from by \n",
    "    # storing them into memory.  This way we only have to send these get requests \n",
    "    # once and Craigslist doesn't ban us for sending the same https requests over \n",
    "    # and over\n",
    "    soup_objects_list = []\n",
    "    #link_list = []\n",
    "    for i, post in enumerate(posts_dict[region]):\n",
    "        # Impose a timer so that we send each get request between 5 and 10 seconds.\n",
    "        # This is again to help prevent from getting banned for too many HTTP \n",
    "        # requests.\n",
    "        random_int = random.randint(5,10)\n",
    "        time.sleep(random_int)\n",
    "        current_link = post.a.get('href')\n",
    "        #link_list.append(current_link)\n",
    "        response_object = session.get(current_link)\n",
    "        soup_object = BeautifulSoup(response_object.text, 'html.parser')\n",
    "        soup_objects_list.append(soup_object) \n",
    "        # Impose condition that every 10th post will trigger something printed\n",
    "        # to the screen.  This part of the code is a long process and I wanted\n",
    "        # something to help keep track of how much progress has been made\n",
    "        if (i !=0) and ((i-1) % 10 == 9):\n",
    "            print(F\"Post number {i} in {region} is being extracted.\")\n",
    "    \n",
    "    soup_objects_dict[region] = soup_objects_list\n",
    "    if count != len(posts_dict):\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Waiting for next region...\")\n",
    "        print()\n",
    "    else:\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-interaction",
   "metadata": {},
   "source": [
    "## Pre-processing Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ec9ba83-0ce9-4b7d-82fd-eccf2b3ff58f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "error_list_text = []\n",
    "error_list_links = []\n",
    "\n",
    "# Walk through each region that contains a list of soup objects corresponding to the # search of services for math tutors.\n",
    "for search_region in soup_objects_dict:\n",
    "    # Initialize several lists to store relevant information for analysis\n",
    "    price_list = []\n",
    "    city_list = []\n",
    "    datetime_list = []\n",
    "    body_text_list = []\n",
    "    subregion_list = []\n",
    "    region_list = []\n",
    "    link_list = []\n",
    "    search_region_price_list = []\n",
    "    \n",
    "    # Walk through each soup object in the list corresponding to the search region \n",
    "    # and get the link of the soup object to scrape from.\n",
    "    for soup in soup_objects_dict[search_region]:\n",
    "        try:\n",
    "            link = soup.find(\"meta\", property=\"og:url\")['content']\n",
    "        except:\n",
    "            # In case a link can't be found, we add the soup object to a list\n",
    "            # to inspect later and set link to 'None', which we'll use to filter\n",
    "            # these results out later\n",
    "            link = 'None'\n",
    "            error_list_links.append(soup)\n",
    "            print(\"Couldn't get link\")\n",
    "\n",
    "        # Extract region of post from Craigslist\n",
    "        post_region = soup.find_all('li',class_='crumb area')[0].find('a').get_text()\n",
    "        post_region = post_region.replace(' ', '_')\n",
    "        post_region = post_region.lower()\n",
    "        \n",
    "        # Get text of postingbody of the post and remove unwanted text.\n",
    "        try:\n",
    "            text = soup.find('section', id='postingbody').get_text()\n",
    "            #text = text.replace('\\n', '')\n",
    "            text = text.replace(';', ',') # We do this so that we can use ; as \n",
    "                                          # a delimiter when copying data from a \n",
    "                                          # CSV file into a SQL database later.\n",
    "            text = text.replace('QR Code Link to This Post', '') # We do this \n",
    "                                                                 # because this\n",
    "                                                                 # text from one\n",
    "                                                                 # post in\n",
    "                                                                 # particular was                                                                      # giving me \n",
    "                                                                 # trouble and\n",
    "                                                                 # the best way I \n",
    "                                                                 # could find to \n",
    "                                                                 # handle it was \n",
    "                                                                 # to remove the \n",
    "                                                                 # text.\n",
    "            text = text.replace(u'\\xa0', u' ')\n",
    "\n",
    "        except:\n",
    "            error_list_text.append(soup)\n",
    "            text = 'None'\n",
    "            #body_text_list.append(text)\n",
    "            print(\"Couldn't get text\")\n",
    "\n",
    "        # Only let posts through that have a link to scrape from and those posts \n",
    "        # where the region of the post matches the region of the search.  Some CL \n",
    "        # search results are for neighboring areas, ones that come up in a different\n",
    "        # region than the region your search was from, which leads to duplicates in \n",
    "        # areas like Los Angeles and San Diego.  This will weed out duplicates.\n",
    "        if post_region == search_region and link!= 'None':\n",
    "            region_list.append(post_region)\n",
    "            link_list.append(link)\n",
    "            body_text_list.append(text)\n",
    "\n",
    "            # Use regular expressions to find all instances of prices in the text\n",
    "            #old_prices = re.findall('(?:[\\$]{1}[,\\d]+.?\\d*)', text)\n",
    "            old_prices = re.findall('(?:[\\$]{1}[,\\d]+\\d*)', text)\n",
    "            # Alternative, if trying to capture decimals \n",
    "            # ^(?:\\${1}\\d+(?:,\\d{3})*(?:\\.{1}\\d{2}){0,1})?$\n",
    "\n",
    "\n",
    "\n",
    "            # Intialize empty list to store the new prices after processing old\n",
    "            # prices.\n",
    "            new_prices = []\n",
    "            #print(F\"Initialized new_prices: {new_prices}\")\n",
    "            # Walk through each price in the post.\n",
    "            for price in old_prices:\n",
    "                # Clean unwanted characters.\n",
    "                price = price.replace('$', '')\n",
    "                price = price.replace('/', '')\n",
    "                price = price.replace('!', '')\n",
    "                price = price.replace('h', '')\n",
    "                price = price.replace('.', '')\n",
    "                price = price.replace(')', '')\n",
    "                price = price.replace(',', '')\n",
    "                price = price.replace('>', '')\n",
    "                price = price.rstrip()   \n",
    "                # Some tutors give prices as a range ie '$30-40'.  In order to\n",
    "                # work with this data, I split based on the hyphen, then I can \n",
    "                # use each price individually.\n",
    "                split_prices = price.split('-')\n",
    "            #print(F\"Here are the old_prices: {old_prices}\")\n",
    "            #print(F\"Here are the split_prices: {split_prices}\")\n",
    "\n",
    "                # Walk through each price in the posting, after any necessary splits \n",
    "                # have been made.\n",
    "                for p in split_prices:\n",
    "                    # Only proceed if the post contained prices, ie if p is a non-\n",
    "                    # empty string.\n",
    "                    if len(p)!=0:\n",
    "\n",
    "                        try:\n",
    "                            # Convert string price to int.\n",
    "                            new_int = int(p)\n",
    "                            if new_int <= 200:\n",
    "                                new_prices.append(new_int)\n",
    "\n",
    "                        except:\n",
    "                            # Show which prices aren't able to convert to an int and \n",
    "                            # the post they came from so we can isolate and fix the \n",
    "                            # issue.\n",
    "                            print(F'Error converting this price: {p}')\n",
    "                            print(split_prices)\n",
    "                            print()\n",
    "                            print('Here is the text of the post:')\n",
    "                            print()\n",
    "                            print(text)\n",
    "                            print('-'*50)\n",
    "                            print()\n",
    "                            # Set prices that can't be covered to NaN so the process \n",
    "                            # can finish.\n",
    "                            new_prices.append(np.nan) \n",
    "            #print(F\"Here are the processed new_prices: {new_prices}\")\n",
    "                    #print(len(new_prices))\n",
    "\n",
    "\n",
    "            # Append prices before they're processed to a separate list, in case we\n",
    "            # need to isolate issues and fix them later.\n",
    "\n",
    "            search_region_price_list.append(new_prices)\n",
    "\n",
    "            # For posts that had no prices listed, we append new_prices with \"None\"\n",
    "            if len(new_prices)==0:\n",
    "                #price_list.append('None')\n",
    "                price_list.append(np.nan)\n",
    "            # For posts that had a single price, we use it.\n",
    "            elif len(new_prices)==1:\n",
    "                price_list.append(new_prices[0])\n",
    "            # For posts that contained two prices, we average them.  This helps with \n",
    "            # posts that give a range of prices (ie $25-30).\n",
    "            elif len(new_prices)==2:\n",
    "                avg_price_2 = np.average(new_prices)\n",
    "                price_list.append(avg_price_2)\n",
    "            # If a post has more than 3 prices, we append them, but this means we \n",
    "            # have to inspect them manually and deal with them later.\n",
    "            else:\n",
    "                #price_list.append(new_prices)\n",
    "                price_list.append(np.nan)\n",
    "            #print(price_list)\n",
    "\n",
    "\n",
    "            # Get city information for each posting.\n",
    "            try:\n",
    "                city = soup.find(class_='postingtitletext').small.get_text()\n",
    "\n",
    "                # Because of the way CL operates, one has to choose a city from a\n",
    "                # radio button list that CL provides when one creates a post to offer \n",
    "                # a service, however later, there's a field where they can type in \n",
    "                # any city they want.  Many people will randomly choose a city from \n",
    "                # the radio button list, but then  post their city as \"online\".  This \n",
    "                # makes sure we capture them. \n",
    "                re_pattern = re.compile('online')\n",
    "                online_flag = re.search(re_pattern, city.lower())\n",
    "                if online_flag:\n",
    "                    city_list.append('Online')\n",
    "                else:\n",
    "                    # Strip out leading and trailing white spaces, replace\n",
    "                    # parentheses, and capitalize each word in the str.\n",
    "                    city = city.strip()\n",
    "                    city = city.replace('(', '').replace(')', '')        \n",
    "                    city = city.title()\n",
    "                    city_list.append(city)\n",
    "            except:\n",
    "                # If a post has no city information, use None\n",
    "                city_list.append('None')\n",
    "\n",
    "            # Extract subregion of Craigslist that the post was made in.\n",
    "            # This will allow for comparison of prices across different cities\n",
    "            # within the same metropolitan sub_region.\n",
    "            try:\n",
    "                subregion = soup.find_all('li', class_='crumb subarea')[0].find('a').get_text()\n",
    "                subregion = subregion.title()\n",
    "                subregion_list.append(subregion)\n",
    "            except:\n",
    "                subregion_list.append('None')\n",
    "\n",
    "\n",
    "            # Extract time the posting was made.\n",
    "            try:\n",
    "                dt_object = soup.find('time')['datetime']\n",
    "                datetime_list.append(dt_object)\n",
    "            except:\n",
    "                datetime_list.append('None')\n",
    "        else:\n",
    "            pass\n",
    "    #print(price_list)\n",
    "    # Create temporary df to store results for each region\n",
    "    temp_df = pd.DataFrame(data=zip(datetime_list,\n",
    "                                    link_list, \n",
    "                                    price_list, \n",
    "                                    city_list, \n",
    "                                    subregion_list, \n",
    "                                    region_list, \n",
    "                                    body_text_list,\n",
    "                                    search_region_price_list),\n",
    "                        columns=['date_posted', \n",
    "                                 'link', \n",
    "                                 'price', \n",
    "                                 'city', \n",
    "                                 'subregion', \n",
    "                                 'region', \n",
    "                                 'post_text',\n",
    "                                 'price_list']\n",
    "                          )\n",
    "\n",
    "# # Find indices of duplicate results, then drop them and reset indices.\n",
    "# temp_duplicate_indices = temp_df[temp_df['post_text'].duplicated()==True].index\n",
    "# temp_df_no_dups = temp_df.drop(index=temp_duplicate_indices)\n",
    "# temp_df_no_dups = temp_df_no_dups.reset_index(drop=True)\n",
    "# temp_df_no_dups['len_of_price_list']=temp_df_no_dups['price_list'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "# temp_text_for_comparison = temp_df_no_dups['post_text']\n",
    "# vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "# temp_tfidf = vect.fit_transform(temp_text_for_comparison)\n",
    "# temp_pairwise_similarity = temp_tfidf * temp_tfidf.T\n",
    "# temp_pairwise_array = temp_pairwise_similarity.toarray()\n",
    "# np.fill_diagonal(temp_pairwise_array, np.nan)\n",
    "# temp_argwhere_array = np.argwhere(temp_pairwise_array > 0.9)\n",
    "\n",
    "\n",
    "# df_row_idx = []\n",
    "# dup_row_idx = []\n",
    "# for row in temp_argwhere_array:\n",
    "#     current_idx = row[0]\n",
    "#     #print(F\"Current row: {row}, Current idx: {current_idx}\")\n",
    "#     duplicate_list = []\n",
    "#     if current_idx in df_row_idx:\n",
    "#         continue\n",
    "#     else:\n",
    "#         df_row_idx.append(current_idx)\n",
    "#     for other_row in temp_argwhere_array:\n",
    "#         other_idx = other_row[1]\n",
    "#         #print(F\"Here's the other_row: {other_row}, Other idx: {other_idx}\")\n",
    "#         if current_idx == other_row[0]:\n",
    "#             duplicate_list.append(other_idx)\n",
    "#     #print(F\"This is the current dup_list: {duplicate_list}\")\n",
    "#     #print()\n",
    "#     dup_row_idx.append(duplicate_list)\n",
    "\n",
    "\n",
    "# temp_df_no_dups['match'] = np.array(temp_df_no_dups.index.values, dtype='object')\n",
    "# # temp_df_no_dups['match'] = temp_df_no_dups['match'].apply(lambda x: [x])\n",
    "\n",
    "\n",
    "# match_col_idx = temp_df_no_dups.columns.get_loc('match')\n",
    "# temp_df_no_dups.iloc[df_row_idx, match_col_idx] = dup_row_idx\n",
    "# temp_df_no_dups['match'] = temp_df_no_dups['match'].apply(lambda x: [x])\n",
    "\n",
    "# indices = []\n",
    "\n",
    "# # for i, row in temp_df_no_dups.iterrows():\n",
    "# #     indices.append(i)\n",
    "# #     temp_df_no_dups = temp_df_no_dups.drop(\n",
    "# #         index=[item for item in row[\"match\"] if item not in indices], errors=\"ignore\"\n",
    "# #     )\n",
    "\n",
    "# # if search_region=='phoenix':\n",
    "# #     print()\n",
    "# #     print(F'search region: {search_region}')\n",
    "# #     for i, row in temp_df_no_dups.iterrows():\n",
    "# #         indices.append(i)\n",
    "# #         drop_idx = []\n",
    "# #         print(i, row['match'])\n",
    "# #         try:\n",
    "# #             for item in row['match']:\n",
    "# #                 if item not in indices:\n",
    "# #                     drop_idx.append(item)\n",
    "# #             temp_df_no_dups = temp_df_no_dups.drop(index=drop_idx, errors=\"ignore\")\n",
    "# #         except Exception as e:\n",
    "# #             #print(i, item, row['match'])\n",
    "# #             print(e, i, item, row['match'])\n",
    "\n",
    "# print()\n",
    "# print(F'search region: {search_region} starting')\n",
    "# for i, row in temp_df_no_dups.iterrows():\n",
    "#     indices.append(i)\n",
    "#     drop_idx = []\n",
    "#     #print(i, row['match'])\n",
    "#     try:\n",
    "#         for item in row['match']:\n",
    "#             if item not in indices:\n",
    "#                 drop_idx.append(item)\n",
    "#         temp_df_no_dups = temp_df_no_dups.drop(index=drop_idx, errors=\"ignore\")\n",
    "#     except Exception as e:\n",
    "#         #print(i, item, row['match'])\n",
    "#         print(e, i, item, row['match'])\n",
    "# print(F'search region: {search_region} complete')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Append each temporary df to a list, which we can concatenate into one larger \n",
    "    # df, later.\n",
    "\n",
    "\n",
    "\n",
    "    df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "nearby-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for errors in getting text from a post, or from getting the URL of a post.\n",
    "len(error_list_text), len(error_list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "endangered-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1326, 8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dfs for each region into one larger df and check its shape.\n",
    "concat_df = pd.concat(df_list, ignore_index=True)\n",
    "concat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2e528-e3db-4b1d-b869-afba2b9b709c",
   "metadata": {},
   "source": [
    "### Dropping Duplicate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "spanish-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     668\n",
       "False    658\n",
       "Name: post_text, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get date of html request to label our output with.\n",
    "date_of_html_request = str(dt.date.today())\n",
    "\n",
    "# Include the date posts were scraped on to track tutoring prices over time.\n",
    "concat_df['posts_scraped_on'] = date_of_html_request\n",
    "\n",
    "# Count duplicates.\n",
    "concat_df['post_text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "52774d13-f298-4586-be24-e5f33cf0e8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658, 10)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find indices of duplicate results, then drop them and reset indices.\n",
    "duplicate_indices = concat_df[concat_df['post_text'].duplicated()==True].index\n",
    "df_exact_txt_dropped = concat_df.drop(index=duplicate_indices)\n",
    "df_exact_txt_dropped = df_exact_txt_dropped.reset_index(drop=True)\n",
    "df_exact_txt_dropped['len_of_price_list']=df_exact_txt_dropped['price_list'].apply(lambda x: len(x))\n",
    "df_exact_txt_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e30cd031-8c4b-4907-86b5-77467325ee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 141],\n",
       "       [  1,  31],\n",
       "       [  1,  37],\n",
       "       ...,\n",
       "       [646, 643],\n",
       "       [655, 656],\n",
       "       [656, 655]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_comparison = df_exact_txt_dropped['post_text']\n",
    "vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "tfidf = vect.fit_transform(text_for_comparison)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "pairwise_array = pairwise_similarity.toarray()\n",
    "np.fill_diagonal(pairwise_array, np.nan)\n",
    "argwhere_array = np.argwhere(pairwise_array > 0.63)\n",
    "argwhere_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b6737df-7701-4f01-a968-898e60b48476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_row_idx = []\n",
    "dup_row_idx = []\n",
    "for row in argwhere_array:\n",
    "    current_idx = row[0]\n",
    "    #print(F\"Current row: {row}, Current idx: {current_idx}\")\n",
    "    duplicate_list = []\n",
    "    if current_idx in df_row_idx:\n",
    "        continue\n",
    "    else:\n",
    "        df_row_idx.append(current_idx)\n",
    "    for other_row in argwhere_array:\n",
    "        other_idx = other_row[1]\n",
    "        #print(F\"Here's the other_row: {other_row}, Other idx: {other_idx}\")\n",
    "        if current_idx == other_row[0]:\n",
    "            duplicate_list.append(other_idx)\n",
    "    #print(F\"This is the current dup_list: {duplicate_list}\")\n",
    "    #print()\n",
    "    dup_row_idx.append(duplicate_list)\n",
    "#list(zip(df_row_idx, dup_row_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "48949840-611f-496c-b1a3-adf81253b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rancher/opt/anaconda3/envs/ox/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n",
      "/Users/rancher/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/internals/blocks.py:937: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n"
     ]
    }
   ],
   "source": [
    "df_exact_txt_dropped['match'] = np.array(df_exact_txt_dropped.index.values, dtype='object')\n",
    "df_exact_txt_dropped['match'] = df_exact_txt_dropped['match'].apply(lambda x: [x])\n",
    "\n",
    "match_col_idx = df_exact_txt_dropped.columns.get_loc('match')\n",
    "df_exact_txt_dropped.iloc[df_row_idx, match_col_idx] = dup_row_idx\n",
    "#df_exact_txt_dropped['match'] = df_exact_txt_dropped['match'].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "44365784-710f-4abe-b872-8f31c2eec7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [141]\n",
       "1      [31, 37, 217, 354, 456, 485, 615]\n",
       "2                 [3, 68, 130, 342, 384]\n",
       "3                 [2, 68, 130, 342, 384]\n",
       "4                                    [4]\n",
       "                     ...                \n",
       "653                                [653]\n",
       "654                                [654]\n",
       "655                                [656]\n",
       "656                                [655]\n",
       "657                                [657]\n",
       "Name: match, Length: 658, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exact_txt_dropped['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "52b39abd-71d1-4352-a29c-7ef24255b7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sby/lss/d/sunnyvale-expert-tutor-for-writing/7430472194.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nDon't wait to get a tutor! This applies to struggling students or brilliant ones, as I can help both achieve their best.\\n\\n\\n\\nWould your student perform better in his or her subjects with a weekly tutor? I am here to help. I am a master tutor with fifteen years of experience bringing out the best of my students. I can help your student improve their English and Math skills, and maximize both learning and grades.\\n\\n\\nI earned a graduate degree from Harvard University, and I specialize adapting my teaching style to each unique student. Additionally, I have lived overseas for five years, and so I have a speciality helping students who have English as a second language (ESL). I also am an expert SAT and ACT tutor.\\n\\n\\nI can help you with the following:ENGLISH: English classes,Writing, Grammar, StyleHISTORY & OTHER LIBERAL ARTS: History, Social Studies, Government, Political Science, etc.MATH: Arithmetic, Fractions, Algebra, Geometry, TrigonometrySCIENCE: Chemistry, Biology, Physics\\n\\n\\nIndividualized Tutoring\\nEach student is different and I listen carefully so that our sessions are most productive. There is no substitute for one-on-one tutoring.\\n\\nShort Term or Long Term\\nSome students have stayed with me their entire high school career. Others need just a bit of help to complete a difficult assignment. \\n\\n\\nRates\\nIndividualized tutoring costs $90/hr for in-person or $60/hr for online. (AT LEAST UNTIL MAY \\n 2022 CORONA VIRUS PRICING: $40/hr for online weekdays.) The introductory meeting is free without any obligation. I tutor online or in-person, and I use google docs and a whiteboard program for online students. Online tutoring works very well.\\n\\n\\nNOTE: If your income has been disrupted due to the coronavirus, please indicate the hourly rate you are able to pay. \\n\\n\\nContact\\nYou can text me or simply reply to this ad and we'll arrange a Zoom meeting.\\n\\n\\nkenaritutor.com\\n    \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=0\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_exact_txt_dropped.iloc[x]['link'])\n",
    "  display(df_exact_txt_dropped.iloc[x]['post_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "460d823f-6380-43e4-865b-83c166f48f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sacramento.craigslist.org/lss/d/elk-grove-certified-teacher-stanford/7429909819.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n(916) 238-6790\\n\\nMy name is Samantha and I’m a former teacher.\\n\\nMy rates are very reasonable and affordable. I tutor in-person and online.\\n\\nI tutor students of all ages and for all subjects. I tutor 7 days a week and I have a ton of material I can share with students including over 15 years of previous exams as well as many unique study guides I helped create.\\n\\nIf you are looking for tutoring for tests such as the MCAT, SAT, ACT, GRE, GMAT, LSAT, or any subject, I would be the perfect tutor. I've taken all of the tests listed above (because I’ve been a tutor for several years) and I scored in the top 5% in each.\\n\\nI received:\\n\\n36 on the ACT\\n1580 on the SAT\\n178 on the LSAT\\n525 on the MCAT\\n780 on the GMAT\\n334 on the GRE.\\n\\nI graduated from Stanford University as an undergrad and recently graduated with a Ph.D. from Harvard.\\n\\nAside from helping students with improving their test scores, I can also help students with their math (Algebra, Geometry, Trigonometry, Pre-Calculus, Calculus, Statistics), science (Biology, Chemistry, Physics), and English classes/exams/papers/college and grad school applications. I enjoy tutoring AP classes also.\\n\\nRegards,\\nSamantha\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSacramento, California, Alta Sierra, Antelope, Arden-Arcade, Auburn, Beale Air Force Base, Cameron Park, Carmichael, Challenge–Brownsville, Citrus Heights, Colfax, Davis, Diamond Springs, Dollar Point, El Dorado Hills, Elk Grove, Elverta, Esparto, Fair Oaks, Florin, Folsom, Foothill Farms, Foresthill, Galt, Georgetown, Gold River, Granite Bay, Grass Valley, Isleton, Kings Beach, La Riviera, Lake of the Pines, Lake Wildwood, Lincoln, Linda, Live Oak, Loma Rica, Loomis, Marysville, Meadow Vista, Nevada City, North Auburn, North Highlands, Olivehurst, Orangevale, Parkway–South Sacramento, Penn Valley, Placerville, Plumas Lake, Pollock Pines, Rancho Cordova, Rancho Murieta, Rio Linda, Rocklin, Rosemont, Roseville, Sacramento, Shingle Springs, South Lake Tahoe, South Yuba City, Sunnyside–Tahoe City, Sutter, Tahoe Vista, Tierra Buena, Truckee, Vineyard, Walnut Grove, West Sacramento, Wheatland, Wilton, Winters, Woodland, Yuba City, CA\\n    \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=471\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_exact_txt_dropped.iloc[x]['link'])\n",
    "  display(df_exact_txt_dropped.iloc[x]['post_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ae76bd3-20ac-443b-b254-ad184673580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "\n",
    "# for i, row in temp_df_exact_txt_dropped.iterrows():\n",
    "#     indices.append(i)\n",
    "#     temp_df_exact_txt_dropped = temp_df_exact_txt_dropped.drop(\n",
    "#         index=[item for item in row[\"match\"] if item not in indices], errors=\"ignore\"\n",
    "#     )\n",
    "\n",
    "# if search_region=='phoenix':\n",
    "#     print()\n",
    "#     print(F'search region: {search_region}')\n",
    "#     for i, row in temp_df_exact_txt_dropped.iterrows():\n",
    "#         indices.append(i)\n",
    "#         drop_idx = []\n",
    "#         print(i, row['match'])\n",
    "#         try:\n",
    "#             for item in row['match']:\n",
    "#                 if item not in indices:\n",
    "#                     drop_idx.append(item)\n",
    "#             temp_df_exact_txt_dropped = temp_df_exact_txt_dropped.drop(index=drop_idx, errors=\"ignore\")\n",
    "#         except Exception as e:\n",
    "#             #print(i, item, row['match'])\n",
    "#             print(e, i, item, row['match'])\n",
    "\n",
    "df_no_dups = df_exact_txt_dropped.copy()\n",
    "\n",
    "for i, row in df_no_dups.iterrows():\n",
    "    indices.append(i)\n",
    "    drop_idx = []\n",
    "    #print(i, row['match'])\n",
    "    try:\n",
    "        for item in row['match']:\n",
    "            if item not in indices:\n",
    "                drop_idx.append(item)\n",
    "        df_no_dups = df_no_dups.drop(index=drop_idx, errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        #print(i, item, row['match'])\n",
    "        print(e, i, item, row['match'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24427708-b9e4-4b53-b0f2-0f2ec6962382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((658, 11), (346, 11))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exact_txt_dropped.shape, df_no_dups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934c9f5-05eb-4048-b07b-d26d16edbb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1527c-6009-4291-987c-8c735a1954be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "literary-compound",
   "metadata": {},
   "source": [
    "### Dropping posts that contained no prices, which aren't helpful for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "434e8abf-7b66-4232-9059-93272966788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dups['len_of_price_list'] = df_no_dups['price_list'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "guided-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out results that don't have a price and reset indices.\n",
    "df_with_prices = df_no_dups[df_no_dups['len_of_price_list'] > 0]\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "273eca8b-0205-4458-9536-0cebd167096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 11)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "residential-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1326, there were 346 posts that weren't duplicated, or 26.09%.\n",
      "There were 142 posts that had prices included and weren't duplicates.\n",
      "Only 10.71% of the posts that we scraped remain.\n"
     ]
    }
   ],
   "source": [
    "unique_posts_count = len(df_no_dups)\n",
    "post_with_prices_count = len(df_with_prices)\n",
    "num_posts = len(concat_df)\n",
    "\n",
    "percent_unique = unique_posts_count / num_posts * 100\n",
    "percent_with_prices = post_with_prices_count / num_posts * 100\n",
    "\n",
    "print(F\"Out of {num_posts}, there were {unique_posts_count} posts that weren't duplicated, or {percent_unique:.2f}%.\")\n",
    "print(F\"There were {post_with_prices_count} posts that had prices included and weren't duplicates.\")\n",
    "\n",
    "print(F\"Only {percent_with_prices:.2f}% of the posts that we scraped remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-start",
   "metadata": {},
   "source": [
    "### Extracting complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-measurement",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *Transforming* Craigslist data: Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-andrew",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Are there any posts that might need manual cleaning?  This would include:\n",
    "* Posts that had 3 or more prices and were marked as null\n",
    "* Posts where the price wasn't able to convert from `str` -> `int` and were marked as null during pre-processing\n",
    "\n",
    "There are the entries that were marked as `Null`.  Let's investigate them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5118dd4a-b106-4d3c-830b-42db498f7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[90, 60, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[40, 40, 45, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 35, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 30, 60, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[60, 50, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[100, 115, 130, 65, 30, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[40, 40, 40, 40, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[50, 10, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[50, 100, 135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[40, 80, 40, 10, 40, 30, 40, 80, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[25, 45, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[35, 35, 40, 40, 55, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[45, 55, 40, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[65, 55, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[20, 25, 30, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[100, 115, 130, 65, 30, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[50, 50, 35, 30, 25, 55, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[25, 30, 50, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[20, 25, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 40, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[20, 30, 20, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[25, 50, 25]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price                             price_list\n",
       "0      NaN                           [90, 60, 40]\n",
       "3      NaN                       [40, 40, 45, 45]\n",
       "9      NaN                           [30, 35, 45]\n",
       "16     NaN                       [30, 30, 60, 90]\n",
       "19     NaN                          [60, 50, 100]\n",
       "25     NaN            [100, 115, 130, 65, 30, 60]\n",
       "29     NaN                   [40, 40, 40, 40, 40]\n",
       "31     NaN                           [50, 10, 50]\n",
       "32     NaN                         [50, 100, 135]\n",
       "34     NaN                           [30, 50, 60]\n",
       "36     NaN  [40, 80, 40, 10, 40, 30, 40, 80, 100]\n",
       "43     NaN                           [25, 45, 25]\n",
       "44     NaN               [35, 35, 40, 40, 55, 80]\n",
       "49     NaN                       [45, 55, 40, 50]\n",
       "54     NaN                           [65, 55, 55]\n",
       "56     NaN                       [20, 25, 30, 30]\n",
       "64     NaN            [100, 115, 130, 65, 30, 60]\n",
       "68     NaN           [50, 50, 35, 30, 25, 55, 30]\n",
       "95     NaN                       [25, 30, 50, 50]\n",
       "98     NaN                           [20, 25, 30]\n",
       "122    NaN                           [30, 40, 50]\n",
       "130    NaN                       [20, 30, 20, 30]\n",
       "137    NaN                           [25, 50, 25]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null_prices = df_with_prices[df_with_prices['price'].isnull()==True]\n",
    "df_null_prices[['price', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cbfb0dec-46fb-4693-b660-10214d8f6b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 23 posts with price marked null.\n"
     ]
    }
   ],
   "source": [
    "posts_with_mult_prices = df_null_prices.shape[0]\n",
    "print(F\"There were {posts_with_mult_prices} posts with price marked null.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a6106d30-41ee-47f0-9695-22b95ad7e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_prices = df_null_prices.drop(columns=['len_of_price_list', 'match'])\n",
    "df_null_prices.to_csv('./posts_to_investigate/{}_posts_with_null_prices.csv'.format(date_of_html_request), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "930b77ec-483e-4b34-803b-3032be8e5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/pen/lss/d/san-mateo-math-emcsci-english-chem-bio/7429754585.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect links manually, one by one, to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=3\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-integration",
   "metadata": {},
   "source": [
    "### Cleaning posts with three or more prices manually - distilling down to one price\n",
    "\n",
    "We distill posts that had more complicated text that involved three or more prices, such as :\n",
    "\n",
    "* $40$/hr, $50$/1.5hr, $60$/2hr\n",
    "  * Complicated pricing schedule\n",
    "* $40$/hr but $10$ additional per person, if a group session is desired\n",
    "  * Group rates\n",
    "* $30$/hr Science, $40$/hr math, come and try a first session for the reduced price of $20$.\n",
    "  * Special offers\n",
    "\n",
    "into a single price.  Other posts repeated their prices multiple times, so we distill those down to a single price as well, then mark any of the entries we changed as being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "raised-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_col_idx = df_with_prices.columns.get_loc('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5484075c-56c5-4c45-a037-4a0d7320078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $40 for in person, or $45 for at home, so I took the average.\n",
    "san_mateo_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('I mainly tutor, in person, at the Downtown Redwood City, downtown San Mateo')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[san_mateo_tutor_idx,price_col_idx] = 42.5\n",
    "\n",
    "except:\n",
    "    print(\"Issue with san_mateo_tutor and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "musical-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the ad says $90 in person, $60 for online, and Corona Virus pricing of\n",
    "# $40 for online weekdays, I'm using the $40 per hour rate because it seems the\n",
    "# most reasonable.\n",
    "kenari_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('kenaritutor.com')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[kenari_tutor_idx,price_col_idx] = 40\n",
    "except:\n",
    "    print('Issue with kenari_tutor_idx and iloc.')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "511ec79c-4a71-4cbf-9efd-466b286386b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad mentions several prices for different subjects, but explicitly says $30 for math.\n",
    "la_honda_idx = df_with_prices[df_with_prices['post_text'].str.contains('909-640-3570')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[la_honda_idx,price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with la_honda_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14ca5d7e-f730-4fc3-841a-c70205c847a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says #60 per hour.\n",
    "glasses_lady_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"offering virtual one-on-one Math tutoring via Zoom\")==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[glasses_lady_idx, price_col_idx] = 60\n",
    "except:\n",
    "    print(\"Issue with glasses_lady_idx and iloc.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e1402316-6126-4b99-ab97-5ed93f0238bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says #60 per hour.\n",
    "UC_Davis_data_scientist = df_with_prices[df_with_prices['post_text'].str.contains(\"PhD in Engineering from UC Davis\")==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[UC_Davis_data_scientist, price_col_idx] = 60\n",
    "except:\n",
    "    print(\"Issue with UC_Davis_data_scientist and iloc.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "286df5e3-4fcb-481e-b8b1-238ecc789fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This guy has weird price structuring, but I used his hourly rate for each time interval, $100 for 80 minutes, $115 for 100 minutes, $130 for 120 minutes, then averaged those hourly rates to estimate for what a single hour would cost.\n",
    "oakland_exp_tutor_online_idx = df_with_prices[df_with_prices['post_text'].str.contains('I received a full scholarship to University of Cincinnati and held a 3.8 GPA through my master’s program in aerospace')==True].index\n",
    "\n",
    "oakland_tutor_avg_rate = ((100/80) + (115/100) + (130/120)) * 60 / 3\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[oakland_exp_tutor_online_idx, price_col_idx] = oakland_tutor_avg_rate\n",
    "\n",
    "except:\n",
    "    print(\"Issue with oakland_exp_tutor_online_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "competitive-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ad repeats the price of $40 over and over, so I'm replacing the price with \n",
    "# a single instance.\n",
    "star_star_college_math_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('https://www.youtube.com/channel/UCqhFZRmUqOAAPMQpo58TV7g'\n",
    "                   ) == True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[star_star_college_math_tutor_idx, price_col_idx] = 40\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with star_star_college_math_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "75bacbee-fda4-40d8-b864-f086777b76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $50/hr    \n",
    "trevor_skelly_idx = df_with_prices[df_with_prices['post_text'].str.contains('trevorskelly')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[trevor_skelly_idx,price_col_idx] = 50\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with trevor_skelly_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "sophisticated-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $50 per hour for sessions under 3 hours\n",
    "spss_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('datameer', case=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[spss_tutor_idx, price_col_idx] = 50\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with spss_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d6f8a00e-df48-4e40-9b18-886f989c6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $50 per hour\n",
    "tutor_sam_idx = df_with_prices[df_with_prices['post_text'].str.contains('thetutorsam')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[tutor_sam_idx, price_col_idx] = 50\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with tutor_sam_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "nonprofit-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $40 per hour\n",
    "peter_d_idx = df_with_prices[df_with_prices['post_text'].str.contains('Peter D.')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[peter_d_idx, price_col_idx] = 40\n",
    "except:\n",
    "    print(\"Issue with peter_d_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "289270da-cd1d-4aac-b1cb-a4070238e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $45 per hour for individual lessons\n",
    "algebra_exclusively_idx = df_with_prices[df_with_prices['post_text'].str.contains('algebra EXCLUSIVELY')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[algebra_exclusively_idx, price_col_idx] = 45\n",
    "except:\n",
    "    print(\"Issue with algebra_exclusively_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "improving-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post includes many prices, but states $55/hr for Precalc and $80/hr for Calculus, so I took the average of those prices\n",
    "aerospace_engineer_idx = df_with_prices[df_with_prices['post_text'].str.contains('in the aerospace industry looking', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[aerospace_engineer_idx, price_col_idx] = (55 + 80)/2\n",
    "\n",
    "except:\n",
    "    print(\"Issue with aerospace_engineer_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "careful-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad mentions $45 for lower division college courses, which are a large segment of the subjects I help with, so I'm using that price to compare myself against.\n",
    "ucb_phd_student_and_ta_idx = df_with_prices[df_with_prices['post_text'].str.contains('Former UC-Berkeley economics Ph.D. student and TA')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucb_phd_student_and_ta_idx, price_col_idx] = 45\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucb_phd_student_and_ta_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "identical-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The add says $55/hr for K-12, then $65/hr for AP/Honors, as well as Pre-calc, \n",
    "# etc., I'm going to average the two prices.  Set needs cleaning column to False \n",
    "# b/c the prices have been cleaned.\n",
    "park_academy_idx = df_with_prices[df_with_prices['post_text'].str.contains('(949) 490-0872', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[park_academy_idx, price_col_idx] = 60\n",
    "\n",
    "except:\n",
    "    print(\"Issue with park_academy_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "explicit-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $25/hr for high school, $30/hr for college, just went with $30/hr\n",
    "sharp_mind_idx = df_with_prices[df_with_prices['post_text'].str.contains('(650) 398-9490', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[sharp_mind_idx, price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with sharp_mind_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d4988ca4-9ad0-4703-a94a-ed4065e4523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $50/hr if travelling, $30-35/hr if virtual, so I took the average of 50 and 35\n",
    "stock_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('714.425.3828', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[stock_tutor_idx, price_col_idx] = (35 + 50)/2\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with stock_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "formal-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post says $30/hr for Precalc/Trig and $50/hr for Calculus, so I took the average\n",
    "lonzo_tutoring_idx = df_with_prices[df_with_prices['post_text'].str.contains('951-795-5027', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[lonzo_tutoring_idx, price_col_idx] = 40\n",
    "\n",
    "except:\n",
    "    print(\"Issue with lonzo_tutoring_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "smoking-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $30 for one hour.\n",
    "poway_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('(619)735-2579', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[poway_tutor_idx, price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with poway_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "average-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $20/hr online, $30/hr in person, split the difference at $25\n",
    "austin_sabrina_idx = df_with_prices[df_with_prices['post_text'].str.contains('My girlfriend Sabrina')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[austin_sabrina_idx, price_col_idx] = 25\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with austin_sabrina_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "catholic-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $25/hr\n",
    "alex_farrell_idx = df_with_prices[df_with_prices['post_text'].str.contains('Alexander Farrell')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[alex_farrell_idx, price_col_idx] = 25\n",
    "\n",
    "except:\n",
    "    print(\"Issue with alex_farrell_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12b76bc0-85a3-495c-b385-4972b767534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $25/hr if meeting near CSU Sac, $35/hr if they drive to you, $20/hr for online.\n",
    "# I chose $30/hr to split the difference between the in person prices.\n",
    "best_math_idx = df_with_prices[df_with_prices['post_text'].str.contains('bestmathtutoring.com')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[best_math_idx, price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with best_math_idx and iloc.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "internal-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucla_grad_henry_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"916 390-7923\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucla_grad_henry_idx, price_col_idx] = 35\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucla_grad_henry_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5023b-7234-4c18-a3bc-476cfc8cdbc9",
   "metadata": {},
   "source": [
    "#### Checking results - Are there any posts that were marked as needing to be cleaned that we missed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "designed-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 posts that need cleaning.\n"
     ]
    }
   ],
   "source": [
    "num_still_null = len(df_with_prices[df_with_prices['price'].isnull()==True])\n",
    "\n",
    "if num_still_null==0:\n",
    "    print(\"There are no posts with null prices still needing cleaning.\")\n",
    "else:\n",
    "    print(F\"There are {num_still_null} posts that need cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce8997-a97e-46ce-bdd4-401d75a6f997",
   "metadata": {},
   "source": [
    "### Checking Posts that have two prices listed to see if averaging them is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "72f4dd25-cd1a-4e13-bc8c-4526d76f150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46.0</td>\n",
       "      <td>[57, 35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40.0</td>\n",
       "      <td>[35, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[60, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>57.5</td>\n",
       "      <td>[45, 70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>46.5</td>\n",
       "      <td>[80, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100.0</td>\n",
       "      <td>[80, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>70.0</td>\n",
       "      <td>[60, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>104.5</td>\n",
       "      <td>[84, 125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[40, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47.5</td>\n",
       "      <td>[45, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>65.0</td>\n",
       "      <td>[60, 70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>70.0</td>\n",
       "      <td>[60, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[50, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>42.5</td>\n",
       "      <td>[45, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20.0</td>\n",
       "      <td>[20, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[40, 70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>70.0</td>\n",
       "      <td>[60, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>100.0</td>\n",
       "      <td>[115, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>30.0</td>\n",
       "      <td>[30, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>32.5</td>\n",
       "      <td>[25, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>40.0</td>\n",
       "      <td>[30, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[35, 65]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>82.5</td>\n",
       "      <td>[65, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>150.0</td>\n",
       "      <td>[200, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[55, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>105.0</td>\n",
       "      <td>[50, 160]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>30.0</td>\n",
       "      <td>[30, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>20.0</td>\n",
       "      <td>[15, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>27.5</td>\n",
       "      <td>[45, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>25.0</td>\n",
       "      <td>[40, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>45.0</td>\n",
       "      <td>[30, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>37.5</td>\n",
       "      <td>[45, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>37.5</td>\n",
       "      <td>[30, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>22.5</td>\n",
       "      <td>[20, 25]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price  price_list\n",
       "5     46.0    [57, 35]\n",
       "7     40.0    [35, 45]\n",
       "12    55.0    [60, 50]\n",
       "27    57.5    [45, 70]\n",
       "30    46.5    [80, 13]\n",
       "35   100.0   [80, 120]\n",
       "38    70.0    [60, 80]\n",
       "40    55.0    [50, 60]\n",
       "41   104.5   [84, 125]\n",
       "45    50.0    [40, 60]\n",
       "47    47.5    [45, 50]\n",
       "48    65.0    [60, 70]\n",
       "51    70.0    [60, 80]\n",
       "53    50.0    [50, 50]\n",
       "65    42.5    [45, 40]\n",
       "81    20.0    [20, 20]\n",
       "83    55.0    [40, 70]\n",
       "84    70.0    [60, 80]\n",
       "85   100.0   [115, 85]\n",
       "90    30.0    [30, 30]\n",
       "92    32.5    [25, 40]\n",
       "93    40.0    [30, 50]\n",
       "96    50.0    [35, 65]\n",
       "100   82.5   [65, 100]\n",
       "107  150.0  [200, 100]\n",
       "109   55.0    [55, 55]\n",
       "111  105.0   [50, 160]\n",
       "113   30.0    [30, 30]\n",
       "114   20.0    [15, 25]\n",
       "116   27.5    [45, 10]\n",
       "117   25.0    [40, 10]\n",
       "119   45.0    [30, 60]\n",
       "131   37.5    [45, 30]\n",
       "133   37.5    [30, 45]\n",
       "135   55.0    [50, 60]\n",
       "140   22.5    [20, 25]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[df_with_prices['len_of_price_list']==2][['price','price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc8e2d5d-7df4-43e0-b914-9be1a1967153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dallas.craigslist.org/ndf/lss/d/plano-private-math-physics-tutor/7429037374.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nIn-person tutor for mathematics and physics. Range of topics include:\\n\\nMath: Pre-algebra through calculus\\nPhysics: High school through college freshman/sophomore level\\n\\nCredentials:\\nMS in Physics, MS in Electrical Engineering\\nBS in Physics, BS in Mathematics (Honors)\\nFour years private math tutoring experience\\nFour years formal physics teaching experience\\nSeven years industry work as an electrical engineer at US defense contractor\\n\\nFlexible schedule.  Willing to travel.  Rate is $45/hour.  Feel free to contact with any questions or to set up a session!\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=136\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e02647-d05b-40bc-9776-9fd62f92b97e",
   "metadata": {},
   "source": [
    "#### Ads where averaging doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cc06f34c-357f-4508-88ff-7cf515eef0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says 35$/half hour, but explicitly says $57 per hour, so averaging doesn't make sense.  \n",
    "blake_tutoring_idx = df_with_prices[df_with_prices['post_text'].str.contains('BlakeTutoring.com', case=False)==True].index\n",
    "\n",
    "df_with_prices.iloc[blake_tutoring_idx, price_col_idx] = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "allied-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $84/hr but then mentions a $125 for 1.5 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $84\n",
    "test_trainer_inc_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"TestTrainerinc\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[test_trainer_inc_idx, price_col_idx] = 84\n",
    "\n",
    "except:\n",
    "    print(\"Issue with test_trainer_inc_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4ab8ea92-15a6-43b2-b913-a86880a2f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $60/45mins, but $80 per hour.  Either price comes out to the same hourly rate, so averaging doesn't make sense.\n",
    "hiro_kobayashi_idx = df_with_prices[df_with_prices['post_text'].str.contains('415-250-4831', case=False)==True].index\n",
    "\n",
    "df_with_prices.iloc[hiro_kobayashi_idx, price_col_idx] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bb557fbe-9c04-4f3b-a253-44be8dbcceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $40/1hr, $70/2hr, so averaging doesn't make sense\n",
    "guy_with_suit_idx = df_with_prices[df_with_prices['post_text'].str.contains('trained mathematician with about 20 years experience')==True].index\n",
    "\n",
    "df_with_prices.iloc[guy_with_suit_idx, price_col_idx] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "74a848a8-b063-4cce-9aef-4fd838551210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $25/1hr, $40/2hr, so averaging doesn't make sense\n",
    "christian_cerritos_college_idx = df_with_prices[df_with_prices['post_text'].str.contains('trained mathematician with about 20 years experience')==True].index\n",
    "\n",
    "df_with_prices.iloc[christian_cerritos_college_idx, price_col_idx] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "04b6fe95-7589-4650-b38f-ca229bdb16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $30/half hr, $50/1hr, so averaging doesn't make sense\n",
    "dustin_csu_long_beach_idx = df_with_prices[df_with_prices['post_text'].str.contains('International Society of Automation')==True].index\n",
    "\n",
    "df_with_prices.iloc[dustin_csu_long_beach_idx, price_col_idx] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f7298c75-8a00-4705-90a4-0b3e7ae5a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $65/hr for subject tutoring, $100/hr for standardized tests.  I'm primarily competing against subject tutoring, so I'll use that price\n",
    "smarter_than_you_think_idx = df_with_prices[df_with_prices['post_text'].str.contains('guarantee you are smarter than you think')==True].index\n",
    "\n",
    "df_with_prices.iloc[smarter_than_you_think_idx, price_col_idx] = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "01101fb3-221a-482e-9e36-0280d415e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $50/hr or $160/4hr, so it doesn't make sense to average.\n",
    "dead_in_ditch_idx = df_with_prices[df_with_prices['post_text'].str.contains('dead in a ditch')==True].index\n",
    "\n",
    "df_with_prices.iloc[dead_in_ditch_idx, price_col_idx] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "124abde4-be0f-4d62-b675-417fa672cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $45/hr +$10 more per student, so it doesn't make sense to average.\n",
    "distinguished_teacher_idx = df_with_prices[df_with_prices['post_text'].str.contains('\"Distinguished Teacher\"')==True].index\n",
    "\n",
    "df_with_prices.iloc[distinguished_teacher_idx, price_col_idx] = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ad1a8716-a2ef-4661-b162-d3e959f92b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $40/hr +$10 more for each additional person, so it doesn't make sense to average.\n",
    "vahab_idx = df_with_prices[df_with_prices['post_text'].str.contains('vababtaghizade@gmail.com')==True].index\n",
    "\n",
    "df_with_prices.iloc[vahab_idx, price_col_idx] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "25252b56-959e-46de-9b52-95834da63513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $30/hr for trial session, then $60/hr afterwards, so it doesn't make sense to average.\n",
    "myles_ahead_idx = df_with_prices[df_with_prices['post_text'].str.contains('mylesaheadtutoring')==True].index\n",
    "\n",
    "df_with_prices.iloc[myles_ahead_idx, price_col_idx] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "11aa0f22-d46b-4295-b318-94fb14053bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $45/hr, then talks about selling a workbook for $30, so it doesn't make sense to average.\n",
    "john_the_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('480-343-2212')==True].index\n",
    "\n",
    "df_with_prices.iloc[john_the_tutor_idx, price_col_idx] = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912951a-bc71-4baf-8f94-192f579e14a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851f733-dc72-4d90-b754-23adcc139b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lovely-substance",
   "metadata": {},
   "source": [
    "## Investigating posts with extreme prices.  Are there any price outliers that we need to clean?\n",
    "\n",
    "Prices >= 100 or <= 20 are what I would consider to be extreme prices.  Let's investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "58751ba1-6c7d-40b7-b22f-62db9a52a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.0</td>\n",
       "      <td>\\n\\n\\n\\n\\n*****I am currently offering both Zo...</td>\n",
       "      <td>[120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...</td>\n",
       "      <td>[80, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>150.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[150]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>200.0</td>\n",
       "      <td>\\n\\n\\n\\n\\ncheck out my website!\\nmd-maker.com\\...</td>\n",
       "      <td>[200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>19.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>15.0</td>\n",
       "      <td>\\n\\n\\n\\n\\njargon free math tutor $15 all level...</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nWhy I am an exceptional tutor: \\n\\nF...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>18.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nSAT prep for as low as $18 per hour!...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nExperienced tutor (more than 16 year...</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHAPPY NEW YEAR!!!\\n\\nYes, you read r...</td>\n",
       "      <td>[20, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nIf you would like your K-through-5th...</td>\n",
       "      <td>[115, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>120.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...</td>\n",
       "      <td>[120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>150.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name is Daniel. I graduate...</td>\n",
       "      <td>[200, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>120.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nFormer science teacher and current g...</td>\n",
       "      <td>[120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi everyone, do you need math, physi...</td>\n",
       "      <td>[15, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nTenured math professor at a major un...</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>10.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi! My name is Sher. I have a Bachel...</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>10.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Jai! I am offering...</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>17.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nI teach math to elementary kids, my ...</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price                                          post_text  price_list\n",
       "4    120.0  \\n\\n\\n\\n\\n*****I am currently offering both Zo...       [120]\n",
       "8     20.0  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...        [20]\n",
       "35   100.0  \\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...   [80, 120]\n",
       "37   150.0  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...       [150]\n",
       "55   200.0  \\n\\n\\n\\n\\ncheck out my website!\\nmd-maker.com\\...       [200]\n",
       "61    19.0  \\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...        [19]\n",
       "73    15.0  \\n\\n\\n\\n\\njargon free math tutor $15 all level...        [15]\n",
       "74    20.0  \\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...        [20]\n",
       "75    20.0  \\n\\n\\n\\n\\nWhy I am an exceptional tutor: \\n\\nF...        [20]\n",
       "76    18.0  \\n\\n\\n\\n\\nSAT prep for as low as $18 per hour!...        [18]\n",
       "77   100.0  \\n\\n\\n\\n\\nExperienced tutor (more than 16 year...       [100]\n",
       "81    20.0  \\n\\n\\n\\n\\nHAPPY NEW YEAR!!!\\n\\nYes, you read r...    [20, 20]\n",
       "85   100.0  \\n\\n\\n\\n\\nIf you would like your K-through-5th...   [115, 85]\n",
       "106  120.0  \\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...       [120]\n",
       "107  150.0  \\n\\n\\n\\n\\nG'day! My name is Daniel. I graduate...  [200, 100]\n",
       "112  120.0  \\n\\n\\n\\n\\nFormer science teacher and current g...       [120]\n",
       "114   20.0  \\n\\n\\n\\n\\nHi everyone, do you need math, physi...    [15, 25]\n",
       "118  100.0  \\n\\n\\n\\n\\nTenured math professor at a major un...       [100]\n",
       "126   10.0  \\n\\n\\n\\n\\nHi! My name is Sher. I have a Bachel...        [10]\n",
       "134   10.0  \\n\\n\\n\\n\\nHello! My name is Jai! I am offering...        [10]\n",
       "141   17.0  \\n\\n\\n\\n\\nI teach math to elementary kids, my ...        [17]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[(df_with_prices['price']>=100) | (df_with_prices['price']<=20)][['price', 'post_text', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9a4892b0-a385-4119-9897-659036333b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sby/lss/d/san-jose-math-enrichment-classes-for/7427917503.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nHi, I'm Mark, and I'm a tutor with six years of experience tutoring in math, English, biology, chemistry, SAT, and economics. I am currently teaching three math enrichment classes - Accelerated Algebra, Accelerated Precalculus/Calculus, and Calculus for Young Learners.\\n\\nAccelerated Algebra: In this course, the concepts behind Algebra 1 and Algebra 2 are combined with a focus on precalculus to challenge students and prepare them for the next level of math. My goal is to challenge the students to understand the next-level applications of the concepts they learn in algebra.\\n\\nAccelerated Precalculus/Calculus: This course is intended for students who have taken or are planning to take precalculus/calculus and want to learn at a faster, more challenging level. This is ideal for students planning to take the AP Calculus AB/BC Exams or College Math.\\n\\nCalculus for Young Learners:  is a special course intended for children who aspire to gain a deep understanding of the concepts unique to Calculus - limits, differentiation, and integration. Although this may appear to be a difficult class, I break it down into understandable bits that use intuitive concepts applicable to all levels of math.\\n\\nI love working with students of all ages - I currently tutor 20 students between the second and twelfth grades. I graduated high school at the age of 16 and I graduated from UC Berkeley with a B.A. in Molecular and Cell Biology at the age of 20 last month. I will be matriculating to medical school next fall.\\n\\nI'm free to tutor virtually over Zoom at most times on weekdays. I also provide math help for struggling students who benefit more from interactive and engaging teaching. I earned perfect scores on both the AP Calculus AB/BC exams, the SAT Math section, and the SAT Math Subject Test). I also coach and train basketball. Feel free to give me a call or email to schedule a trial lesson. I charge $50-$60/hour.    \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=40\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-belize",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dropping posts with extreme prices that aren't relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "green-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for poker tutoring/coaching, not really what I'm competing against, so we drop all instances.  He also mentions he tutors math in this post, but he has a separate post up that we've captured which has his math tutoring pricing information.\n",
    "australia_daniel_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"I'm available as a dealer if you need one\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=australia_daniel_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-storage",
   "metadata": {},
   "source": [
    "### Correcting pricing information for posts with extreme prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "radical-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $50/hr but then mentions a prepay plan for $160 for 4 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $50\n",
    "google_maps_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"willing to travel if Google Maps\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[google_maps_idx, price_col_idx] = 50\n",
    "\n",
    "except:\n",
    "    print(\"Issue with google_maps_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "nutritional-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $45/hr for high school or college, but then mentions a $35 for middle school.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $45, since I primarily tutor high school or college students.\n",
    "rancho_penasquitos_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"Rancho Penasquitos (Park Village Neighborhood)\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[rancho_penasquitos_idx, price_col_idx] = 45\n",
    "\n",
    "except:\n",
    "    print(\"Issue with rancho_penasquitos_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920c90f-dd8c-43ef-bbef-ebf45f58d031",
   "metadata": {},
   "source": [
    "### Transforming Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-jurisdiction",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *Load* - Saving results\n",
    "\n",
    "### Store results locally as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "spread-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.  CL links will expire after some number of days, the prices_need_cleaning and price_to_investigate columns have been manually inspected, and lastly we've distilled the multiple prices in the price_list down to a single value\n",
    "df_for_sql = df_with_prices.drop(labels=['link', 'price_list', 'len_of_price_list', 'match'], axis=1)\n",
    "\n",
    "# In order for psycopg2 to parse our CSV file correctly later, we need to escape all new line characters by adding an additional \\ in front of \\n.\n",
    "df_for_sql['post_text'] = df_for_sql['post_text'].str.replace('\\n', '\\\\n')\n",
    "\n",
    "# Store cleaned data as CSV file in preparation for importing to SQL database\n",
    "df_for_sql.to_csv(\"./csv_files/{}_all_regions_with_prices.csv\".format(date_of_html_request), index=False, sep=';')\n",
    "\n",
    "# Store original data, before we applied any cleaning to it, in case it's needed for something later on.\n",
    "concat_df.to_csv(\"./csv_files/{}_all_regions_posts.csv\".format(date_of_html_request), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-boards",
   "metadata": {},
   "source": [
    "### Importing into PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "conscious-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to PSQL database\n",
    "conn = psycopg2.connect(\"host=localhost dbname=rancher user=rancher\")\n",
    "\n",
    "# Instantiate a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Use cursor object to create a database for storing the information we scraped and cleaned, if one doesn't already exist.\n",
    "cur.execute(\"\"\"    \n",
    "    CREATE TABLE IF NOT EXISTS cl_tutoring2(\n",
    "    id SERIAL primary key,\n",
    "    date_scraped date,\n",
    "    price decimal,\n",
    "    city text,\n",
    "    subregion text,\n",
    "    region text,\n",
    "    post_text text,\n",
    "    date_posted timestamp\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "framed-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Copy data from our CSV file into database.  \n",
    "### Note, we can use the ; separator freely because we replaced all instances of semicolons in a post to commas during the preprocessing stage, ensuring that psycopg2 won't misinterpret a semicolon in the body of a post as a separator, splitting a row in the CSV file into too many columns as a result.\n",
    "### Also, we must specify null=\"\" because Python represents null values as an empty string when writing to a CSV file and psycopg2 needs to know how null values are represented in the CSV file in order to properly insert null values into the database\n",
    "with open('./csv_files/' + str(date_of_html_request) + '_all_regions_with_prices.csv', 'r') as file:\n",
    "    next(file) # Skip the header row\n",
    "    cur.copy_from(file, 'cl_tutoring2', sep=';', null=\"\", columns=('date_posted', 'price', 'city', 'subregion', 'region', 'post_text', 'date_scraped'))\n",
    "    \n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee0f05-73d9-4c56-85ae-291626e99d95",
   "metadata": {},
   "source": [
    "### Done!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-wheat",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "397e7879-89c5-405b-9185-5efacad9b6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>subregion</th>\n",
       "      <th>region</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "      <th>posts_scraped_on</th>\n",
       "      <th>len_of_price_list</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2022-01-08T05:45:44-0800</td>\n",
       "      <td>https://losangeles.craigslist.org/wst/lss/d/lo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Westside-Southbay</td>\n",
       "      <td>los_angeles</td>\n",
       "      <td>\\n\\n\\n\\n\\nElementary teacher/Home school Teach...</td>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>2022-01-09</td>\n",
       "      <td>3</td>\n",
       "      <td>[390]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date_posted  \\\n",
       "98  2022-01-08T05:45:44-0800   \n",
       "\n",
       "                                                 link  price  city  \\\n",
       "98  https://losangeles.craigslist.org/wst/lss/d/lo...    NaN  None   \n",
       "\n",
       "            subregion       region  \\\n",
       "98  Westside-Southbay  los_angeles   \n",
       "\n",
       "                                            post_text    price_list  \\\n",
       "98  \\n\\n\\n\\n\\nElementary teacher/Home school Teach...  [20, 25, 30]   \n",
       "\n",
       "   posts_scraped_on  len_of_price_list  match  \n",
       "98       2022-01-09                  3  [390]  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[df_with_prices['price'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3acabb25-fc98-456f-b7ba-6efaa1d30479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://losangeles.craigslist.org/wst/lss/d/los-angeles-elementary-virtual-remote/7430151416.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nElementary teacher/Home school Teacher/Private tutor  with over 10 years of teaching experience .  \\nServices provided: virtual/remote online support for elementary students (K-4th) using Zoom or other platform.    \\n\\nOnline support with teaching elementary subjects - Reading, Writing and Math, teaching English as a second language,  teaching social emotional learning tools and/or creating fun and engaging Science or Social Studies lessons. \\n\\nLessons will be engaging and interactive.      $20/hr  OR   $25 hr for 2 students OR $30/hr for 3-5 students. \\n\\n Payment via Paypal or Venmo.\\n    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=98\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298ab55-1540-4c01-b002-aac0cb052bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af291fc-3eea-45b2-9228-2ef0820d9302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378a575-b577-42a5-a5ef-18c5ea986a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac027c6-34dd-4897-ac55-2d5915db4557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cfb2a6a-dd99-4472-bc24-96138d29dd93",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Transforming Craigslist data REMOVING ENTRIES AND QUERYING WITH SQL LATER -- TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd605d-5ceb-4b2d-b343-f8ae0417eaef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Are there any posts that might need manual cleaning?  This would include:\n",
    "* Posts that had 3 or more prices and were marked as null\n",
    "* Posts where the price wasn't able to convert from `str` -> `int` and were marked as null during pre-processing\n",
    "\n",
    "I'll identify these posts, then remove them from our `DataFrame` to be analyzed later.  All remaining posts will have just a single price listed, which we can input to our SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65e34c-0bc3-408a-8f9d-215dd876136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the entries with 3 or more prices listed, let's investigate why\n",
    "df_null_prices = df_with_prices[df_with_prices['price'].isnull()==True]\n",
    "df_null_prices[['price', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1b971-5385-4c90-9a22-d17a21a8b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_mult_prices = df_null_prices.shape[0]\n",
    "print(F\"There were {posts_with_mult_prices} posts with price marked null.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302f1d5-432f-42fb-8a53-d427748cc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_price_idx = df_null_prices.index\n",
    "df_with_single_price = df_with_prices.drop(index=null_price_idx)\n",
    "df_with_single_price = df_with_single_price.reset_index(drop=True)\n",
    "df_with_single_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405892fe-4fb3-4b84-a2fb-09df8d335fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_prices = df_null_prices.drop(columns=['len_of_price_list', 'match'])\n",
    "df_null_prices.to_csv('./posts_to_investigate/{}_posts_with_null_prices.csv'.format(date_of_html_request), index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
