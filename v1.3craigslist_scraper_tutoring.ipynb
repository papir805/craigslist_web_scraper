{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import requests\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv \n",
    "import psycopg2\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I picked the 10 largest metropolitan areas by population to scrape data from, as well as Sacramento, since it's nearby to me and is another major city\n",
    "regions_to_scrape = ['sf_bay_area',\n",
    "                    'new_york',\n",
    "                    'los_angeles',\n",
    "                    'sacramento',\n",
    "                    'chicago',\n",
    "                    'san_diego',\n",
    "                    'houston',\n",
    "                    'phoenix',\n",
    "                    'philadelphia',\n",
    "                    'dallas',\n",
    "                    'san_antonio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-testament",
   "metadata": {},
   "source": [
    "# *Extract* Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "public-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Session and Retry object to manage the quota Craigslist imposes on HTTP get requests within a certain time period \n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a748d9e7-d53d-40e5-a24a-679531b7a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sf_bay_area 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "sf_bay_area 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "sf_bay_area 3 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for sf_bay_area received.  Process completed.\n",
      "\n",
      "new_york 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "new_york 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "new_york 3 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for new_york received.  Process completed.\n",
      "\n",
      "los_angeles 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "los_angeles 2 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for los_angeles received.  Process completed.\n",
      "\n",
      "sacramento 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for sacramento received.  Process completed.\n",
      "\n",
      "chicago 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for chicago received.  Process completed.\n",
      "\n",
      "san_diego 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for san_diego received.  Process completed.\n",
      "\n",
      "houston 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for houston received.  Process completed.\n",
      "\n",
      "phoenix 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for phoenix received.  Process completed.\n",
      "\n",
      "philadelphia 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for philadelphia received.  Process completed.\n",
      "\n",
      "dallas 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for dallas received.  Process completed.\n",
      "\n",
      "san_antonio 1 response received.\n",
      "Waiting 10 seconds...\n",
      "\n",
      "Last response for san_antonio received.  Process completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Walk through each region in our list of regions_to_scrape to get the HTML page corresponding to a search for \"math tutor\" in the services section\n",
    "\n",
    "response_dict = {}\n",
    "sleep_timer = 10\n",
    "\n",
    "for count, region in enumerate(regions_to_scrape):\n",
    "    # This gets the first page of search results\n",
    "    i=1\n",
    "    current_region = region.replace('_', '')\n",
    "    current_response = session.get('https://' + current_region + '.craigslist.org/d/services/search/bbb?query=math%20tutor&sort=rel')\n",
    "    print(F\"{region} {i} response received.\")\n",
    "    print(F\"Waiting {sleep_timer} seconds...\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(sleep_timer)\n",
    "    \n",
    "    region_response_list = []\n",
    "    region_response_list.append(current_response)\n",
    "\n",
    "    # This gets all subsequent pages, using the next button\n",
    "    is_next_button = True\n",
    "    while is_next_button:\n",
    "        i+=1\n",
    "        try:\n",
    "            next_response = current_response\n",
    "            next_soup = BeautifulSoup(next_response.text, 'html.parser')\n",
    "            html_suffix = next_soup.find(class_='button next').get('href')\n",
    "            if html_suffix != '':\n",
    "                new_button = 'https://' + current_region + '.craigslist.org' + html_suffix\n",
    "                current_response = session.get(new_button)\n",
    "                region_response_list.append(current_response)\n",
    "                \n",
    "                \n",
    "                time.sleep(sleep_timer)\n",
    "                print(F\"{region} {i} response received.\")\n",
    "                print(F\"Waiting {sleep_timer} seconds...\")\n",
    "                print()\n",
    "            else:\n",
    "                is_next_button = False\n",
    "                print(F\"Last response for {region} received.  Process completed.\")\n",
    "                print()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Store all search pages for math tutor\n",
    "    response_dict[region] = region_response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3d3e52-075f-4d8e-8494-41ef4f040317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through each region to get a list of all individual postings for math tutoring in the results pages we searched up earlier.\n",
    "posts_dict = {}\n",
    "for region, responses in response_dict.items():\n",
    "    #current_region = region\n",
    "    region_posts = []\n",
    "    for response in responses:\n",
    "        current_html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        current_posts = current_html_soup.find_all('li', class_='result-row')\n",
    "        region_posts.extend(current_posts)\n",
    "    posts_dict[region] = region_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548788bc-0604-4f90-a8b6-d854ba5b4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many posts in total are to be scraped\n",
    "num_posts = 0\n",
    "num_regions = len(posts_dict)\n",
    "for region, posts in posts_dict.items():\n",
    "    num_posts += len(posts_dict[region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "empirical-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time is 23:58:14\n",
      "Process will finish by 04:16:34\n",
      "\n",
      "Post number 10 in sf_bay_area is being extracted.\n",
      "Post number 20 in sf_bay_area is being extracted.\n",
      "Post number 30 in sf_bay_area is being extracted.\n",
      "Post number 40 in sf_bay_area is being extracted.\n",
      "Post number 50 in sf_bay_area is being extracted.\n",
      "Post number 60 in sf_bay_area is being extracted.\n",
      "Post number 70 in sf_bay_area is being extracted.\n",
      "Post number 80 in sf_bay_area is being extracted.\n",
      "Post number 90 in sf_bay_area is being extracted.\n",
      "Post number 100 in sf_bay_area is being extracted.\n",
      "Post number 110 in sf_bay_area is being extracted.\n",
      "Post number 120 in sf_bay_area is being extracted.\n",
      "Post number 130 in sf_bay_area is being extracted.\n",
      "Post number 140 in sf_bay_area is being extracted.\n",
      "Post number 150 in sf_bay_area is being extracted.\n",
      "Post number 160 in sf_bay_area is being extracted.\n",
      "Post number 170 in sf_bay_area is being extracted.\n",
      "Post number 180 in sf_bay_area is being extracted.\n",
      "Post number 190 in sf_bay_area is being extracted.\n",
      "Post number 200 in sf_bay_area is being extracted.\n",
      "Post number 210 in sf_bay_area is being extracted.\n",
      "Post number 220 in sf_bay_area is being extracted.\n",
      "Post number 230 in sf_bay_area is being extracted.\n",
      "Post number 240 in sf_bay_area is being extracted.\n",
      "Post number 250 in sf_bay_area is being extracted.\n",
      "Post number 260 in sf_bay_area is being extracted.\n",
      "Post number 270 in sf_bay_area is being extracted.\n",
      "Post number 280 in sf_bay_area is being extracted.\n",
      "\n",
      "Soup objects for sf_bay_area acquired.  Waiting for next region...\n",
      "Process will now finish by 04:04:53\n",
      "\n",
      "Post number 10 in new_york is being extracted.\n",
      "Post number 20 in new_york is being extracted.\n",
      "Post number 30 in new_york is being extracted.\n",
      "Post number 40 in new_york is being extracted.\n",
      "Post number 50 in new_york is being extracted.\n",
      "Post number 60 in new_york is being extracted.\n",
      "Post number 70 in new_york is being extracted.\n",
      "Post number 80 in new_york is being extracted.\n",
      "Post number 90 in new_york is being extracted.\n",
      "Post number 100 in new_york is being extracted.\n",
      "Post number 110 in new_york is being extracted.\n",
      "Post number 120 in new_york is being extracted.\n",
      "Post number 130 in new_york is being extracted.\n",
      "Post number 140 in new_york is being extracted.\n",
      "Post number 150 in new_york is being extracted.\n",
      "Post number 160 in new_york is being extracted.\n",
      "Post number 170 in new_york is being extracted.\n",
      "Post number 180 in new_york is being extracted.\n",
      "Post number 190 in new_york is being extracted.\n",
      "Post number 200 in new_york is being extracted.\n",
      "Post number 210 in new_york is being extracted.\n",
      "Post number 220 in new_york is being extracted.\n",
      "Post number 230 in new_york is being extracted.\n",
      "Post number 240 in new_york is being extracted.\n",
      "Post number 250 in new_york is being extracted.\n",
      "Post number 260 in new_york is being extracted.\n",
      "Post number 270 in new_york is being extracted.\n",
      "Post number 280 in new_york is being extracted.\n",
      "Post number 290 in new_york is being extracted.\n",
      "Post number 300 in new_york is being extracted.\n",
      "\n",
      "Soup objects for new_york acquired.  Waiting for next region...\n",
      "Process will now finish by 03:52:34\n",
      "\n",
      "Post number 10 in los_angeles is being extracted.\n",
      "Post number 20 in los_angeles is being extracted.\n",
      "Post number 30 in los_angeles is being extracted.\n",
      "Post number 40 in los_angeles is being extracted.\n",
      "Post number 50 in los_angeles is being extracted.\n",
      "Post number 60 in los_angeles is being extracted.\n",
      "Post number 70 in los_angeles is being extracted.\n",
      "Post number 80 in los_angeles is being extracted.\n",
      "Post number 90 in los_angeles is being extracted.\n",
      "Post number 100 in los_angeles is being extracted.\n",
      "Post number 110 in los_angeles is being extracted.\n",
      "Post number 120 in los_angeles is being extracted.\n",
      "Post number 130 in los_angeles is being extracted.\n",
      "Post number 140 in los_angeles is being extracted.\n",
      "Post number 150 in los_angeles is being extracted.\n",
      "Post number 160 in los_angeles is being extracted.\n",
      "Post number 170 in los_angeles is being extracted.\n",
      "Post number 180 in los_angeles is being extracted.\n",
      "Post number 190 in los_angeles is being extracted.\n",
      "Post number 200 in los_angeles is being extracted.\n",
      "\n",
      "Soup objects for los_angeles acquired.  Waiting for next region...\n",
      "Process will now finish by 03:44:32\n",
      "\n",
      "Post number 10 in sacramento is being extracted.\n",
      "Post number 20 in sacramento is being extracted.\n",
      "Post number 30 in sacramento is being extracted.\n",
      "Post number 40 in sacramento is being extracted.\n",
      "Post number 50 in sacramento is being extracted.\n",
      "Post number 60 in sacramento is being extracted.\n",
      "Post number 70 in sacramento is being extracted.\n",
      "Post number 80 in sacramento is being extracted.\n",
      "Post number 90 in sacramento is being extracted.\n",
      "Post number 100 in sacramento is being extracted.\n",
      "Post number 110 in sacramento is being extracted.\n",
      "\n",
      "Soup objects for sacramento acquired.  Waiting for next region...\n",
      "Process will now finish by 03:39:00\n",
      "\n",
      "Post number 10 in chicago is being extracted.\n",
      "Post number 20 in chicago is being extracted.\n",
      "Post number 30 in chicago is being extracted.\n",
      "Post number 40 in chicago is being extracted.\n",
      "Post number 50 in chicago is being extracted.\n",
      "Post number 60 in chicago is being extracted.\n",
      "\n",
      "Soup objects for chicago acquired.  Waiting for next region...\n",
      "Process will now finish by 03:36:23\n",
      "\n",
      "Post number 10 in san_diego is being extracted.\n",
      "Post number 20 in san_diego is being extracted.\n",
      "Post number 30 in san_diego is being extracted.\n",
      "Post number 40 in san_diego is being extracted.\n",
      "Post number 50 in san_diego is being extracted.\n",
      "Post number 60 in san_diego is being extracted.\n",
      "Post number 70 in san_diego is being extracted.\n",
      "Post number 80 in san_diego is being extracted.\n",
      "Post number 90 in san_diego is being extracted.\n",
      "Post number 100 in san_diego is being extracted.\n",
      "Post number 110 in san_diego is being extracted.\n",
      "\n",
      "Soup objects for san_diego acquired.  Waiting for next region...\n",
      "Process will now finish by 03:31:43\n",
      "\n",
      "Post number 10 in houston is being extracted.\n",
      "Post number 20 in houston is being extracted.\n",
      "Post number 30 in houston is being extracted.\n",
      "Post number 40 in houston is being extracted.\n",
      "Post number 50 in houston is being extracted.\n",
      "Post number 60 in houston is being extracted.\n",
      "Post number 70 in houston is being extracted.\n",
      "Post number 80 in houston is being extracted.\n",
      "\n",
      "Soup objects for houston acquired.  Waiting for next region...\n",
      "Process will now finish by 03:28:05\n",
      "\n",
      "Post number 10 in phoenix is being extracted.\n",
      "Post number 20 in phoenix is being extracted.\n",
      "Post number 30 in phoenix is being extracted.\n",
      "Post number 40 in phoenix is being extracted.\n",
      "Post number 50 in phoenix is being extracted.\n",
      "\n",
      "Soup objects for phoenix acquired.  Waiting for next region...\n",
      "Process will now finish by 03:25:13\n",
      "\n",
      "Post number 10 in philadelphia is being extracted.\n",
      "Post number 20 in philadelphia is being extracted.\n",
      "Post number 30 in philadelphia is being extracted.\n",
      "Post number 40 in philadelphia is being extracted.\n",
      "Post number 50 in philadelphia is being extracted.\n",
      "Post number 60 in philadelphia is being extracted.\n",
      "Post number 70 in philadelphia is being extracted.\n",
      "Post number 80 in philadelphia is being extracted.\n",
      "Post number 90 in philadelphia is being extracted.\n",
      "Post number 100 in philadelphia is being extracted.\n",
      "Post number 110 in philadelphia is being extracted.\n",
      "\n",
      "Soup objects for philadelphia acquired.  Waiting for next region...\n",
      "Process will now finish by 03:20:20\n",
      "\n",
      "Post number 10 in dallas is being extracted.\n",
      "Post number 20 in dallas is being extracted.\n",
      "Post number 30 in dallas is being extracted.\n",
      "Post number 40 in dallas is being extracted.\n",
      "Post number 50 in dallas is being extracted.\n",
      "Post number 60 in dallas is being extracted.\n",
      "Post number 70 in dallas is being extracted.\n",
      "Post number 80 in dallas is being extracted.\n",
      "\n",
      "Soup objects for dallas acquired.  Waiting for next region...\n",
      "Process will now finish by 03:16:46\n",
      "\n",
      "Post number 10 in san_antonio is being extracted.\n",
      "Post number 20 in san_antonio is being extracted.\n",
      "Post number 30 in san_antonio is being extracted.\n",
      "Post number 40 in san_antonio is being extracted.\n",
      "Post number 50 in san_antonio is being extracted.\n",
      "Post number 60 in san_antonio is being extracted.\n",
      "Post number 70 in san_antonio is being extracted.\n",
      "Post number 80 in san_antonio is being extracted.\n",
      "Post number 90 in san_antonio is being extracted.\n",
      "\n",
      "Soup objects for san_antonio acquired.  Process complete.\n"
     ]
    }
   ],
   "source": [
    "soup_objects_dict = {}\n",
    "\n",
    "num_posts_remaining = num_posts\n",
    "current_time = dt.datetime.now()\n",
    "max_seconds_until_finish = num_posts * 10\n",
    "max_finish_time = current_time + dt.timedelta(seconds=max_seconds_until_finish)\n",
    "\n",
    "print(F\"Current time is {current_time.strftime('%H:%M:%S')}\")\n",
    "print(F\"Process will finish by {max_finish_time.strftime('%H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "for count, region in enumerate(posts_dict, start=1):\n",
    "    # Walk through each region and create a list of soup_objects to scrape from by storing them into memory.  This way we only have to send these get requests once and Craigslist doesn't ban us for sending the same https requests over and over\n",
    "    soup_objects_list = []\n",
    "    for i, post in enumerate(posts_dict[region]):\n",
    "        # Impose a timer so that we send each get request between 5 and 10 seconds.  This is again to help prevent from getting banned for too many HTTP requests.\n",
    "        random_int = random.randint(5,10)\n",
    "        time.sleep(random_int)\n",
    "        current_link = post.a.get('href')\n",
    "        response_object = session.get(current_link)\n",
    "        soup_object = BeautifulSoup(response_object.text, 'html.parser')\n",
    "        soup_objects_list.append(soup_object) \n",
    "        # Impose condition that every 10th post will trigger something printed to the screen.  This part of the code is a long process and I wanted something to help keep track of how much progress has been made\n",
    "        if (i !=0) and ((i-1) % 10 == 9):\n",
    "            print(F\"Post number {i} in {region} is being extracted.\")\n",
    "    \n",
    "    soup_objects_dict[region] = soup_objects_list\n",
    "    if count != len(posts_dict):\n",
    "        num_posts_remaining -= len(posts_dict[region])\n",
    "        current_time = dt.datetime.now()\n",
    "        new_seconds_until_finish = num_posts_remaining * 10\n",
    "        new_max_finish_time = current_time + dt.timedelta(seconds=new_seconds_until_finish)\n",
    "        \n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Waiting for next region...\")\n",
    "        print(F\"Process will now finish by {new_max_finish_time.strftime('%H:%M:%S')}\")\n",
    "        print()\n",
    "    else:\n",
    "        print()\n",
    "        print(F\"Soup objects for {region} acquired.  Process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-interaction",
   "metadata": {},
   "source": [
    "## Pre-processing Craigslist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec9ba83-0ce9-4b7d-82fd-eccf2b3ff58f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n",
      "Couldn't get link\n",
      "Couldn't get text\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "error_list_text = []\n",
    "error_list_links = []\n",
    "\n",
    "# Walk through each region that contains a list of soup objects corresponding to the search of services for math tutors.\n",
    "for search_region in soup_objects_dict:\n",
    "    # Initialize several lists to store relevant information for analysis\n",
    "    price_list = []\n",
    "    city_list = []\n",
    "    datetime_list = []\n",
    "    body_text_list = []\n",
    "    subregion_list = []\n",
    "    region_list = []\n",
    "    link_list = []\n",
    "    search_region_price_list = []\n",
    "    \n",
    "    # Walk through each soup object in the list corresponding to the search region and get the link of the soup object to scrape from.\n",
    "    for soup in soup_objects_dict[search_region]:\n",
    "        try:\n",
    "            link = soup.find(\"meta\", property=\"og:url\")['content']\n",
    "        except:\n",
    "            # In case a link can't be found, we add the soup object to a list to inspect later and set link to 'None', which we'll use as a filter later so Python doesn't try to scrape from them\n",
    "            link = 'None'\n",
    "            error_list_links.append(soup)\n",
    "            print(\"Couldn't get link\")\n",
    "\n",
    "        # Extract region of post from Craigslist\n",
    "        post_region = soup.find_all('li',class_='crumb area')[0].find('a').get_text()\n",
    "        post_region = post_region.replace(' ', '_')\n",
    "        post_region = post_region.lower()\n",
    "        \n",
    "        # Get text of postingbody of the post and remove unwanted text.\n",
    "        try:\n",
    "            text = soup.find('section', id='postingbody').get_text()\n",
    "            text = text.replace(u'\\xa0', u' ')\n",
    "            # We do this so that we can use ; as a delimiter when copying data from a CSV file into a SQL database later.\n",
    "            text = text.replace(';', ',') \n",
    "            # We do this because one post in particular had this text and was giving me trouble.  The best way I could find to handle it was to remove the text.\n",
    "            text = text.replace('QR Code Link to This Post', '') \n",
    "\n",
    "        except:\n",
    "            error_list_text.append(soup)\n",
    "            text = 'None'\n",
    "            #body_text_list.append(text)\n",
    "            print(\"Couldn't get text\")\n",
    "\n",
    "        # Only let posts through that have a link to scrape from and those posts where the region of the post matches the region of the search.  Some CL search results are for neighboring areas, ones that come up in a different region than the region your search was from, which leads to duplicates in nearby areas like Los Angeles and San Diego.  This will weed out duplicates.\n",
    "        if post_region == search_region and link!= 'None':\n",
    "            region_list.append(post_region)\n",
    "            link_list.append(link)\n",
    "            body_text_list.append(text)\n",
    "\n",
    "            # Use regular expressions to find all instances of prices in the text\n",
    "            #old_prices = re.findall('(?:[\\$]{1}[,\\d]+.?\\d*)', text)\n",
    "            old_prices = re.findall('(?:[\\$]{1}[,\\d]+\\d*)', text)\n",
    "            # Alternative, if trying to capture decimals \n",
    "            # ^(?:\\${1}\\d+(?:,\\d{3})*(?:\\.{1}\\d{2}){0,1})?$\n",
    "\n",
    "\n",
    "\n",
    "            # Intialize empty list to store the new prices after processing old prices.\n",
    "            new_prices = []\n",
    "            #print(F\"Initialized new_prices: {new_prices}\")\n",
    "            # Walk through each price in the post.\n",
    "            for price in old_prices:\n",
    "                # Clean unwanted characters.\n",
    "                price = price.replace('$', '')\n",
    "                price = price.replace('/', '')\n",
    "                price = price.replace('!', '')\n",
    "                price = price.replace('h', '')\n",
    "                price = price.replace('.', '')\n",
    "                price = price.replace(')', '')\n",
    "                price = price.replace(',', '')\n",
    "                price = price.replace('>', '')\n",
    "                price = price.rstrip()   \n",
    "                # Some tutors give prices as a range ie '$30-40'.  In order to work with this data, I split based on the hyphen, then I can use each price individually.\n",
    "                split_prices = price.split('-')\n",
    "            #print(F\"Here are the old_prices: {old_prices}\")\n",
    "            #print(F\"Here are the split_prices: {split_prices}\")\n",
    "\n",
    "                # Walk through each price in the posting, after any necessary splits have been made.\n",
    "                for p in split_prices:\n",
    "                    # Only proceed if the post contained prices, ie if p is a non-empty string.\n",
    "                    if len(p)!=0:\n",
    "                        try:\n",
    "                            # Convert string price to int.\n",
    "                            new_int = int(p)\n",
    "                            # Ignore prices which are too high to be reasonable.  Some posts included scholarship amounts as ways for a tutor to boast about their abilities, but this will only allow dollar amounts that are reasonable through.\n",
    "                            if new_int <= 200:\n",
    "                                new_prices.append(new_int)\n",
    "\n",
    "                        except:\n",
    "                            # Show which prices aren't able to convert to an int and the post they came from so we can isolate and fix the issue if need be.\n",
    "                            print(F'Error converting this price: {p}')\n",
    "                            print(split_prices)\n",
    "                            print()\n",
    "                            print('Here is the text of the post:')\n",
    "                            print()\n",
    "                            print(text)\n",
    "                            print('-'*50)\n",
    "                            print()\n",
    "                            # Set prices that can't be covered to NaN so the process can finish.\n",
    "                            new_prices.append(np.nan) \n",
    "            #print(F\"Here are the processed new_prices: {new_prices}\")\n",
    "                    #print(len(new_prices))\n",
    "\n",
    "\n",
    "            # Append all prices from the post to a separate list, in case we need to isolate issues and fix them later.\n",
    "\n",
    "            search_region_price_list.append(new_prices)\n",
    "\n",
    "            # For posts that had no prices listed, we use null\n",
    "            if len(new_prices)==0:\n",
    "                price_list.append(np.nan)\n",
    "            # For posts that had a single price, we use it.\n",
    "            elif len(new_prices)==1:\n",
    "                price_list.append(new_prices[0])\n",
    "            # For posts that contained two prices, we average them.  This helps with posts that give a range of prices (ie $25-30).\n",
    "            elif len(new_prices)==2:\n",
    "                avg_price_2 = np.average(new_prices)\n",
    "                price_list.append(avg_price_2)\n",
    "            # If a post has more than 3 prices, we append null.  We'll have to inspect these posts manually and deal with them later.\n",
    "            else:\n",
    "                price_list.append(np.nan)\n",
    "            #print(price_list)\n",
    "\n",
    "\n",
    "            # Get city information for each posting.\n",
    "            try:\n",
    "                city = soup.find(class_='postingtitletext').small.get_text()\n",
    "\n",
    "                # Because of the way CL operates, one has to choose a city from a radio button list, that CL provides, when one creates a post to offer a service, however later, there's a field where they can type in any city they want.  Many people will randomly choose a city from the radio button list, but then  post their city as \"online\".  This makes sure we capture them. \n",
    "                re_pattern = re.compile('online')\n",
    "                online_flag = re.search(re_pattern, city.lower())\n",
    "                if online_flag:\n",
    "                    city_list.append('Online')\n",
    "                else:\n",
    "                    # Strip out leading and trailing white spaces, replace parentheses, and capitalize each word in the str.\n",
    "                    city = city.strip()\n",
    "                    city = city.replace('(', '').replace(')', '')        \n",
    "                    city = city.title()\n",
    "                    city_list.append(city)\n",
    "            except:\n",
    "                # If a post has no city information, use None\n",
    "                city_list.append('None')\n",
    "\n",
    "            # Extract subregion of Craigslist that the post was made in. This will allow for comparison of prices across different cities within the same metropolitan sub_region.\n",
    "            try:\n",
    "                subregion = soup.find_all('li', class_='crumb subarea')[0].find('a').get_text()\n",
    "                subregion = subregion.title()\n",
    "                subregion_list.append(subregion)\n",
    "            except:\n",
    "                subregion_list.append('None')\n",
    "\n",
    "\n",
    "            # Extract time the posting was made.\n",
    "            try:\n",
    "                dt_object = soup.find('time')['datetime']\n",
    "                datetime_list.append(dt_object)\n",
    "            except:\n",
    "                datetime_list.append('None')\n",
    "        else:\n",
    "            pass\n",
    "    #print(price_list)\n",
    "    # Create temporary df to store results for each region\n",
    "    temp_df = pd.DataFrame(data=zip(datetime_list,\n",
    "                                    link_list, \n",
    "                                    price_list, \n",
    "                                    city_list, \n",
    "                                    subregion_list, \n",
    "                                    region_list, \n",
    "                                    body_text_list,\n",
    "                                    search_region_price_list),\n",
    "                        columns=['date_posted', \n",
    "                                 'link', \n",
    "                                 'price', \n",
    "                                 'city', \n",
    "                                 'subregion', \n",
    "                                 'region', \n",
    "                                 'post_text',\n",
    "                                 'price_list']\n",
    "                          )\n",
    "\n",
    "# # Find indices of duplicate results, then drop them and reset indices.\n",
    "# temp_duplicate_indices = temp_df[temp_df['post_text'].duplicated()==True].index\n",
    "# temp_df_no_dups = temp_df.drop(index=temp_duplicate_indices)\n",
    "# temp_df_no_dups = temp_df_no_dups.reset_index(drop=True)\n",
    "# temp_df_no_dups['len_of_price_list']=temp_df_no_dups['price_list'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "# temp_text_for_comparison = temp_df_no_dups['post_text']\n",
    "# vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "# temp_tfidf = vect.fit_transform(temp_text_for_comparison)\n",
    "# temp_pairwise_similarity = temp_tfidf * temp_tfidf.T\n",
    "# temp_pairwise_array = temp_pairwise_similarity.toarray()\n",
    "# np.fill_diagonal(temp_pairwise_array, np.nan)\n",
    "# temp_argwhere_array = np.argwhere(temp_pairwise_array > 0.9)\n",
    "\n",
    "\n",
    "# df_row_idx = []\n",
    "# dup_row_idx = []\n",
    "# for row in temp_argwhere_array:\n",
    "#     current_idx = row[0]\n",
    "#     #print(F\"Current row: {row}, Current idx: {current_idx}\")\n",
    "#     duplicate_list = []\n",
    "#     if current_idx in df_row_idx:\n",
    "#         continue\n",
    "#     else:\n",
    "#         df_row_idx.append(current_idx)\n",
    "#     for other_row in temp_argwhere_array:\n",
    "#         other_idx = other_row[1]\n",
    "#         #print(F\"Here's the other_row: {other_row}, Other idx: {other_idx}\")\n",
    "#         if current_idx == other_row[0]:\n",
    "#             duplicate_list.append(other_idx)\n",
    "#     #print(F\"This is the current dup_list: {duplicate_list}\")\n",
    "#     #print()\n",
    "#     dup_row_idx.append(duplicate_list)\n",
    "\n",
    "\n",
    "# temp_df_no_dups['match'] = np.array(temp_df_no_dups.index.values, dtype='object')\n",
    "# # temp_df_no_dups['match'] = temp_df_no_dups['match'].apply(lambda x: [x])\n",
    "\n",
    "\n",
    "# match_col_idx = temp_df_no_dups.columns.get_loc('match')\n",
    "# temp_df_no_dups.iloc[df_row_idx, match_col_idx] = dup_row_idx\n",
    "# temp_df_no_dups['match'] = temp_df_no_dups['match'].apply(lambda x: [x])\n",
    "\n",
    "# indices = []\n",
    "\n",
    "# # for i, row in temp_df_no_dups.iterrows():\n",
    "# #     indices.append(i)\n",
    "# #     temp_df_no_dups = temp_df_no_dups.drop(\n",
    "# #         index=[item for item in row[\"match\"] if item not in indices], errors=\"ignore\"\n",
    "# #     )\n",
    "\n",
    "# # if search_region=='phoenix':\n",
    "# #     print()\n",
    "# #     print(F'search region: {search_region}')\n",
    "# #     for i, row in temp_df_no_dups.iterrows():\n",
    "# #         indices.append(i)\n",
    "# #         drop_idx = []\n",
    "# #         print(i, row['match'])\n",
    "# #         try:\n",
    "# #             for item in row['match']:\n",
    "# #                 if item not in indices:\n",
    "# #                     drop_idx.append(item)\n",
    "# #             temp_df_no_dups = temp_df_no_dups.drop(index=drop_idx, errors=\"ignore\")\n",
    "# #         except Exception as e:\n",
    "# #             #print(i, item, row['match'])\n",
    "# #             print(e, i, item, row['match'])\n",
    "\n",
    "# print()\n",
    "# print(F'search region: {search_region} starting')\n",
    "# for i, row in temp_df_no_dups.iterrows():\n",
    "#     indices.append(i)\n",
    "#     drop_idx = []\n",
    "#     #print(i, row['match'])\n",
    "#     try:\n",
    "#         for item in row['match']:\n",
    "#             if item not in indices:\n",
    "#                 drop_idx.append(item)\n",
    "#         temp_df_no_dups = temp_df_no_dups.drop(index=drop_idx, errors=\"ignore\")\n",
    "#     except Exception as e:\n",
    "#         #print(i, item, row['match'])\n",
    "#         print(e, i, item, row['match'])\n",
    "# print(F'search region: {search_region} complete')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Append each temporary df to a list, which we can concatenate into one larger df, later.\n",
    "    df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nearby-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for errors in getting text from a post, or from getting the URL of a post.\n",
    "len(error_list_text), len(error_list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "endangered-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1327, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dfs for each region into one larger df and check its shape.\n",
    "concat_df = pd.concat(df_list, ignore_index=True)\n",
    "concat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2e528-e3db-4b1d-b869-afba2b9b709c",
   "metadata": {},
   "source": [
    "### Dropping Duplicate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spanish-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    679\n",
       "True     648\n",
       "Name: post_text, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get date of html request to label our output with.\n",
    "date_of_html_request = str(dt.date.today())\n",
    "\n",
    "# Include the date posts were scraped on to track tutoring prices over time.\n",
    "concat_df['posts_scraped_on'] = date_of_html_request\n",
    "\n",
    "# Count duplicates.\n",
    "concat_df['post_text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52774d13-f298-4586-be24-e5f33cf0e8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find indices of rows that have exactly the same post_text, then drop them and reset indices.\n",
    "duplicate_indices = concat_df[concat_df['post_text'].duplicated()==True].index\n",
    "df_exact_txt_dropped = concat_df.drop(index=duplicate_indices)\n",
    "df_exact_txt_dropped = df_exact_txt_dropped.reset_index(drop=True)\n",
    "df_exact_txt_dropped['len_of_price_list']=df_exact_txt_dropped['price_list'].apply(lambda x: len(x))\n",
    "df_exact_txt_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e30cd031-8c4b-4907-86b5-77467325ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize each posts' text and calculate the cosine similarity of each post against all other posts to determine which are duplicates\n",
    "## https://kanoki.org/2018/12/27/text-matching-cosine-similarity/\n",
    "text_for_comparison = df_exact_txt_dropped['post_text']\n",
    "vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "tfidf = vect.fit_transform(text_for_comparison)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "# Store results in a 2D NumPy array\n",
    "pairwise_array = pairwise_similarity.toarray()\n",
    "\n",
    "# The diagonal of our array is the similarity of a post to itself, which we fill will null so that these are essentially ignored\n",
    "np.fill_diagonal(pairwise_array, np.nan)\n",
    "\n",
    "# Many people on CL will change their posting in ways to avoid CL flagging them as duplicates for removal.  This finds all posts above a certain similarity threshold.\n",
    "argwhere_array = np.argwhere(pairwise_array > 0.63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6737df-7701-4f01-a968-898e60b48476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In order to remove the duplicates, we need to restructure our 2D NumPy array in such a way that the first column is the index of the post that has a duplicate and the second column contains a list of the indices of the duplicate post(s).\n",
    "df_row_idx = []\n",
    "dup_row_idx = []\n",
    "for row in argwhere_array:\n",
    "    current_idx = row[0]\n",
    "    #print(F\"Current row: {row}, Current idx: {current_idx}\")\n",
    "    duplicate_list = []\n",
    "    if current_idx in df_row_idx:\n",
    "        continue\n",
    "    else:\n",
    "        df_row_idx.append(current_idx)\n",
    "    for other_row in argwhere_array:\n",
    "        other_idx = other_row[1]\n",
    "        #print(F\"Here's the other_row: {other_row}, Other idx: {other_idx}\")\n",
    "        if current_idx == other_row[0]:\n",
    "            duplicate_list.append(other_idx)\n",
    "    #print(F\"This is the current dup_list: {duplicate_list}\")\n",
    "    #print()\n",
    "    dup_row_idx.append(duplicate_list)\n",
    "#list(zip(df_row_idx, dup_row_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48949840-611f-496c-b1a3-adf81253b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rancher/opt/anaconda3/envs/ox/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n",
      "/Users/rancher/opt/anaconda3/envs/ox/lib/python3.9/site-packages/pandas/core/internals/blocks.py:937: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                               [91]\n",
       "1                    [264, 432, 657]\n",
       "2           [42, 152, 231, 463, 479]\n",
       "3                         [212, 355]\n",
       "4      [41, 213, 237, 372, 605, 610]\n",
       "                   ...              \n",
       "674                            [674]\n",
       "675                            [675]\n",
       "676                            [677]\n",
       "677                            [676]\n",
       "678                            [678]\n",
       "Name: match, Length: 679, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create match column in our df, initialize it so that each row contains the index of that row and convert to a list, so we can iterate over it\n",
    "df_exact_txt_dropped['match'] = np.array(df_exact_txt_dropped.index.values, dtype='object')\n",
    "df_exact_txt_dropped['match'] = df_exact_txt_dropped['match'].apply(lambda x: [x])\n",
    "\n",
    "# For rows that are duplicate postings, we overwrite the match column with the indices of all other rows that have duplicated text\n",
    "match_col_idx = df_exact_txt_dropped.columns.get_loc('match')\n",
    "df_exact_txt_dropped.iloc[df_row_idx, match_col_idx] = dup_row_idx\n",
    "#df_exact_txt_dropped['match'] = df_exact_txt_dropped['match'].apply(lambda x: [x])\n",
    "\n",
    "df_exact_txt_dropped['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ae76bd3-20ac-443b-b254-ad184673580b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "\n",
    "# for i, row in temp_df_exact_txt_dropped.iterrows():\n",
    "#     indices.append(i)\n",
    "#     temp_df_exact_txt_dropped = temp_df_exact_txt_dropped.drop(\n",
    "#         index=[item for item in row[\"match\"] if item not in indices], errors=\"ignore\"\n",
    "#     )\n",
    "\n",
    "# if search_region=='phoenix':\n",
    "#     print()\n",
    "#     print(F'search region: {search_region}')\n",
    "#     for i, row in temp_df_exact_txt_dropped.iterrows():\n",
    "#         indices.append(i)\n",
    "#         drop_idx = []\n",
    "#         print(i, row['match'])\n",
    "#         try:\n",
    "#             for item in row['match']:\n",
    "#                 if item not in indices:\n",
    "#                     drop_idx.append(item)\n",
    "#             temp_df_exact_txt_dropped = temp_df_exact_txt_dropped.drop(index=drop_idx, errors=\"ignore\")\n",
    "#         except Exception as e:\n",
    "#             #print(i, item, row['match'])\n",
    "#             print(e, i, item, row['match'])\n",
    "\n",
    "\n",
    "df_no_dups = df_exact_txt_dropped.copy()\n",
    "\n",
    "# Iterate over each row and remove all rows that have duplicated text\n",
    "for i, row in df_no_dups.iterrows():\n",
    "    indices.append(i)\n",
    "    drop_idx = []\n",
    "    #print(i, row['match'])\n",
    "    try:\n",
    "        for item in row['match']:\n",
    "            if item not in indices:\n",
    "                drop_idx.append(item)\n",
    "        df_no_dups = df_no_dups.drop(index=drop_idx, errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        #print(i, item, row['match'])\n",
    "        print(e, i, item, row['match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24427708-b9e4-4b53-b0f2-0f2ec6962382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((679, 11), (351, 11))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape when we dropped posts with exactly the same post_text against the shape after we dropped text deemed similar by cosine similarity \n",
    "df_exact_txt_dropped.shape, df_no_dups.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-compound",
   "metadata": {},
   "source": [
    "### Dropping posts that contained no prices, which aren't helpful for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434e8abf-7b66-4232-9059-93272966788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the len of price_list to find posts that contained no prices\n",
    "df_no_dups['len_of_price_list'] = df_no_dups['price_list'].apply(lambda x: len(x))\n",
    "\n",
    "# Filter out results that don't have a price and reset indices.\n",
    "df_with_prices = df_no_dups[df_no_dups['len_of_price_list'] > 0]\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "273eca8b-0205-4458-9536-0cebd167096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "residential-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1327 posts, there were 351 that were unique, or 26.45%.\n",
      "Out of those, there were 137 posts that had prices included.\n",
      "Only 10.32% of the posts that we scraped remain.\n"
     ]
    }
   ],
   "source": [
    "unique_posts_count = len(df_no_dups)\n",
    "post_with_prices_count = len(df_with_prices)\n",
    "num_posts = len(concat_df)\n",
    "\n",
    "percent_unique = unique_posts_count / num_posts * 100\n",
    "percent_with_prices = post_with_prices_count / num_posts * 100\n",
    "\n",
    "print(F\"Out of {num_posts} posts, there were {unique_posts_count} that were unique, or {percent_unique:.2f}%.\")\n",
    "print(F\"Out of those, there were {post_with_prices_count} posts that had prices included.\")\n",
    "\n",
    "print(F\"Only {percent_with_prices:.2f}% of the posts that we scraped remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-start",
   "metadata": {},
   "source": [
    "### Extracting complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-measurement",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *Transforming* Craigslist data: Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-andrew",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Are there any posts that might need manual cleaning?  This would include:\n",
    "* Posts that had 3 or more prices and were marked as null\n",
    "* Posts where the price wasn't able to convert from `str` -> `int` and were marked as null during pre-processing\n",
    "\n",
    "There are the entries that were marked as `Null`.  Let's investigate them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5118dd4a-b106-4d3c-830b-42db498f7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[90, 60, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[40, 40, 45, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 35, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 30, 60, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[60, 50, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[100, 115, 130, 65, 30, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[50, 10, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[40, 40, 40, 40, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[50, 100, 135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[40, 80, 40, 10, 40, 30, 40, 80, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[25, 45, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[35, 35, 40, 40, 55, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[65, 55, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[20, 25, 30, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[100, 115, 130, 65, 30, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[50, 50, 35, 30, 25, 55, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[25, 30, 50, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[60, 50, 75]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[20, 25, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[30, 40, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[20, 30, 20, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[25, 50, 25]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price                             price_list\n",
       "1      NaN                           [90, 60, 40]\n",
       "4      NaN                       [40, 40, 45, 45]\n",
       "10     NaN                           [30, 35, 45]\n",
       "17     NaN                       [30, 30, 60, 90]\n",
       "20     NaN                          [60, 50, 100]\n",
       "29     NaN            [100, 115, 130, 65, 30, 60]\n",
       "33     NaN                           [50, 10, 50]\n",
       "34     NaN                   [40, 40, 40, 40, 40]\n",
       "35     NaN                         [50, 100, 135]\n",
       "36     NaN                           [30, 50, 60]\n",
       "38     NaN  [40, 80, 40, 10, 40, 30, 40, 80, 100]\n",
       "43     NaN                           [25, 45, 25]\n",
       "44     NaN               [35, 35, 40, 40, 55, 80]\n",
       "52     NaN                           [65, 55, 55]\n",
       "54     NaN                       [20, 25, 30, 30]\n",
       "62     NaN            [100, 115, 130, 65, 30, 60]\n",
       "67     NaN           [50, 50, 35, 30, 25, 55, 30]\n",
       "93     NaN                       [25, 30, 50, 50]\n",
       "95     NaN                           [60, 50, 75]\n",
       "97     NaN                           [20, 25, 30]\n",
       "118    NaN                           [30, 40, 50]\n",
       "125    NaN                       [20, 30, 20, 30]\n",
       "132    NaN                           [25, 50, 25]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null_prices = df_with_prices[df_with_prices['price'].isnull()==True]\n",
    "df_null_prices[['price', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbfb0dec-46fb-4693-b660-10214d8f6b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 23 posts with price marked null.\n"
     ]
    }
   ],
   "source": [
    "posts_with_mult_prices = df_null_prices.shape[0]\n",
    "print(F\"There were {posts_with_mult_prices} posts with price marked null.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6106d30-41ee-47f0-9695-22b95ad7e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store posts with null prices to CSV to manually inspect later\n",
    "df_null_prices = df_null_prices.drop(columns=['len_of_price_list', 'match'])\n",
    "df_null_prices.to_csv('./posts_to_investigate/{}_posts_with_null_prices.csv'.format(date_of_html_request), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "930b77ec-483e-4b34-803b-3032be8e5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/eby/lss/d/danville-harvard-math-tutor-text-aor/7430164887.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "35.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect links manually, one by one, to decide what to do about price information\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=3\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-integration",
   "metadata": {},
   "source": [
    "### Cleaning posts with three or more prices manually - distilling down to one price\n",
    "\n",
    "We distill posts that had more complicated text that involved three or more prices, such as :\n",
    "\n",
    "* $40$/hr, $50$/1.5hr, $60$/2hr\n",
    "  * Complicated pricing schedule\n",
    "* $40$/hr but $10$ additional per person, if a group session is desired\n",
    "  * Group rates\n",
    "* $30$/hr Science, $40$/hr math, come and try a first session for the reduced price of $20$.\n",
    "  * Special offers\n",
    "\n",
    "into a single price.  Other posts repeated their prices multiple times, so we distill those down to a single price as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "raised-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_col_idx = df_with_prices.columns.get_loc('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5484075c-56c5-4c45-a037-4a0d7320078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $40 for in person, or $45 for at home, so I took the average.\n",
    "san_mateo_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('I mainly tutor, in person, at the Downtown Redwood City, downtown San Mateo')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[san_mateo_tutor_idx,price_col_idx] = 42.5\n",
    "\n",
    "except:\n",
    "    print(\"Issue with san_mateo_tutor and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "musical-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the ad says $90 in person, $60 for online, and Corona Virus pricing of\n",
    "# $40 for online weekdays, I'm using the $40 per hour rate because it seems the\n",
    "# most reasonable and is most similar to what I'm competing against.\n",
    "kenari_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('kenaritutor.com')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[kenari_tutor_idx,price_col_idx] = 40\n",
    "except:\n",
    "    print('Issue with kenari_tutor_idx and iloc.')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "511ec79c-4a71-4cbf-9efd-466b286386b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad mentions several prices for different subjects, but explicitly says $30 for math.\n",
    "la_honda_idx = df_with_prices[df_with_prices['post_text'].str.contains('909-640-3570')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[la_honda_idx,price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with la_honda_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14ca5d7e-f730-4fc3-841a-c70205c847a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says #60 per hour.\n",
    "glasses_lady_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"offering virtual one-on-one Math tutoring via Zoom\")==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[glasses_lady_idx, price_col_idx] = 60\n",
    "except:\n",
    "    print(\"Issue with glasses_lady_idx and iloc.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1402316-6126-4b99-ab97-5ed93f0238bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says #60 per hour.\n",
    "UC_Davis_data_scientist = df_with_prices[df_with_prices['post_text'].str.contains(\"PhD in Engineering from UC Davis\")==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[UC_Davis_data_scientist, price_col_idx] = 60\n",
    "except:\n",
    "    print(\"Issue with UC_Davis_data_scientist and iloc.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "286df5e3-4fcb-481e-b8b1-238ecc789fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This guy has weird price structuring, but I used his hourly rate for each time interval, $100 for 80 minutes, $115 for 100 minutes, $130 for 120 minutes, then averaged those hourly rates to estimate what a single hour would cost.\n",
    "oakland_exp_tutor_online_idx = df_with_prices[df_with_prices['post_text'].str.contains('I received a full scholarship to University of Cincinnati and held a 3.8 GPA through my master’s program in aerospace')==True].index\n",
    "\n",
    "oakland_tutor_avg_rate = ((100/80) + (115/100) + (130/120)) * 60 / 3\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[oakland_exp_tutor_online_idx, price_col_idx] = oakland_tutor_avg_rate\n",
    "\n",
    "except:\n",
    "    print(\"Issue with oakland_exp_tutor_online_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "competitive-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ad repeats the price of $40 over and over, so I'm replacing the price with \n",
    "# a single instance.\n",
    "star_star_college_math_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('https://www.youtube.com/channel/UCqhFZRmUqOAAPMQpo58TV7g'\n",
    "                   ) == True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[star_star_college_math_tutor_idx, price_col_idx] = 40\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with star_star_college_math_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75bacbee-fda4-40d8-b864-f086777b76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $50/hr    \n",
    "trevor_skelly_idx = df_with_prices[df_with_prices['post_text'].str.contains('trevorskelly')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[trevor_skelly_idx,price_col_idx] = 50\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with trevor_skelly_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sophisticated-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $50 per hour for sessions under 3 hours\n",
    "spss_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('datameer', case=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[spss_tutor_idx, price_col_idx] = 50\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with spss_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6f8a00e-df48-4e40-9b18-886f989c6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $50 per hour\n",
    "tutor_sam_idx = df_with_prices[df_with_prices['post_text'].str.contains('thetutorsam')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[tutor_sam_idx, price_col_idx] = 50\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with tutor_sam_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "nonprofit-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $40 per hour\n",
    "peter_d_idx = df_with_prices[df_with_prices['post_text'].str.contains('Peter D.')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[peter_d_idx, price_col_idx] = 40\n",
    "except:\n",
    "    print(\"Issue with peter_d_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "289270da-cd1d-4aac-b1cb-a4070238e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges $45 per hour for individual lessons\n",
    "algebra_exclusively_idx = df_with_prices[df_with_prices['post_text'].str.contains('algebra EXCLUSIVELY')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[algebra_exclusively_idx, price_col_idx] = 45\n",
    "except:\n",
    "    print(\"Issue with algebra_exclusively_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "improving-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post includes many prices, but states $55/hr for Precalc and $80/hr for Calculus, which are primarily what I help with, so I took the average of those prices\n",
    "aerospace_engineer_idx = df_with_prices[df_with_prices['post_text'].str.contains('in the aerospace industry looking', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[aerospace_engineer_idx, price_col_idx] = (55 + 80)/2\n",
    "\n",
    "except:\n",
    "    print(\"Issue with aerospace_engineer_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "careful-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad mentions $45 for lower division college courses, which are a large segment of the subjects I help with, so I'm using that price to compare myself against.\n",
    "ucb_phd_student_and_ta_idx = df_with_prices[df_with_prices['post_text'].str.contains('Former UC-Berkeley economics Ph.D. student and TA')].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucb_phd_student_and_ta_idx, price_col_idx] = 45\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucb_phd_student_and_ta_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "identical-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The add says $55/hr for K-12, then $65/hr for AP/Honors, as well as Pre-calc, \n",
    "# etc., I'm going to average the two prices.\n",
    "park_academy_idx = df_with_prices[df_with_prices['post_text'].str.contains('(949) 490-0872', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[park_academy_idx, price_col_idx] = 60\n",
    "\n",
    "except:\n",
    "    print(\"Issue with park_academy_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "explicit-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $25/hr for high school, $30/hr for college, just went with $30/hr\n",
    "sharp_mind_idx = df_with_prices[df_with_prices['post_text'].str.contains('(650) 398-9490', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[sharp_mind_idx, price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with sharp_mind_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4988ca4-9ad0-4703-a94a-ed4065e4523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $50/hr if travelling, $30-35/hr if virtual, so I took the average of 50 and 35\n",
    "stock_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('714.425.3828', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[stock_tutor_idx, price_col_idx] = (35 + 50)/2\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with stock_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "formal-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post says $30/hr for Precalc/Trig and $50/hr for Calculus, so I took the average\n",
    "lonzo_tutoring_idx = df_with_prices[df_with_prices['post_text'].str.contains('951-795-5027', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[lonzo_tutoring_idx, price_col_idx] = 40\n",
    "\n",
    "except:\n",
    "    print(\"Issue with lonzo_tutoring_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "smoking-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $30 for one hour.\n",
    "poway_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('(619)735-2579', regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[poway_tutor_idx, price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with poway_tutor_idx and iloc.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "average-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $20/hr online, $30/hr in person, split the difference at $25\n",
    "austin_sabrina_idx = df_with_prices[df_with_prices['post_text'].str.contains('My girlfriend Sabrina')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[austin_sabrina_idx, price_col_idx] = 25\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with austin_sabrina_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "catholic-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says $25/hr\n",
    "alex_farrell_idx = df_with_prices[df_with_prices['post_text'].str.contains('Alexander Farrell')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[alex_farrell_idx, price_col_idx] = 25\n",
    "\n",
    "except:\n",
    "    print(\"Issue with alex_farrell_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12b76bc0-85a3-495c-b385-4972b767534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $25/hr if meeting near CSU Sac, $35/hr if they drive to you, $20/hr for online.\n",
    "# I chose $30/hr to split the difference between the in person prices.\n",
    "best_math_idx = df_with_prices[df_with_prices['post_text'].str.contains('bestmathtutoring.com')==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[best_math_idx, price_col_idx] = 30\n",
    "    \n",
    "except:\n",
    "    print(\"Issue with best_math_idx and iloc.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "internal-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucla_grad_henry_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"916 390-7923\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[ucla_grad_henry_idx, price_col_idx] = 35\n",
    "\n",
    "except:\n",
    "    print(\"Issue with ucla_grad_henry_idx and iloc.\")\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5023b-7234-4c18-a3bc-476cfc8cdbc9",
   "metadata": {},
   "source": [
    "#### Checking results - Are there any posts that were marked as needing to be cleaned that we missed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "designed-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 posts that need cleaning.\n"
     ]
    }
   ],
   "source": [
    "num_still_null = len(df_with_prices[df_with_prices['price'].isnull()==True])\n",
    "\n",
    "if num_still_null==0:\n",
    "    print(\"There are no posts with null prices still needing cleaning.\")\n",
    "else:\n",
    "    print(F\"There are {num_still_null} posts that need cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce8997-a97e-46ce-bdd4-401d75a6f997",
   "metadata": {},
   "source": [
    "### Checking Posts that have two prices listed to see if averaging them is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72f4dd25-cd1a-4e13-bc8c-4526d76f150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>46.0</td>\n",
       "      <td>[57, 35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.0</td>\n",
       "      <td>[35, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[60, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>57.5</td>\n",
       "      <td>[45, 70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>46.5</td>\n",
       "      <td>[80, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100.0</td>\n",
       "      <td>[80, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>70.0</td>\n",
       "      <td>[60, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>104.5</td>\n",
       "      <td>[84, 125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[40, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>47.5</td>\n",
       "      <td>[45, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[50, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>42.5</td>\n",
       "      <td>[45, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>20.0</td>\n",
       "      <td>[20, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[40, 70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>70.0</td>\n",
       "      <td>[60, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>100.0</td>\n",
       "      <td>[115, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>30.0</td>\n",
       "      <td>[30, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>32.5</td>\n",
       "      <td>[25, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>40.0</td>\n",
       "      <td>[30, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[35, 65]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>82.5</td>\n",
       "      <td>[65, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>150.0</td>\n",
       "      <td>[200, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[55, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>105.0</td>\n",
       "      <td>[50, 160]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>30.0</td>\n",
       "      <td>[30, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20.0</td>\n",
       "      <td>[15, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>27.5</td>\n",
       "      <td>[45, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>25.0</td>\n",
       "      <td>[40, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>45.0</td>\n",
       "      <td>[30, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>37.5</td>\n",
       "      <td>[45, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>37.5</td>\n",
       "      <td>[30, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>55.0</td>\n",
       "      <td>[50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>22.5</td>\n",
       "      <td>[20, 25]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price  price_list\n",
       "6     46.0    [57, 35]\n",
       "8     40.0    [35, 45]\n",
       "13    55.0    [60, 50]\n",
       "31    57.5    [45, 70]\n",
       "32    46.5    [80, 13]\n",
       "37   100.0   [80, 120]\n",
       "39    70.0    [60, 80]\n",
       "41    55.0    [50, 60]\n",
       "42   104.5   [84, 125]\n",
       "46    50.0    [40, 60]\n",
       "48    47.5    [45, 50]\n",
       "51    50.0    [50, 50]\n",
       "64    42.5    [45, 40]\n",
       "79    20.0    [20, 20]\n",
       "82    55.0    [40, 70]\n",
       "83    70.0    [60, 80]\n",
       "84   100.0   [115, 85]\n",
       "88    30.0    [30, 30]\n",
       "90    32.5    [25, 40]\n",
       "91    40.0    [30, 50]\n",
       "94    50.0    [35, 65]\n",
       "99    82.5   [65, 100]\n",
       "105  150.0  [200, 100]\n",
       "106   55.0    [55, 55]\n",
       "108  105.0   [50, 160]\n",
       "109   30.0    [30, 30]\n",
       "110   20.0    [15, 25]\n",
       "111   27.5    [45, 10]\n",
       "112   25.0    [40, 10]\n",
       "114   45.0    [30, 60]\n",
       "127   37.5    [45, 30]\n",
       "128   37.5    [30, 45]\n",
       "130   55.0    [50, 60]\n",
       "135   22.5    [20, 25]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[df_with_prices['len_of_price_list']==2][['price','price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc8e2d5d-7df4-43e0-b914-9be1a1967153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sanantonio.craigslist.org/lss/d/san-antonio-elementary-math-tutor/7427765931.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nI teach math to elementary kids, my fees are of $17.50 hr and the first 3 lessons are in person. After that we can continue online if you prefer.\\n\\nCall me at (210) four75 fifteen 42    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect posts manually, one by one\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=136\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e02647-d05b-40bc-9776-9fd62f92b97e",
   "metadata": {},
   "source": [
    "#### Ads where averaging doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc06f34c-357f-4508-88ff-7cf515eef0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says 35$/half hour, but explicitly says $57 per hour, so averaging doesn't make sense.  \n",
    "blake_tutoring_idx = df_with_prices[df_with_prices['post_text'].str.contains('BlakeTutoring.com', case=False)==True].index\n",
    "\n",
    "df_with_prices.iloc[blake_tutoring_idx, price_col_idx] = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "allied-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $84/hr but then mentions a $125 for 1.5 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $84\n",
    "test_trainer_inc_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"TestTrainerinc\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[test_trainer_inc_idx, price_col_idx] = 84\n",
    "\n",
    "except:\n",
    "    print(\"Issue with test_trainer_inc_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ab8ea92-15a6-43b2-b913-a86880a2f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $60/45mins, but $80 per hour.  Either price comes out to the same hourly rate, so averaging doesn't make sense.\n",
    "hiro_kobayashi_idx = df_with_prices[df_with_prices['post_text'].str.contains('415-250-4831', case=False)==True].index\n",
    "\n",
    "df_with_prices.iloc[hiro_kobayashi_idx, price_col_idx] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb557fbe-9c04-4f3b-a253-44be8dbcceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $40/1hr, $70/2hr, so averaging doesn't make sense\n",
    "guy_with_suit_idx = df_with_prices[df_with_prices['post_text'].str.contains('trained mathematician with about 20 years experience')==True].index\n",
    "\n",
    "df_with_prices.iloc[guy_with_suit_idx, price_col_idx] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74a848a8-b063-4cce-9aef-4fd838551210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $25/1hr, $40/2hr, so averaging doesn't make sense\n",
    "christian_cerritos_college_idx = df_with_prices[df_with_prices['post_text'].str.contains('trained mathematician with about 20 years experience')==True].index\n",
    "\n",
    "df_with_prices.iloc[christian_cerritos_college_idx, price_col_idx] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04b6fe95-7589-4650-b38f-ca229bdb16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $30/half hr, $50/1hr, so averaging doesn't make sense\n",
    "dustin_csu_long_beach_idx = df_with_prices[df_with_prices['post_text'].str.contains('International Society of Automation')==True].index\n",
    "\n",
    "df_with_prices.iloc[dustin_csu_long_beach_idx, price_col_idx] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7298c75-8a00-4705-90a4-0b3e7ae5a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $65/hr for subject tutoring, $100/hr for standardized tests.  I'm primarily competing against subject tutoring, so I'll use that price\n",
    "smarter_than_you_think_idx = df_with_prices[df_with_prices['post_text'].str.contains('guarantee you are smarter than you think')==True].index\n",
    "\n",
    "df_with_prices.iloc[smarter_than_you_think_idx, price_col_idx] = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01101fb3-221a-482e-9e36-0280d415e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $50/hr or $160/4hr, so it doesn't make sense to average.\n",
    "dead_in_ditch_idx = df_with_prices[df_with_prices['post_text'].str.contains('dead in a ditch')==True].index\n",
    "\n",
    "df_with_prices.iloc[dead_in_ditch_idx, price_col_idx] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "124abde4-be0f-4d62-b675-417fa672cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $45/hr +$10 more per student, so it doesn't make sense to average.\n",
    "distinguished_teacher_idx = df_with_prices[df_with_prices['post_text'].str.contains('\"Distinguished Teacher\"')==True].index\n",
    "\n",
    "df_with_prices.iloc[distinguished_teacher_idx, price_col_idx] = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad1a8716-a2ef-4661-b162-d3e959f92b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $40/hr +$10 more for each additional person, so it doesn't make sense to average.\n",
    "vahab_idx = df_with_prices[df_with_prices['post_text'].str.contains('vababtaghizade@gmail.com')==True].index\n",
    "\n",
    "df_with_prices.iloc[vahab_idx, price_col_idx] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25252b56-959e-46de-9b52-95834da63513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $30/hr for trial session, then $60/hr afterwards, so it doesn't make sense to average.\n",
    "myles_ahead_idx = df_with_prices[df_with_prices['post_text'].str.contains('mylesaheadtutoring')==True].index\n",
    "\n",
    "df_with_prices.iloc[myles_ahead_idx, price_col_idx] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11aa0f22-d46b-4295-b318-94fb14053bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This guy's ad says $45/hr, then talks about selling a workbook for $30, so it doesn't make sense to average.\n",
    "john_the_tutor_idx = df_with_prices[df_with_prices['post_text'].str.contains('480-343-2212')==True].index\n",
    "\n",
    "df_with_prices.iloc[john_the_tutor_idx, price_col_idx] = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba278c-56b6-46ad-b3dd-430f40a5dfde",
   "metadata": {},
   "source": [
    "Conclusion: Averaging doesn't make sense for a good chunk of these posts, but averaging is helpful for others.  I need to come up with a better process here, but will leave that for later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-substance",
   "metadata": {},
   "source": [
    "## Investigating posts with extreme prices.  Are there any price outliers that we need to clean?\n",
    "\n",
    "Prices >= 100 or <= 20 are what I would consider to be extreme prices.  Let's investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58751ba1-6c7d-40b7-b22f-62db9a52a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>post_text</th>\n",
       "      <th>price_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120.0</td>\n",
       "      <td>\\n\\n\\n\\n\\n*****I am currently offering both Zo...</td>\n",
       "      <td>[120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nText 2133408660 or register at peerl...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>150.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Connor and I've be...</td>\n",
       "      <td>[150]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...</td>\n",
       "      <td>[80, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>120.0</td>\n",
       "      <td>\\n\\n\\n\\n\\n🔅Former science teacher and current ...</td>\n",
       "      <td>[120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>200.0</td>\n",
       "      <td>\\n\\n\\n\\n\\ncheck out my website!\\nmd-maker.com\\...</td>\n",
       "      <td>[200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>19.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10.0</td>\n",
       "      <td>\\n\\n\\n\\n\\n\"\"\"𝗪𝗲 𝗳𝗼𝘂𝗻𝗱𝗲𝗱 𝗘𝘀𝘀𝗮𝘆𝗣𝗿𝗼 𝗶𝗻 𝗼𝗿𝗱𝗲𝗿 𝘁𝗼 𝗵...</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>15.0</td>\n",
       "      <td>\\n\\n\\n\\n\\njargon free math tutor $15 all level...</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nWhy I am an exceptional tutor: \\n\\nF...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>18.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nSAT prep for as low as $18 per hour!...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nExperienced tutor (more than 16 year...</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHAPPY NEW YEAR!!!\\n\\nYes, you read r...</td>\n",
       "      <td>[20, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nIf you would like your K-through-5th...</td>\n",
       "      <td>[115, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>120.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...</td>\n",
       "      <td>[120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>150.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nG'day! My name is Daniel. I graduate...</td>\n",
       "      <td>[200, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi everyone, do you need math, physi...</td>\n",
       "      <td>[15, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>100.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nTenured math professor at a major un...</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>10.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHi! My name is Sher. I have a Bachel...</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>10.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nHello! My name is Jai! I am offering...</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>17.0</td>\n",
       "      <td>\\n\\n\\n\\n\\nI teach math to elementary kids, my ...</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price                                          post_text  price_list\n",
       "5    120.0  \\n\\n\\n\\n\\n*****I am currently offering both Zo...       [120]\n",
       "9     20.0  \\n\\n\\n\\n\\nText 2133408660 or register at peerl...        [20]\n",
       "26   150.0  \\n\\n\\n\\n\\nHello! My name is Connor and I've be...       [150]\n",
       "37   100.0  \\n\\n\\n\\n\\nMy name is Sameer Tyagi, former Harv...   [80, 120]\n",
       "49   120.0  \\n\\n\\n\\n\\n🔅Former science teacher and current ...       [120]\n",
       "53   200.0  \\n\\n\\n\\n\\ncheck out my website!\\nmd-maker.com\\...       [200]\n",
       "59    19.0  \\n\\n\\n\\n\\nHi! \\n\\nI am a certified teacher wit...        [19]\n",
       "63    10.0  \\n\\n\\n\\n\\n\"\"\"𝗪𝗲 𝗳𝗼𝘂𝗻𝗱𝗲𝗱 𝗘𝘀𝘀𝗮𝘆𝗣𝗿𝗼 𝗶𝗻 𝗼𝗿𝗱𝗲𝗿 𝘁𝗼 𝗵...        [10]\n",
       "72    15.0  \\n\\n\\n\\n\\njargon free math tutor $15 all level...        [15]\n",
       "73    20.0  \\n\\n\\n\\n\\nLocated in NYC. I graduated with a b...        [20]\n",
       "74    20.0  \\n\\n\\n\\n\\nWhy I am an exceptional tutor: \\n\\nF...        [20]\n",
       "75    18.0  \\n\\n\\n\\n\\nSAT prep for as low as $18 per hour!...        [18]\n",
       "76   100.0  \\n\\n\\n\\n\\nExperienced tutor (more than 16 year...       [100]\n",
       "79    20.0  \\n\\n\\n\\n\\nHAPPY NEW YEAR!!!\\n\\nYes, you read r...    [20, 20]\n",
       "84   100.0  \\n\\n\\n\\n\\nIf you would like your K-through-5th...   [115, 85]\n",
       "104  120.0  \\n\\n\\n\\n\\nG'day! My name's Daniel, and I'm a f...       [120]\n",
       "105  150.0  \\n\\n\\n\\n\\nG'day! My name is Daniel. I graduate...  [200, 100]\n",
       "110   20.0  \\n\\n\\n\\n\\nHi everyone, do you need math, physi...    [15, 25]\n",
       "113  100.0  \\n\\n\\n\\n\\nTenured math professor at a major un...       [100]\n",
       "121   10.0  \\n\\n\\n\\n\\nHi! My name is Sher. I have a Bachel...        [10]\n",
       "129   10.0  \\n\\n\\n\\n\\nHello! My name is Jai! I am offering...        [10]\n",
       "136   17.0  \\n\\n\\n\\n\\nI teach math to elementary kids, my ...        [17]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_prices[(df_with_prices['price']>=100) | (df_with_prices['price']<=20)][['price', 'post_text', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a4892b0-a385-4119-9897-659036333b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sfbay.craigslist.org/sfc/lss/d/harvard-grad-gmat-ea-gre-tutor-99th/7427997518.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nOnline GMAT / Executive Assessment (EA) / GRE Tutoring and MBA Admissions Consulting\\n\\nMany clients have been admitted to HBS, Stanford GSB and Wharton (HSW) as well as all the other top business schools - some with generous scholarships.\\n\\nOne recent client who found me on Craigslist wrote, “I achieved my dream and will be attending Harvard Business School in the fall! Working with Stuart was the best decision I could have made and will now change the trajectory of my career.” After we worked together on the GMAT, she raised her score by 90 points to 730, and she then hired me to help her with her applications. She was also accepted at Wharton and two other great schools. See below for more HSW testimonials.\\n\\nCredentials\\n* Harvard grad with 99th percentile quant and verbal GMAT scores on the 1st attempt\\n* 10+ years of 1-on-1 online tutoring experience with 1,000+ clients, including hundreds who have achieved 700+ GMAT or 320+ GRE scores\\n* Clients from diverse backgrounds, including multiple members of the White House staff, multiple fighter pilots & other military officers, and numerous individuals from investment banking, private equity, venture capital and management consulting, among other industries\\n* 10+ years tutoring and teaching elite GMAT classes for Veritas Prep, where I helped improve the company’s curriculum\\n* 50+ 5-star Yelp reviews in major U.S. cities\\n\\nI’ve perfected modular GMAT, EA and GRE curricula that will help you reach your goals as efficiently as possible. Many of my clients have already taken some of the most popular courses (Veritas, Manhattan, Kaplan, Princeton, etc.), and as a result, I’ve built an understanding of the strengths and weaknesses of those programs. I’m also thoroughly familiar with the Official Guides. \\n\\nEfficient Online Tutoring\\nI use GoToMeeting, a pen-enabled laptop and all the best test prep ebooks to deliver efficient online tutoring.\\n\\nMBA Admissions Consulting\\nI can also help you with your business school application strategy, resume/CV, essays, short answers and interview prep. I’ve had admissions consulting clients admitted to all the top business schools. My strategic communications background includes helping C-level executives at dozens of major companies in closing more than $20 billion in transactions and achieving numerous breakthroughs. I now apply these skills and 10+ years of admissions consulting experience in advising clients.\\n\\nContact\\nFor my rates and dozens of testimonials, search Simply Brilliant Prep to find me, then complete a form to get started.\\n\\nYou can also call me at 877-228-5293 or email me through this ad.\\n\\nMore HSW Testimonials\\n\\n“Stuart was an instrumental piece to my business school application process. We worked tirelessly at math fundamentals that I haven’t reviewed in over a decade, and my score skyrocketed from 149 to 160 over the course of a year. I ended up receiving acceptances to both Harvard and Stanford, and will forever be grateful to Stuart’s commitment to helping me achieve my goals.”\\nA.O., GRE Client Accepted to HBS and Stanford GSB for the Class of 2024\\n\\n“Got in to HBS! Thanks for all your help, Stuart! It’s been a long time in the making, and I don’t think I could have done it without your help... You were great to work with.”\\nMichael W., HBS Class of 2021, 700+ Score\\n\\n“I got into both hbs and gsb yesterday! I’m very excited and I want to thank you for your your time and help throughout the process! You definitely have a reference in me going forward!”\\nPeter D., Stanford GSB Class of 2021, 700+ Score\\n\\n“Working online with Stuart from Simply Brilliant Prep was a game changer in my GMAT experience. I scored a 770 on the exam, exceeding my wildest expectations, and I have Stuart to thank! I spent the months leading up to my test date working with Stuart online on a one-on-one basis. Going into this experience, I had resisted signing up for a GMAT class or tutoring. Yet after months of slogging through practice exams and problems, I figured I would give a tutor a shot. Stuart introduced incredible clarity and focus to my studying, empowering me with eye-opening strategies to manage my time, navigate GMAT tricks, and employ straightforward approaches in responding to the most challenging problems. With Stuart, I gained individualized attention and support – working with him gave me true 'bang for my buck.' I can’t recommend him strongly enough, and will continue to talk him up to my colleagues and friends pursuing the GMAT.”\\nSam D., Stanford GSB Class of 2019 and Arjay Miller Scholar, 770 Score\\n\\n“I worked with Stuart for the 3 weeks leading up to my final attempt at the GMAT. I wish I would have met him earlier in my studies because it would have probably saved me months of studying on my own... Not only is Stuart an expert in everything related to the GMAT, but he is also patient and had a good sense of the topics I should focus on. Most importantly, his energy and excitement about the topics kept me engaged, which was incredibly important as most of our sessions were during the evenings after my 11-12 hour work days. I highly recommend working with Stuart at whichever stage you may be in your GMAT studies!”\\nKaren B., Wharton Class of 2023, 700+ Score\\n\\n“Stuart was an excellent GMAT tutor! Prior to hiring Stuart I had scored 690 on the GMAT multiple times. After 6 sessions, I retook the exam and scored a 750. Stuart's teaching materials were extremely helpful and he tailored the lesson to target my weak spots. His assigned study material in between sessions and did a fantastic job drilling in the fundamentals and repetition required to work your way through the GMAT. My advice to anyone working with Stuart is to be up front and honest with him on exactly what you're struggling with or need on the test, and he will provide you with the tools you need to improve your score. Would absolutely recommend to anyone looking to achieve a high score on the GMAT!”\\nRyan K., Wharton Class of 2021, 750 Score\\n\\nMore GRE Testimonials\\n“With less than a month to deadlines, I was working on my essays and needed 5 points to reach my target GRE score, which meant I did not have any time to spare. I needed guidance to quickly achieve those points, so reached out to Stuart. He was easy to work with and allowed me to set the agenda. Besides working on my weaknesses, his encouraging style and highlights of my strengths helped me mentally tackle the exam. I was not pressured into taking more sessions, and the homework he assigned was much appreciated and a helpful way to study. I exceeded my target, but wish I had utilized Stuart earlier in my studies to unlock a higher score. Even if you are a self-studier like myself, I would recommend supplementing your studies with Stuart to help you make breakthroughs and accelerate your studying.”\\nJames D., 2020 Client, Improved Both Verbal (+8) and Quant (+3) for an 11 Point Overall Increase to 330 (V164 Q166)\\n\\n“If you are going to hire a tutor, hire Stuart! After trying multiple other test prep companies and studying for close to a year without reaching my target GRE score, I can say without a doubt that Stuart's help and expertise is what made the difference for me. With his guidance, I was able to not only beat my target score, but get into my dream school! In addition to being extremely bright, focused, professional, and knowledgeable, Stuart is kind and patient. He really understands the emotional and mental side of the test prep game, one of the most critical pieces of this process to help you unlock your full potential. When I was stressed and anxious (many times), Stuart was there to not only provide practical, actionable, no-BS advice, but was also so positive, encouraging, and motivating, which was very refreshing and appreciated. Stuart's thoughtful and thorough approach makes it clear that he truly wants to help you achieve the best possible outcome in your MBA journey - I highly recommend Stuart!”\\nErika J., 2019 Client Admitted to Columbia Business School, 320+ Score    \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Manually inspect these posts one by one\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  x=40\n",
    "  #display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['link'])\n",
    "  display(df_with_prices.iloc[x]['post_text'])\n",
    "  display(df_with_prices.iloc[x]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-belize",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dropping posts with extreme prices that aren't relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "green-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad is for poker tutoring/coaching, not really what I'm competing against, so we drop it.  He also mentions he tutors math in this post, but he has a separate post, that we've captured, which has his math tutoring pricing information.\n",
    "australia_daniel_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"I'm available as a dealer if you need one\", regex=False)==True].index\n",
    "\n",
    "df_with_prices.drop(labels=australia_daniel_idx, inplace=True)\n",
    "df_with_prices = df_with_prices.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-storage",
   "metadata": {},
   "source": [
    "### Correcting pricing information for posts with extreme prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "radical-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $50/hr but then mentions a prepay plan for $160 for 4 hours.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $50\n",
    "google_maps_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"willing to travel if Google Maps\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[google_maps_idx, price_col_idx] = 50\n",
    "\n",
    "except:\n",
    "    print(\"Issue with google_maps_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "nutritional-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ad says $45/hr for high school or college, but then mentions a $35 for middle school.  Since these are the only two prices in the post, our code averages them, so we set the correct price to $45, since I primarily tutor high school or college students.\n",
    "rancho_penasquitos_idx = df_with_prices[df_with_prices['post_text'].str.contains(\"Rancho Penasquitos (Park Village Neighborhood)\", regex=False)==True].index\n",
    "\n",
    "try:\n",
    "    df_with_prices.iloc[rancho_penasquitos_idx, price_col_idx] = 45\n",
    "\n",
    "except:\n",
    "    print(\"Issue with rancho_penasquitos_idx and iloc.\")\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920c90f-dd8c-43ef-bbef-ebf45f58d031",
   "metadata": {},
   "source": [
    "### Transforming Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-jurisdiction",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *Load* - Saving results\n",
    "\n",
    "### Store results locally as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "spread-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.\n",
    "df_for_sql = df_with_prices.drop(labels=['link', 'price_list', 'len_of_price_list', 'match'], axis=1)\n",
    "\n",
    "# In order for psycopg2 to parse our CSV file correctly later, we need to escape all new line characters by adding an additional \\ in front of \\n.\n",
    "df_for_sql['post_text'] = df_for_sql['post_text'].str.replace('\\n', '\\\\n')\n",
    "\n",
    "# Store cleaned data as CSV file in preparation for importing to SQL database\n",
    "df_for_sql.to_csv(\"./csv_files/{}_all_regions_with_prices.csv\".format(date_of_html_request), index=False, sep=';')\n",
    "\n",
    "# Store original data, before we applied any cleaning to it, in case it's needed for something later on.\n",
    "concat_df.to_csv(\"./csv_files/{}_all_regions_posts.csv\".format(date_of_html_request), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-boards",
   "metadata": {},
   "source": [
    "### Importing into PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "conscious-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to PSQL database\n",
    "conn = psycopg2.connect(\"host=localhost dbname=rancher user=rancher\")\n",
    "\n",
    "# Instantiate a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Use cursor object to create a database for storing the information we scraped and cleaned, if one doesn't already exist.\n",
    "cur.execute(\"\"\"    \n",
    "    CREATE TABLE IF NOT EXISTS cl_tutoring2(\n",
    "    id SERIAL primary key,\n",
    "    date_scraped date,\n",
    "    price decimal,\n",
    "    city text,\n",
    "    subregion text,\n",
    "    region text,\n",
    "    post_text text,\n",
    "    date_posted timestamp\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "framed-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Copy data from our CSV file into database.  \n",
    "### Note, we can use the ; separator freely because we replaced all instances of semicolons in post_text to commas during the preprocessing stage, ensuring that psycopg2 won't misinterpret a semicolon in the body of a post as a separator.\n",
    "### Also, we must specify null=\"\" because Python represents null values as an empty string when writing to a CSV file and psycopg2 needs to know how null values are represented in the CSV file in order to properly insert null values into the database\n",
    "with open('./csv_files/' + str(date_of_html_request) + '_all_regions_with_prices.csv', 'r') as file:\n",
    "    next(file) # Skip the header row\n",
    "    cur.copy_from(file, 'cl_tutoring2', sep=';', null=\"\", columns=('date_posted', 'price', 'city', 'subregion', 'region', 'post_text', 'date_scraped'))\n",
    "    \n",
    "# Commit changes to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee0f05-73d9-4c56-85ae-291626e99d95",
   "metadata": {},
   "source": [
    "### Done!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-wheat",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298ab55-1540-4c01-b002-aac0cb052bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af291fc-3eea-45b2-9228-2ef0820d9302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378a575-b577-42a5-a5ef-18c5ea986a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac027c6-34dd-4897-ac55-2d5915db4557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cfb2a6a-dd99-4472-bc24-96138d29dd93",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] toc-hr-collapsed=true"
   },
   "source": [
    "# IDEA: Transforming Craigslist data REMOVING ENTRIES AND QUERYING WITH SQL LATER -- TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd605d-5ceb-4b2d-b343-f8ae0417eaef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Are there any posts that might need manual cleaning?  This would include:\n",
    "* Posts that had 3 or more prices and were marked as null\n",
    "* Posts where the price wasn't able to convert from `str` -> `int` and were marked as null during pre-processing\n",
    "\n",
    "I'll identify these posts, then remove them from our `DataFrame` to be analyzed later.  All remaining posts will have just a single price listed, which we can input to our SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65e34c-0bc3-408a-8f9d-215dd876136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the entries with 3 or more prices listed, let's investigate why\n",
    "df_null_prices = df_with_prices[df_with_prices['price'].isnull()==True]\n",
    "df_null_prices[['price', 'price_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1b971-5385-4c90-9a22-d17a21a8b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_mult_prices = df_null_prices.shape[0]\n",
    "print(F\"There were {posts_with_mult_prices} posts with price marked null.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302f1d5-432f-42fb-8a53-d427748cc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_price_idx = df_null_prices.index\n",
    "df_with_single_price = df_with_prices.drop(index=null_price_idx)\n",
    "df_with_single_price = df_with_single_price.reset_index(drop=True)\n",
    "df_with_single_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405892fe-4fb3-4b84-a2fb-09df8d335fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_prices = df_null_prices.drop(columns=['len_of_price_list', 'match'])\n",
    "df_null_prices.to_csv('./posts_to_investigate/{}_posts_with_null_prices.csv'.format(date_of_html_request), index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
